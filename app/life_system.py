import asyncio
import json
import os
from datetime import date, timedelta
from utils.logging_config import get_logger

logger = get_logger(__name__)
import uuid
from typing import Optional
from datetime import datetime
import redis  # æ·»åŠ  Redis æ”¯æŒ

from services.ai_service import (
    get_weather_info,
    generate_daily_schedule,
    generate_major_event,
    generate_micro_experiences,
)
from utils.postgres_service import (
    insert_daily_schedule,
    get_daily_schedule_by_date,
    update_daily_schedule,
    insert_major_event,
    get_major_event_by_date,  # æ–°å¢
    insert_micro_experience,
    get_micro_experiences_by_daily_schedule_id,
    get_micro_experiences_by_related_item_id,  # æ–°å¢
)


# å®šä¹‰ç”Ÿæˆå†…å®¹å­˜å‚¨çš„æ–‡ä»¶å¤¹
GENERATED_CONTENT_DIR = "generated_content"


async def generate_and_store_daily_life(target_date: date):
    """
    ç”Ÿæˆå¹¶å­˜å‚¨æŒ‡å®šæ—¥æœŸçš„å¾·å…‹è¨æ–¯ç”Ÿæ´»æ—¥ç¨‹ã€‚
    åŒ…æ‹¬è·å–å¤©æ°”ã€ç”Ÿæˆæ—¥ç¨‹ã€å­˜å‚¨åˆ°æ•°æ®åº“å’Œæ–‡ä»¶ã€‚
    """
    date_str = target_date.strftime("%Y-%m-%d")
    logger.info(f"[daily_life] å¼€å§‹ç”Ÿæˆæ¯æ—¥æ—¥ç¨‹: {date_str}")

    # 1. è·å–å¤©æ°”ä¿¡æ¯
    weather = get_weather_info(date_str)
    logger.debug(f"[daily_life] å¤©æ°”ä¿¡æ¯: {weather}")

    # 2. åˆ¤æ–­å·¥ä½œæ—¥/å‘¨æœ« (ç®€åŒ–é€»è¾‘ï¼Œå®é™…å¯æ ¹æ®èŠ‚å‡æ—¥ç­‰æ›´å¤æ‚åˆ¤æ–­)
    day_type = "weekend" if target_date.weekday() >= 5 else "weekday"
    logger.debug(f"[daily_life] æ—¥æœŸç±»å‹: {day_type}")

    # 3. æ£€æŸ¥å¤§äº‹ä»¶
    is_in_major_event = False
    major_event_context = None
    logger.debug(f"[daily_life] æ£€æŸ¥æ˜¯å¦å¤„äºå¤§äº‹ä»¶: {date_str}")

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨åŒ…å«ç›®æ ‡æ—¥æœŸçš„å¤§äº‹ä»¶
    major_event_context = get_major_event_by_date(date_str)

    if major_event_context:
        is_in_major_event = True
    else:
        logger.debug("[daily_life] æœªæ£€æµ‹åˆ°å·²å­˜åœ¨çš„å¤§äº‹ä»¶")

    # å¦‚æœæ²¡æœ‰å¤§äº‹ä»¶ï¼Œåˆ™æ ¹æ®0.028æ¦‚ç‡å†³å®šæ˜¯å¦ç”Ÿæˆæ–°çš„å¤§äº‹ä»¶
    if not is_in_major_event:
        import random

        gen_prob = 0.028  # 0.028
        rand_val = random.random()

        if rand_val < gen_prob:  # 0.028æ¦‚ç‡ç”Ÿæˆå¤§äº‹ä»¶
            # æ­£æ€åˆ†å¸ƒç”ŸæˆæŒç»­å¤©æ•° (Î¼=4, Ïƒ=2)ï¼ŒèŒƒå›´1-7å¤©

            duration_days = max(1, min(7, int(random.gauss(4, 2))))
            logger.debug(f"[daily_life] å¤§äº‹ä»¶æŒç»­å¤©æ•°: {duration_days}å¤© (Î¼=4, Ïƒ=2)")

            # éšæœºé€‰æ‹©äº‹ä»¶ç±»å‹
            event_types = ["å‡ºå·®ä»»åŠ¡", "ç‰¹æ®Šå¿«é€’", "åŸ¹è®­å­¦ä¹ ", "ä¸ªäººäº‹åŠ¡", "ç”Ÿç—…"]
            weights = [0.4, 0.3, 0.15, 0.1, 0.05]  # äº‹ä»¶ç±»å‹æ¦‚ç‡æƒé‡
            event_type = random.choices(event_types, weights=weights)[0]
            logger.debug(f"[daily_life] é€‰æ‹©äº‹ä»¶ç±»å‹: {event_type} (æƒé‡: {weights})")

            # ç”Ÿæˆå¤§äº‹ä»¶
            end_date = target_date + timedelta(days=duration_days - 1)
            major_event_context = await generate_and_store_major_event(
                target_date, end_date, event_type
            )
            is_in_major_event = True
            logger.debug(
                f"[daily_life] æ–°å¤§äº‹ä»¶ç”Ÿæˆå®Œæˆ: {event_type}, æŒç»­{duration_days}å¤©"
            )

    # å¦‚æœå¤„äºå¤§äº‹ä»¶ä¸­ï¼Œä½†æœªè·å–ä¸Šä¸‹æ–‡ï¼Œå°è¯•ä»æ•°æ®åº“è·å–
    if is_in_major_event and not major_event_context:
        logger.warning("å¤§äº‹ä»¶ä¸Šä¸‹æ–‡ç¼ºå¤±ï¼Œå°è¯•ä»æ•°æ®åº“è·å–...")
        major_event_context = get_major_event_by_date(date_str)
        if not major_event_context:
            logger.warning("æ•°æ®åº“ä¸­ä¹Ÿæœªæ‰¾åˆ°å¤§äº‹ä»¶è¯¦æƒ…ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            major_event_context = {
                "event_title": "é»˜è®¤å¤§äº‹ä»¶",
                "event_type": "é»˜è®¤ç±»å‹",
                "main_objective": "é»˜è®¤ç›®æ ‡",
            }
    if is_in_major_event:
        weather += "ä»¥ä¸Šä¸ºéšæœºå¤©æ°”æƒ…å†µï¼Œä»…ä¾›å‚è€ƒï¼Œä»¥å¤§äº‹ä»¶æƒ…å†µä¸ºå‡†ã€‚"
    else:
        logger.debug("[daily_life] å¤§äº‹ä»¶çŠ¶æ€: ä¸å­˜åœ¨")

    # 4. è°ƒç”¨AIç”Ÿæˆæ¯æ—¥æ—¥ç¨‹
    logger.debug("[daily_life] è°ƒç”¨ AI ç”Ÿæˆæ¯æ—¥æ—¥ç¨‹")
    daily_schedule_data = await generate_daily_schedule(
        date=date_str,
        day_type=day_type,
        weather=weather,
        is_in_major_event=is_in_major_event,
        major_event_context=major_event_context,
        special_flags=[],
    )

    if "error" in daily_schedule_data:
        logger.error(f"AIç”Ÿæˆæ—¥ç¨‹å¤±è´¥: {daily_schedule_data['error']}")
        return None

    logger.debug("[daily_life] AIæ—¥ç¨‹ç”ŸæˆæˆåŠŸ")

    # 5. å­˜å‚¨åˆ°æ•°æ®åº“
    logger.debug("[daily_life] å­˜å‚¨æ—¥ç¨‹åˆ°æ•°æ®åº“")
    try:
        # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦å·²å­˜åœ¨æ—¥ç¨‹ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™æ’å…¥
        existing_schedule = get_daily_schedule_by_date(date_str)
        if existing_schedule:
            schedule_id = existing_schedule["id"]
            update_daily_schedule(
                schedule_id=schedule_id,
                schedule_data=daily_schedule_data,
                weather=weather,
                is_in_major_event=is_in_major_event,
                major_event_id=(
                    major_event_context["id"] if major_event_context else None
                ),
            )
            logger.debug(f"[daily_life] æ—¥ç¨‹å·²æ›´æ–° (ID: {schedule_id})")
        else:
            schedule_id = insert_daily_schedule(
                date=date_str,
                schedule_data=daily_schedule_data,
                weather=weather,
                is_in_major_event=is_in_major_event,
                major_event_id=(
                    major_event_context["id"] if major_event_context else None
                ),
            )
            logger.debug(f"[daily_life] æ—¥ç¨‹å·²æ’å…¥ (ID: {schedule_id})")

        daily_schedule_data["id"] = str(schedule_id)  # å°†æ•°æ®åº“ç”Ÿæˆçš„IDæ·»åŠ åˆ°æ•°æ®ä¸­
    except Exception as e:
        logger.error(f"å­˜å‚¨æ—¥ç¨‹åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return None

    # 6. å­˜å‚¨åˆ°æ–‡ä»¶
    logger.debug("[daily_life] å­˜å‚¨æ—¥ç¨‹åˆ°æ–‡ä»¶")
    os.makedirs(GENERATED_CONTENT_DIR, exist_ok=True)
    file_path = os.path.join(GENERATED_CONTENT_DIR, f"daily_schedule_{date_str}.json")
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(daily_schedule_data, f, ensure_ascii=False, indent=2)
        logger.debug(f"[daily_life] æ—¥ç¨‹å·²ä¿å­˜åˆ°æ–‡ä»¶: {file_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ—¥ç¨‹åˆ°æ–‡ä»¶å¤±è´¥: {e}")

    # 7. ç”Ÿæˆå¹¶å­˜å‚¨å¾®è§‚ç»å†
    if "schedule_items" in daily_schedule_data:
        successful_experiences = 0

        # è·å–ä¹‹å‰çš„ç»å†æ‘˜è¦ï¼ˆç®€åŒ–å®ç°ï¼‰
        previous_experiences_summary = None

        for index, item in enumerate(daily_schedule_data["schedule_items"]):
            # è®¾ç½®å½“å‰æ—¶é—´ä¸ºé¡¹ç›®å¼€å§‹æ—¶é—´ï¼ˆå¦‚éœ€ä½¿ç”¨å¯åœ¨åç»­é€»è¾‘ä¸­å¼•ç”¨ï¼‰

            micro_experiences = await generate_and_store_micro_experiences(
                schedule_item=item,
                current_date=target_date,
                previous_experiences=previous_experiences_summary,
                major_event_context=major_event_context,
                schedule_id=schedule_id,  # ä¼ å…¥æ¯æ—¥è®¡åˆ’çš„ID
            )

            if micro_experiences:
                successful_experiences += 1
                # æ›´æ–°ç»å†æ‘˜è¦ï¼ˆä½¿ç”¨ç”Ÿæˆçš„ç»å†å†…å®¹ï¼‰
                exp_summaries = [
                    f"{exp.get('start_time', '')}-{exp.get('end_time', '')}: {exp.get('content', '')[:50]}..."
                    for exp in micro_experiences
                ]
                previous_experiences_summary = exp_summaries

    else:
        logger.warning("æ—¥ç¨‹ä¸­æ²¡æœ‰å¯ç”Ÿæˆå¾®è§‚ç»å†çš„é¡¹ç›®")

    logger.info(f"[daily_life] ç”Ÿæˆå®Œæˆ: {date_str} æ¯æ—¥æ—¥ç¨‹ä¸å­˜å‚¨")

    # 8. ä½¿ç”¨ä¸“ç”¨å‡½æ•°æ”¶é›†éœ€è¦äº¤äº’çš„å¾®è§‚ç»å†
    logger.debug("[daily_life] å¼€å§‹æ”¶é›†éœ€è¦ä¸»åŠ¨äº¤äº’çš„å¾®è§‚ç»å†")
    await collect_interaction_experiences(target_date)

    return daily_schedule_data


async def _store_enhanced_interaction_data(target_date: date, micro_experiences: list, daily_schedule: dict, redis_client):
    """
    å­˜å‚¨å¢å¼ºçš„äº¤äº’æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„å¾®è§‚ç»å†ä¿¡æ¯ã€schedule_itemæ•°æ®å’ŒèƒŒæ™¯ä¿¡æ¯
    """
    date_str = target_date.strftime("%Y-%m-%d")
    enhanced_redis_key = f"interaction_needed_enhanced:{date_str}"
    logger.debug(f"[interactions] å¼€å§‹å­˜å‚¨å¢å¼ºäº¤äº’æ•°æ®: {enhanced_redis_key}")
    
    # è·å–å¤§äº‹ä»¶èƒŒæ™¯ä¿¡æ¯ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
    major_event_context = get_major_event_by_date(date_str)
    
    # è·å–schedule_itemsæ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
    schedule_items_map = {}
    if daily_schedule.get("schedule_data") and daily_schedule["schedule_data"].get("schedule_items"):
        for item in daily_schedule["schedule_data"]["schedule_items"]:
            item_id = item.get("id")
            if item_id:
                schedule_items_map[item_id] = item
    
    # è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´è½¬æ—¶é—´æˆ³
    def time_to_timestamp(date_obj: date, time_str: str) -> float:
        dt_str = f"{date_obj.strftime('%Y-%m-%d')} {time_str}"
        dt_obj = datetime.strptime(dt_str, "%Y-%m-%d %H:%M")
        return dt_obj.timestamp()
    
    enhanced_interactions = []
    
    # å¤„ç†æ¯ä¸ªå¾®è§‚ç»å†è®°å½•
    for record in micro_experiences:
        experiences = record.get("experiences", [])
        related_item_id = record.get("related_item_id")
        
        # è·å–å…³è”çš„schedule_item
        related_schedule_item = schedule_items_map.get(related_item_id) if related_item_id else None
        
        for exp in experiences:
            if exp.get("need_interaction") is True:
                # æ„å»ºå¢å¼ºæ•°æ®å¯¹è±¡
                enhanced_exp = {
                    # åŸæœ‰å¾®è§‚ç»å†æ•°æ®
                    "id": exp.get("id"),
                    "start_time": exp.get("start_time"),
                    "end_time": exp.get("end_time"),
                    "content": exp.get("content"),
                    "emotions": exp.get("emotions"),
                    "thoughts": exp.get("thoughts"),
                    "need_interaction": exp.get("need_interaction"),
                    "interaction_content": exp.get("interaction_content"),
                    
                    # å…³è”çš„schedule_itemæ•°æ®ï¼ˆå¢å¼ºä¿¡æ¯ï¼‰
                    "schedule_context": {
                        "item_id": related_item_id,
                        "title": related_schedule_item.get("title") if related_schedule_item else None,
                        "location": related_schedule_item.get("location") if related_schedule_item else None,
                        "description": related_schedule_item.get("description") if related_schedule_item else None,
                        "companions": related_schedule_item.get("companions", []) if related_schedule_item else [],
                        "category": related_schedule_item.get("category") if related_schedule_item else None,
                    },
                    
                    # å¤§äº‹ä»¶èƒŒæ™¯ä¿¡æ¯
                    "major_event_context": major_event_context,
                    
                    # æ´¾ç”Ÿä¿¡æ¯
                    "date": date_str,
                    "time_period": _get_time_period(exp.get("start_time")),
                    "enhanced_data_version": "1.0",
                }
                
                enhanced_interactions.append(enhanced_exp)
    
    # å­˜å‚¨åˆ°Redis Sorted Set
    for enhanced_exp in enhanced_interactions:
        try:
            score = time_to_timestamp(target_date, enhanced_exp["start_time"])
            redis_client.zadd(enhanced_redis_key, {json.dumps(enhanced_exp, ensure_ascii=False): score})
        except Exception as e:
            logger.warning(f"å­˜å‚¨å•ä¸ªå¢å¼ºäº¤äº’æ•°æ®å¤±è´¥: {e}")
    
    # è®¾ç½®è¿‡æœŸæ—¶é—´
    redis_client.expire(enhanced_redis_key, 86400)  # 24å°æ—¶è¿‡æœŸ
    logger.info(f"[interactions] å¢å¼ºäº¤äº’æ•°æ®å·²å­˜å‚¨: {enhanced_redis_key} (å…± {len(enhanced_interactions)} æ¡)")


def _get_time_period(time_str: str) -> str:
    """æ ¹æ®æ—¶é—´å­—ç¬¦ä¸²è¿”å›æ—¶é—´æ®µæè¿°"""
    if not time_str:
        return "unknown"
    
    try:
        hour = int(time_str.split(":")[0])
        if 5 <= hour < 9:
            return "early_morning"
        elif 9 <= hour < 12:
            return "morning"
        elif 12 <= hour < 14:
            return "noon"
        elif 14 <= hour < 18:
            return "afternoon"
        elif 18 <= hour < 21:
            return "evening"
        else:
            return "night"
    except:
        return "unknown"


async def collect_interaction_experiences(target_date: date):
    """
    å•ç‹¬æ”¶é›†éœ€è¦äº¤äº’çš„å¾®è§‚ç»å†å¹¶å­˜å…¥Redis
    """
    date_str = target_date.strftime("%Y-%m-%d")
    logger.info(f"[interactions] å¼€å§‹æ”¶é›†éœ€è¦ä¸»åŠ¨äº¤äº’çš„å¾®è§‚ç»å†: {date_str}")

    try:
        # ä»æ•°æ®åº“è·å–å½“æ—¥æ—¥ç¨‹ ID
        daily_schedule = get_daily_schedule_by_date(date_str)
        if not daily_schedule:
            logger.warning(f"æœªæ‰¾åˆ° {date_str} çš„æ—¥ç¨‹æ•°æ®")
            return False

        schedule_id = daily_schedule["id"]

        # æŸ¥è¯¢å…³è”çš„å¾®è§‚ç»å†
        micro_experiences = get_micro_experiences_by_daily_schedule_id(schedule_id)
        if not micro_experiences:
            logger.debug("[interactions] å½“æ—¥æ²¡æœ‰å¾®è§‚ç»å†æ•°æ®")
            return False

        # ç­›é€‰éœ€è¦äº¤äº’çš„æ¡ç›®
        interaction_needed = []
        for record in micro_experiences:
            experiences = record.get("experiences", [])
            for exp in experiences:
                if exp.get("need_interaction") is True:
                    interaction_needed.append(exp)


        # å­˜å‚¨åˆ° Redis
        from utils.redis_manager import get_redis_client
        r = get_redis_client()
        redis_key = f"interaction_needed:{date_str}"

        # è¾…åŠ©å‡½æ•°ï¼šå°† HH:MM æ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©çš„ Unix æ—¶é—´æˆ³
        def time_to_timestamp(date_obj: date, time_str: str) -> float:
            dt_str = f"{date_obj.strftime('%Y-%m-%d')} {time_str}"
            dt_obj = datetime.strptime(dt_str, "%Y-%m-%d %H:%M")
            return dt_obj.timestamp()

        # å­˜å‚¨æ–°æ•°æ®åˆ° Sorted Set
        # ä½¿ç”¨ end_time çš„ Unix æ—¶é—´æˆ³ä½œä¸º score
        # å¦‚æœ Sorted Set ä¸­å·²å­˜åœ¨ç›¸åŒçš„ memberï¼Œzadd ä¼šæ›´æ–°å…¶ score
        # å¦‚æœæ¯å¤©ç”Ÿæˆæ–°çš„ keyï¼Œåˆ™ä¸éœ€è¦åˆ é™¤æ—§æ•°æ®
        for exp in interaction_needed:
            try:
                score = time_to_timestamp(target_date, exp["start_time"])
                r.zadd(redis_key, {json.dumps(exp, ensure_ascii=False): score})
            except KeyError as ke:
                logger.error(f"ç¼ºå°‘æ—¶é—´å­—æ®µï¼Œæ— æ³•æ·»åŠ åˆ° Sorted Set: {exp} - {ke}")
            except Exception as add_e:
                logger.error(f"æ·»åŠ åˆ° Redis Sorted Set å¤±è´¥: {exp} - {add_e}")

        # è®¾ç½® 24 å°æ—¶è¿‡æœŸ
        r.expire(redis_key, 86400)
        logger.info(f"[interactions] å·²å­˜å‚¨åˆ° Redis: {redis_key} (24h è¿‡æœŸ)")
        
        # ğŸ†• é¢å¤–å­˜å‚¨å¢å¼ºæ•°æ®ï¼ˆå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰
        try:
            await _store_enhanced_interaction_data(target_date, micro_experiences, daily_schedule, r)
        except Exception as enhanced_error:
            logger.warning(f"å­˜å‚¨å¢å¼ºäº¤äº’æ•°æ®å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {enhanced_error}")
        
        return True

    except Exception as e:
        logger.error(f"æ”¶é›†äº¤äº’å¾®è§‚ç»å†å¤±è´¥: {str(e)}", exc_info=True)
        return False


async def generate_and_store_major_event(
    start_date: date, end_date: date, event_type: str
):
    """
    ç”Ÿæˆå¹¶å­˜å‚¨å¤§äº‹ä»¶ã€‚
    """
    start_date_str = start_date.strftime("%Y-%m-%d")
    end_date_str = end_date.strftime("%Y-%m-%d")
    duration_days = (end_date - start_date).days + 1
    logger.info(f"[major_event] å¼€å§‹ç”Ÿæˆå¤§äº‹ä»¶: {start_date_str} -> {end_date_str}")

    # 1. è·å–çœŸå®å¤©æ°”
    import random

    WORLD_CITIES = [
        (40.71, -74.01),
        (51.51, -0.13),
        (48.86, 2.35),
        (52.52, 13.41),
        (35.68, 139.76),
        (37.57, 126.98),
        (13.75, 100.50),
        (1.35, 103.82),
        (-33.87, 151.21),
        (55.75, 37.62),
        (30.05, 31.25),
        (-1.29, 36.82),
        (-23.55, -46.63),
        (-34.61, -58.38),
        (43.65, -79.38),
        (19.43, -99.13),
        (41.01, 28.97),
        (25.27, 55.30),
        (19.07, 72.88),
        (-36.85, 174.76),
    ]

    # ç»Ÿä¸€é€‰æ‹©ä¸€ä¸ªåœ°ç‚¹ï¼ˆçº¬åº¦, ç»åº¦ï¼‰ï¼Œå¹¶è½¬ä¸ºå­—ç¬¦ä¸²
    lat, lon = random.choice(WORLD_CITIES)
    selected_location = f"{lat:.2f},{lon:.2f}"
    logger.debug(f"[major_event] å¤©æ°”æ¨¡æ‹Ÿåœ°ç‚¹: {selected_location}")

    weather_forecast = {}
    for i in range(duration_days):
        current_date = start_date + timedelta(days=i)
        weather_forecast[current_date.strftime("%Y-%m-%d")] = get_weather_info(
            current_date.strftime("%Y-%m-%d"), location=selected_location
        )

    logger.debug(f"[major_event] æ¨¡æ‹Ÿå¤©æ°”é¢„æŠ¥: {weather_forecast}")

    # 2. è°ƒç”¨AIç”Ÿæˆå¤§äº‹ä»¶
    logger.debug("[major_event] è°ƒç”¨ AI ç”Ÿæˆå¤§äº‹ä»¶")
    major_event_data = await generate_major_event(
        duration_days=duration_days,
        event_type=event_type,
        start_date=start_date_str,
        weather_forecast=weather_forecast,
    )

    if "error" in major_event_data:
        logger.error(f"AIç”Ÿæˆå¤§äº‹ä»¶å¤±è´¥: {major_event_data['error']}")
        return None

    logger.debug("[major_event] AI å¤§äº‹ä»¶ç”ŸæˆæˆåŠŸ")

    # 3. å­˜å‚¨åˆ°æ•°æ®åº“
    logger.debug("[major_event] å­˜å‚¨å¤§äº‹ä»¶åˆ°æ•°æ®åº“")
    try:
        event_id = insert_major_event(
            start_date=start_date_str,
            end_date=end_date_str,
            duration_days=duration_days,
            main_content=major_event_data.get("main_objective", "æ— ä¸»è¦å†…å®¹"),
            daily_summaries=major_event_data.get("daily_plans", []),
            event_type=event_type,
            status="active",  # å‡è®¾ç”Ÿæˆåå³ä¸ºæ´»è·ƒçŠ¶æ€
        )
        logger.debug(f"[major_event] å¤§äº‹ä»¶å·²æ’å…¥ (ID: {event_id})")
        major_event_data["id"] = str(event_id)  # å°†æ•°æ®åº“ç”Ÿæˆçš„IDæ·»åŠ åˆ°æ•°æ®ä¸­
    except Exception as e:
        logger.error(f"å­˜å‚¨å¤§äº‹ä»¶åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return None

    # 4. å­˜å‚¨åˆ°æ–‡ä»¶
    logger.debug("[major_event] å­˜å‚¨å¤§äº‹ä»¶åˆ°æ–‡ä»¶")
    os.makedirs(GENERATED_CONTENT_DIR, exist_ok=True)
    file_path = os.path.join(
        GENERATED_CONTENT_DIR,
        f"major_event_{major_event_data.get('event_id', uuid.uuid4())}.json",
    )
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(major_event_data, f, ensure_ascii=False, indent=2)
        logger.debug(f"[major_event] å¤§äº‹ä»¶å·²ä¿å­˜åˆ°æ–‡ä»¶: {file_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜å¤§äº‹ä»¶åˆ°æ–‡ä»¶å¤±è´¥: {e}")

    logger.info("[major_event] å¤§äº‹ä»¶ç”Ÿæˆä¸å­˜å‚¨å®Œæˆ")
    return major_event_data


async def generate_and_store_micro_experiences(
    schedule_item: dict,
    current_date: date,
    schedule_id: str,
    previous_experiences: Optional[list] = None,
    major_event_context: Optional[dict] = None,
):
    """
    ä¸ºå•ä¸ªæ—¥ç¨‹é¡¹ç›®ç”Ÿæˆå¹¶å­˜å‚¨å¤šä¸ªå¾®è§‚ç»å†é¡¹ï¼ˆ5-30åˆ†é’Ÿé¢—ç²’åº¦ï¼‰
    """
    logger.info(
        f"[micro_exp] å¼€å§‹ä¸ºæ—¥ç¨‹é¡¹ç”Ÿæˆå¾®è§‚ç»å†: {schedule_item.get('title', 'æœªçŸ¥é¡¹ç›®')}"
    )

    # 1. è°ƒç”¨AIç”Ÿæˆå¤šä¸ªå¾®è§‚ç»å†é¡¹
    logger.debug("[micro_exp] è°ƒç”¨ AI ç”Ÿæˆå¾®è§‚ç»å†ï¼ˆ5-30 åˆ†é’Ÿï¼‰")
    micro_experiences = await generate_micro_experiences(
        schedule_item=schedule_item,
        current_date=current_date.strftime("%Y-%m-%d"),
        previous_experiences=previous_experiences,
        major_event_context=major_event_context,
    )

    if not micro_experiences or any("error" in exp for exp in micro_experiences):
        errors = [exp["error"] for exp in micro_experiences if "error" in exp]
        logger.error(f"AIç”Ÿæˆå¾®è§‚ç»å†å¤±è´¥: {', '.join(errors)}")
        return None

    logger.debug(f"[micro_exp] AI ç”ŸæˆæˆåŠŸï¼Œæ•°é‡: {len(micro_experiences)}")

    # 2. å­˜å‚¨åˆ°æ•°æ®åº“
    logger.debug("[micro_exp] å­˜å‚¨å¾®è§‚ç»å†é¡¹åˆ°æ•°æ®åº“")
    try:
        experience_id = insert_micro_experience(
            date=current_date.strftime("%Y-%m-%d"),
            daily_schedule_id=schedule_id,
            related_item_id=schedule_item.get("id"),
            experiences=micro_experiences,
        )
        logger.debug(f"[micro_exp] å¾®è§‚ç»å†å·²å­˜å‚¨ (ID: {experience_id})")
        successful_items = len(micro_experiences)
    except Exception as e:
        logger.error(f"å­˜å‚¨å¾®è§‚ç»å†å¤±è´¥: {e}")
        successful_items = 0


    # 3. å­˜å‚¨åˆ°æ–‡ä»¶
    logger.debug("[micro_exp] å­˜å‚¨å¾®è§‚ç»å†åˆ°æ–‡ä»¶")
    os.makedirs(GENERATED_CONTENT_DIR, exist_ok=True)
    title = schedule_item.get("title", "unknown").replace(" ", "_")
    date_str = current_date.strftime("%Y-%m-%d")
    file_path = os.path.join(
        GENERATED_CONTENT_DIR, f"micro_experiences_{date_str}_{title}.json"
    )
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "date": current_date.strftime("%Y-%m-%d"),
                    "schedule_item_id": schedule_item.get("id", ""),
                    "items": micro_experiences,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        logger.debug(f"[micro_exp] å¾®è§‚ç»å†é¡¹å·²ä¿å­˜åˆ°æ–‡ä»¶: {file_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜å¾®è§‚ç»å†é¡¹åˆ°æ–‡ä»¶å¤±è´¥: {e}")

    logger.info("[micro_exp] å¾®è§‚ç»å†é¡¹ç”Ÿæˆä¸å­˜å‚¨å®Œæˆ")
    return micro_experiences


# async def get_and_summarize_experiences(
#     daily_schedule_id: str, summary_type: str = "æ•´ä½“"
# ):
#     """
#     ä»æ•°æ®åº“è·å–å¾®è§‚ç»å†å¹¶è¿›è¡Œæ€»ç»“ã€‚
#     """
#     logger.info(f"--- æ­£åœ¨è·å–å¹¶æ€»ç»“æ¯æ—¥è®¡åˆ’ {daily_schedule_id} çš„å¾®è§‚ç»å† ---")
#     experiences = get_micro_experiences_by_daily_schedule_id(daily_schedule_id)
#     if not experiences:
#         logger.info("æ²¡æœ‰æ‰¾åˆ°å¾®è§‚ç»å†ã€‚")
#         return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å¾®è§‚ç»å†ã€‚"

#     logger.info(f"æ‰¾åˆ° {len(experiences)} æ¡å¾®è§‚ç»å†ï¼Œæ­£åœ¨æ€»ç»“...")
#     summary = await summarize_experiences(experiences, summary_type)
#     logger.info("æ€»ç»“å®Œæˆã€‚")
#     return summary


# ç¤ºä¾‹ç”¨æ³• (åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›ä¼šé€šè¿‡APIæˆ–è°ƒåº¦å™¨è§¦å‘)
class LifeSystemQuery:
    def __init__(self, target_date: Optional[date] = None):
        self.target_date = target_date if target_date else date.today()
        self.date_str = self.target_date.strftime("%Y-%m-%d")

    async def is_in_major_event(self) -> bool:
        major_event = get_major_event_by_date(self.date_str)
        return major_event is not None

    async def get_major_event_info(self) -> Optional[dict]:
        return get_major_event_by_date(self.date_str)

    async def get_major_event_daily_info(self) -> Optional[dict]:
        major_event = await self.get_major_event_info()
        if major_event and "daily_summaries" in major_event:
            # daily_summaries å­—æ®µæ˜¯ä¸€ä¸ªJSONBç±»å‹ï¼Œå­˜å‚¨çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ—¥æœŸå’Œå†…å®¹
            # éå† daily_summaries åˆ—è¡¨ï¼ŒæŸ¥æ‰¾ä¸å½“å‰æ—¥æœŸåŒ¹é…çš„æ¯æ—¥æ‘˜è¦
            for daily_summary in major_event["daily_summaries"]:
                if daily_summary.get("date") == self.date_str:
                    return daily_summary
        return None

    async def get_daily_schedule_info(self) -> Optional[dict]:
        return get_daily_schedule_by_date(self.date_str)

    async def get_schedule_item_at_time(self, target_time: str) -> Optional[dict]:
        daily_schedule = await self.get_daily_schedule_info()
        if not daily_schedule:
            logger.debug("No daily schedule found.")
            return None

        if not (
            "schedule_data" in daily_schedule
            and "schedule_items" in daily_schedule["schedule_data"]
        ):
            logger.debug("Daily schedule has no'schedule_data' or 'schedule_items'.")
            return None

        try:
            target_time_obj = datetime.strptime(target_time, "%H:%M").time()
        except ValueError:
            logger.error(f"Invalid target_time format: {target_time}")
            return None

        for item in daily_schedule["schedule_data"]["schedule_items"]:
            item_start_time_str = item.get("start_time")
            item_end_time_str = item.get("end_time")

            if not (item_start_time_str and item_end_time_str):
                logger.warning(f"Schedule item missing start_time or end_time: {item}")
                continue

            try:
                item_start_time_obj = datetime.strptime(
                    item_start_time_str, "%H:%M"
                ).time()
                item_end_time_obj = datetime.strptime(item_end_time_str, "%H:%M").time()
            except ValueError:
                logger.error(f"Invalid time format in schedule item: {item}")
                continue

            if item_start_time_obj <= target_time_obj < item_end_time_obj:
                logger.debug(f"Matched schedule item: {item.get('title')}")
                return item

        logger.debug("No matching schedule item found for current time.")
        return None

    async def get_micro_experiences_for_schedule_item(
        self, schedule_item_id: str
    ) -> Optional[list]:
        return get_micro_experiences_by_related_item_id(schedule_item_id)

    async def get_micro_experience_at_time(
        self, schedule_item_id: str, target_time: str
    ) -> Optional[dict]:
        micro_experiences_list = await self.get_micro_experiences_for_schedule_item(
            schedule_item_id
        )
        if not micro_experiences_list:
            return None

        try:
            target_time_obj = datetime.strptime(target_time, "%H:%M").time()
        except ValueError:
            logger.error(f"Invalid target_time format: {target_time}")
            return None

        for record in micro_experiences_list:
            experiences_in_record = record.get("experiences", [])
            if not experiences_in_record:
                continue

            for exp in experiences_in_record:
                exp_start_time_str = exp.get("start_time")
                exp_end_time_str = exp.get("end_time")

                if not (exp_start_time_str and exp_end_time_str):
                    logger.warning(
                        f"Micro experience item missing start_time or end_time: {exp}"
                    )
                    continue

                try:
                    exp_start_time_obj = datetime.strptime(
                        exp_start_time_str, "%H:%M"
                    ).time()
                    exp_end_time_obj = datetime.strptime(
                        exp_end_time_str, "%H:%M"
                    ).time()
                except ValueError:
                    logger.error(f"Invalid time format in micro experience item: {exp}")
                    continue

                if exp_start_time_obj <= target_time_obj < exp_end_time_obj:
                    logger.debug(f"Matched micro experience: {exp.get('content')}")
                    return exp

        logger.debug("No matching micro experience found for current time.")
        return None


# ç¤ºä¾‹ç”¨æ³• (åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›ä¼šé€šè¿‡APIæˆ–è°ƒåº¦å™¨è§¦å‘)
async def main(target_date: date = None):
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼ŒåŒ…å«å¼‚å¸¸å¤„ç†å’Œæ—¥æœŸå‚æ•°"""
    target_date = target_date or date.today()

    try:
        logger.info(f"[main] å¼€å§‹ç”Ÿæˆæ—¥ç¨‹ç³»ç»Ÿ: {target_date}")

        # ç”Ÿæˆä¸»æ—¥ç¨‹
        await generate_and_store_daily_life(target_date)

        # ç¤ºä¾‹æŸ¥è¯¢åŠŸèƒ½éªŒè¯
        logger.debug("[main] éªŒè¯ç³»ç»ŸæŸ¥è¯¢åŠŸèƒ½")
        query = LifeSystemQuery(target_date)
        print(f"\n{target_date} æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­: {await query.is_in_major_event()}")
        print(f"å½“æ—¥æ—¥ç¨‹æ‘˜è¦: {await query.get_daily_schedule_info() or 'æ— æ—¥ç¨‹'}")

    except Exception as e:
        logger.critical(f"â€¼ï¸ ä¸»æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        raise

    print("\n--- LifeSystemQuery ç¤ºä¾‹ ---")
    query_today = LifeSystemQuery()
    print(
        f"ä»Šå¤© ({query_today.date_str}) æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­: {await query_today.is_in_major_event()}"
    )
    print(
        f"ä»Šå¤© ({query_today.date_str}) çš„å¤§äº‹ä»¶ä¿¡æ¯: {await query_today.get_major_event_info()}"
    )
    print(
        f"ä»Šå¤© ({query_today.date_str}) çš„æ—¥ç¨‹ä¿¡æ¯: {await query_today.get_daily_schedule_info()}"
    )

    # å‡è®¾æœ‰ä¸€ä¸ªæ—¥ç¨‹é¡¹ID
    # example_schedule_item_id = "some_uuid_from_db"
    # print(f"æ—¥ç¨‹é¡¹ {example_schedule_item_id} çš„å¾®è§‚ç»å†: {await query_today.get_micro_experiences_for_schedule_item(example_schedule_item_id)}")
    # print(f"æ—¥ç¨‹é¡¹ {example_schedule_item_id} åœ¨ 10:00 çš„å¾®è§‚ç»å†: {await query_today.get_micro_experience_at_time(example_schedule_item_id, '10:00')}")


if __name__ == "__main__":
    import argparse
    from datetime import datetime

    parser = argparse.ArgumentParser(description="å¾·å·ç”Ÿæ´»ç³»ç»Ÿç”Ÿæˆå™¨")
    parser.add_argument(
        "--date", type=str, help="æŒ‡å®šç”Ÿæˆæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)", default=None
    )
    args = parser.parse_args()

    target_date = date.today()
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            logger.error(f"æ— æ•ˆæ—¥æœŸæ ¼å¼: {args.date}, ä½¿ç”¨ä»Šæ—¥æ—¥æœŸ")
            target_date = date.today()

    logger.debug(f"[main] æ‰§è¡Œæ—¥æœŸ: {target_date}")
    asyncio.run(main(target_date))
