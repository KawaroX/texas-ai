import asyncio
import json
import os
from datetime import date, timedelta
import logging
import uuid
from typing import Optional
from datetime import datetime, date
import redis  # æ·»åŠ  Redis æ”¯æŒ

logger = logging.getLogger(__name__)

from services.ai_service import (
    get_weather_info,
    generate_daily_schedule,
    generate_major_event,
    generate_micro_experiences,
)
from utils.postgres_service import (
    insert_daily_schedule,
    get_daily_schedule_by_date,
    update_daily_schedule,
    delete_daily_schedule,
    insert_major_event,
    get_major_event_by_id,
    get_major_event_by_date,  # æ–°å¢
    update_major_event,
    delete_major_event,
    insert_micro_experience,
    get_micro_experiences_by_daily_schedule_id,
    get_micro_experiences_by_related_item_id,  # æ–°å¢
    delete_micro_experience,
)

# å®šä¹‰ç”Ÿæˆå†…å®¹å­˜å‚¨çš„æ–‡ä»¶å¤¹
GENERATED_CONTENT_DIR = "generated_content"


async def generate_and_store_daily_life(target_date: date):
    """
    ç”Ÿæˆå¹¶å­˜å‚¨æŒ‡å®šæ—¥æœŸçš„å¾·å…‹è¨æ–¯ç”Ÿæ´»æ—¥ç¨‹ã€‚
    åŒ…æ‹¬è·å–å¤©æ°”ã€ç”Ÿæˆæ—¥ç¨‹ã€å­˜å‚¨åˆ°æ•°æ®åº“å’Œæ–‡ä»¶ã€‚
    """
    date_str = target_date.strftime("%Y-%m-%d")
    print(f"--- æ­£åœ¨ä¸º {date_str} ç”Ÿæˆæ¯æ—¥æ—¥ç¨‹ ---")

    # 1. è·å–å¤©æ°”ä¿¡æ¯
    weather = get_weather_info(date_str)
    print(f"å¤©æ°”ä¿¡æ¯: {weather}")

    # 2. åˆ¤æ–­å·¥ä½œæ—¥/å‘¨æœ« (ç®€åŒ–é€»è¾‘ï¼Œå®é™…å¯æ ¹æ®èŠ‚å‡æ—¥ç­‰æ›´å¤æ‚åˆ¤æ–­)
    day_type = "weekend" if target_date.weekday() >= 5 else "weekday"
    print(f"æ—¥æœŸç±»å‹: {day_type}")

    # 3. æ£€æŸ¥å¤§äº‹ä»¶
    is_in_major_event = False
    major_event_context = None
    print(f"ğŸ” æ£€æŸ¥ {date_str} æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­...")

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨åŒ…å«ç›®æ ‡æ—¥æœŸçš„å¤§äº‹ä»¶
    major_event_context = get_major_event_by_date(date_str)

    if major_event_context:
        print(
            f"âœ… æ£€æµ‹åˆ°å·²å­˜åœ¨å¤§äº‹ä»¶: {major_event_context.get('event_title', 'æœªçŸ¥äº‹ä»¶')}"
        )
        is_in_major_event = True
    else:
        print("â„¹ï¸ æœªæ£€æµ‹åˆ°å·²å­˜åœ¨çš„å¤§äº‹ä»¶")

    # å¦‚æœæ²¡æœ‰å¤§äº‹ä»¶ï¼Œåˆ™æ ¹æ®0.028æ¦‚ç‡å†³å®šæ˜¯å¦ç”Ÿæˆæ–°çš„å¤§äº‹ä»¶
    if not is_in_major_event:
        import random
        from collections import Counter

        gen_prob = 0.028  # 0.028
        rand_val = random.random()
        logger.info(
            f"ğŸ² æ£€æŸ¥æ˜¯å¦ç”Ÿæˆæ–°å¤§äº‹ä»¶: æ¦‚ç‡={gen_prob*100}%, éšæœºå€¼={rand_val:.4f}"
        )

        if rand_val < gen_prob:  # 0.028æ¦‚ç‡ç”Ÿæˆå¤§äº‹ä»¶
            # æ­£æ€åˆ†å¸ƒç”ŸæˆæŒç»­å¤©æ•° (Î¼=4, Ïƒ=2)ï¼ŒèŒƒå›´1-7å¤©
            results = []
            for _ in range(1000):
                val = max(1, min(7, int(random.gauss(4, 2))))
                results.append(val)

            logger.info(f"éšæœº1000æ¬¡ç»“æœï¼š{Counter(results)}\n\n")

            duration_days = max(1, min(7, int(random.gauss(4, 2))))
            logger.info(f"ğŸ“… ç”Ÿæˆå¤§äº‹ä»¶æŒç»­å¤©æ•°: {duration_days}å¤© (æ­£æ€åˆ†å¸ƒ Î¼=4, Ïƒ=2)")

            # éšæœºé€‰æ‹©äº‹ä»¶ç±»å‹
            event_types = ["å‡ºå·®ä»»åŠ¡", "ç‰¹æ®Šå¿«é€’", "åŸ¹è®­å­¦ä¹ ", "ä¸ªäººäº‹åŠ¡"]
            weights = [0.4, 0.3, 0.15, 0.15]  # äº‹ä»¶ç±»å‹æ¦‚ç‡æƒé‡
            event_type = random.choices(event_types, weights=weights)[0]
            logger.info(f"ğŸ“Œ é€‰æ‹©äº‹ä»¶ç±»å‹: {event_type} (æƒé‡: {weights})")

            # ç”Ÿæˆå¤§äº‹ä»¶
            end_date = target_date + timedelta(days=duration_days - 1)
            logger.info(
                f"ğŸ›« ç”Ÿæˆå¤§äº‹ä»¶: {event_type}, ä» {date_str} åˆ° {end_date.strftime('%Y-%m-%d')}"
            )
            major_event_context = await generate_and_store_major_event(
                target_date, end_date, event_type
            )
            is_in_major_event = True
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆæ–°å¤§äº‹ä»¶: {event_type}, æŒç»­{duration_days}å¤©")

    # å¦‚æœå¤„äºå¤§äº‹ä»¶ä¸­ï¼Œä½†æœªè·å–ä¸Šä¸‹æ–‡ï¼Œå°è¯•ä»æ•°æ®åº“è·å–
    if is_in_major_event and not major_event_context:
        logger.warning("âš ï¸ å¤§äº‹ä»¶ä¸Šä¸‹æ–‡ç¼ºå¤±ï¼Œå°è¯•ä»æ•°æ®åº“è·å–...")
        major_event_context = get_major_event_by_date(date_str)
        if not major_event_context:
            logger.warning("âŒ æ•°æ®åº“ä¸­ä¹Ÿæœªæ‰¾åˆ°å¤§äº‹ä»¶è¯¦æƒ…ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            major_event_context = {
                "event_title": "é»˜è®¤å¤§äº‹ä»¶",
                "event_type": "é»˜è®¤ç±»å‹",
                "main_objective": "é»˜è®¤ç›®æ ‡",
            }
    if is_in_major_event:
        logger.info(
            f"â„¹ï¸ å¤§äº‹ä»¶çŠ¶æ€: 'å­˜åœ¨', ç±»å‹: {major_event_context.get('event_type', 'æ— ')}"
        )
    else:
        logger.info("â„¹ï¸ å¤§äº‹ä»¶çŠ¶æ€: 'ä¸å­˜åœ¨'")

    # 4. è°ƒç”¨AIç”Ÿæˆæ¯æ—¥æ—¥ç¨‹
    logger.info("æ­£åœ¨è°ƒç”¨AIç”Ÿæˆæ¯æ—¥æ—¥ç¨‹...")
    daily_schedule_data = await generate_daily_schedule(
        date=date_str,
        day_type=day_type,
        weather=weather,
        is_in_major_event=is_in_major_event,
        major_event_context=major_event_context,
        special_flags=[],
    )

    if "error" in daily_schedule_data:
        logger.error(f"âŒ AIç”Ÿæˆæ—¥ç¨‹å¤±è´¥: {daily_schedule_data['error']}")
        return None

    logger.info("âœ… AIæ—¥ç¨‹ç”ŸæˆæˆåŠŸã€‚")

    # 5. å­˜å‚¨åˆ°æ•°æ®åº“
    logger.info("æ­£åœ¨å­˜å‚¨æ—¥ç¨‹åˆ°æ•°æ®åº“...")
    try:
        # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦å·²å­˜åœ¨æ—¥ç¨‹ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™æ’å…¥
        existing_schedule = get_daily_schedule_by_date(date_str)
        if existing_schedule:
            schedule_id = existing_schedule["id"]
            update_daily_schedule(
                schedule_id=schedule_id,
                schedule_data=daily_schedule_data,
                weather=weather,
                is_in_major_event=is_in_major_event,
                major_event_id=(
                    major_event_context["id"] if major_event_context else None
                ),
            )
            logger.info(f"âœ… æ—¥ç¨‹å·²æ›´æ–° (ID: {schedule_id})")
        else:
            schedule_id = insert_daily_schedule(
                date=date_str,
                schedule_data=daily_schedule_data,
                weather=weather,
                is_in_major_event=is_in_major_event,
                major_event_id=(
                    major_event_context["id"] if major_event_context else None
                ),
            )
            logger.info(f"âœ… æ—¥ç¨‹å·²æ’å…¥ (ID: {schedule_id})")

        daily_schedule_data["id"] = str(schedule_id)  # å°†æ•°æ®åº“ç”Ÿæˆçš„IDæ·»åŠ åˆ°æ•°æ®ä¸­
    except Exception as e:
        logger.error(f"âŒ å­˜å‚¨æ—¥ç¨‹åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return None

    # 6. å­˜å‚¨åˆ°æ–‡ä»¶
    logger.info("æ­£åœ¨å­˜å‚¨æ—¥ç¨‹åˆ°æ–‡ä»¶...")
    os.makedirs(GENERATED_CONTENT_DIR, exist_ok=True)
    file_path = os.path.join(GENERATED_CONTENT_DIR, f"daily_schedule_{date_str}.json")
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(daily_schedule_data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… æ—¥ç¨‹å·²ä¿å­˜åˆ°æ–‡ä»¶: {file_path}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ—¥ç¨‹åˆ°æ–‡ä»¶å¤±è´¥: {e}")

    # 7. ç”Ÿæˆå¹¶å­˜å‚¨å¾®è§‚ç»å†
    if "schedule_items" in daily_schedule_data:
        logger.info(
            f"å¼€å§‹ç”Ÿæˆå¾®è§‚ç»å†ï¼Œå…± {len(daily_schedule_data['schedule_items'])} ä¸ªé¡¹ç›®..."
        )
        successful_experiences = 0

        # è·å–ä¹‹å‰çš„ç»å†æ‘˜è¦ï¼ˆç®€åŒ–å®ç°ï¼‰
        previous_experiences_summary = None

        for index, item in enumerate(daily_schedule_data["schedule_items"]):
            # è®¾ç½®å½“å‰æ—¶é—´ä¸ºé¡¹ç›®å¼€å§‹æ—¶é—´
            current_time = item["start_time"]

            logger.info(
                f"ç”Ÿæˆå¾®è§‚ç»å†é¡¹ [{index+1}/{len(daily_schedule_data['schedule_items'])}]: {item['title']}"
            )
            micro_experiences = await generate_and_store_micro_experiences(
                schedule_item=item,
                current_date=target_date,
                previous_experiences=previous_experiences_summary,
                major_event_context=major_event_context,
                schedule_id=schedule_id,  # ä¼ å…¥æ¯æ—¥è®¡åˆ’çš„ID
            )

            if micro_experiences:
                successful_experiences += 1
                # æ›´æ–°ç»å†æ‘˜è¦ï¼ˆä½¿ç”¨ç”Ÿæˆçš„ç»å†å†…å®¹ï¼‰
                exp_summaries = [
                    f"{exp.get('start_time', '')}-{exp.get('end_time', '')}: {exp.get('content', '')[:50]}..."
                    for exp in micro_experiences
                ]
                previous_experiences_summary = exp_summaries

        logger.info(
            f"âœ… å¾®è§‚ç»å†ç”Ÿæˆå®Œæˆ: {successful_experiences}/{len(daily_schedule_data['schedule_items'])} æˆåŠŸ"
        )
    else:
        logger.warning("âš ï¸ æ—¥ç¨‹ä¸­æ²¡æœ‰å¯ç”Ÿæˆå¾®è§‚ç»å†çš„é¡¹ç›®")

    logger.info(f"--- {date_str} æ¯æ—¥æ—¥ç¨‹ç”Ÿæˆä¸å­˜å‚¨å®Œæˆ ---")

    # 8. ä½¿ç”¨ä¸“ç”¨å‡½æ•°æ”¶é›†éœ€è¦äº¤äº’çš„å¾®è§‚ç»å†
    logger.info("å¼€å§‹æ”¶é›†éœ€è¦ä¸»åŠ¨äº¤äº’çš„å¾®è§‚ç»å†...")
    await collect_interaction_experiences(target_date)

    return daily_schedule_data


async def collect_interaction_experiences(target_date: date):
    """
    å•ç‹¬æ”¶é›†éœ€è¦äº¤äº’çš„å¾®è§‚ç»å†å¹¶å­˜å…¥Redis
    """
    date_str = target_date.strftime("%Y-%m-%d")
    logger.info(f"--- å¼€å§‹å•ç‹¬æ”¶é›† {date_str} éœ€è¦ä¸»åŠ¨äº¤äº’çš„å¾®è§‚ç»å† ---")

    try:
        # ä»æ•°æ®åº“è·å–å½“æ—¥æ—¥ç¨‹ ID
        daily_schedule = get_daily_schedule_by_date(date_str)
        if not daily_schedule:
            logger.warning(f"æœªæ‰¾åˆ° {date_str} çš„æ—¥ç¨‹æ•°æ®")
            return False

        schedule_id = daily_schedule["id"]

        # æŸ¥è¯¢å…³è”çš„å¾®è§‚ç»å†
        micro_experiences = get_micro_experiences_by_daily_schedule_id(schedule_id)
        if not micro_experiences:
            logger.info("å½“æ—¥æ²¡æœ‰å¾®è§‚ç»å†æ•°æ®")
            return False

        # ç­›é€‰éœ€è¦äº¤äº’çš„æ¡ç›®
        interaction_needed = []
        for record in micro_experiences:
            experiences = record.get("experiences", [])
            for exp in experiences:
                if exp.get("need_interaction") is True:
                    interaction_needed.append(exp)

        logger.info(f"æ‰¾åˆ° {len(interaction_needed)} æ¡éœ€è¦äº¤äº’çš„å¾®è§‚ç»å†")

        # å­˜å‚¨åˆ° Redis
        r = redis.Redis.from_url(os.getenv("REDIS_URL"))
        redis_key = f"interaction_needed:{date_str}"

        # è¾…åŠ©å‡½æ•°ï¼šå°† HH:MM æ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©çš„ Unix æ—¶é—´æˆ³
        def time_to_timestamp(date_obj: date, time_str: str) -> float:
            dt_str = f"{date_obj.strftime('%Y-%m-%d')} {time_str}"
            dt_obj = datetime.strptime(dt_str, "%Y-%m-%d %H:%M")
            return dt_obj.timestamp()

        # å­˜å‚¨æ–°æ•°æ®åˆ° Sorted Set
        # ä½¿ç”¨ end_time çš„ Unix æ—¶é—´æˆ³ä½œä¸º score
        # å¦‚æœ Sorted Set ä¸­å·²å­˜åœ¨ç›¸åŒçš„ memberï¼Œzadd ä¼šæ›´æ–°å…¶ score
        # å¦‚æœæ¯å¤©ç”Ÿæˆæ–°çš„ keyï¼Œåˆ™ä¸éœ€è¦åˆ é™¤æ—§æ•°æ®
        for exp in interaction_needed:
            try:
                score = time_to_timestamp(target_date, exp["start_time"])
                r.zadd(redis_key, {json.dumps(exp, ensure_ascii=False): score})
            except KeyError as ke:
                logger.error(f"âš ï¸ ç¼ºå°‘æ—¶é—´å­—æ®µï¼Œæ— æ³•æ·»åŠ åˆ° Sorted Set: {exp} - {ke}")
            except Exception as add_e:
                logger.error(f"âŒ æ·»åŠ åˆ° Redis Sorted Set å¤±è´¥: {exp} - {add_e}")

        # è®¾ç½® 24 å°æ—¶è¿‡æœŸ
        r.expire(redis_key, 86400)
        logger.info(f"å·²å­˜å‚¨åˆ° Redis Sorted Set é”®: {redis_key} (24å°æ—¶è¿‡æœŸ)")
        return True

    except Exception as e:
        logger.error(f"æ”¶é›†äº¤äº’å¾®è§‚ç»å†å¤±è´¥: {str(e)}", exc_info=True)
        return False


async def generate_and_store_major_event(
    start_date: date, end_date: date, event_type: str
):
    """
    ç”Ÿæˆå¹¶å­˜å‚¨å¤§äº‹ä»¶ã€‚
    """
    start_date_str = start_date.strftime("%Y-%m-%d")
    end_date_str = end_date.strftime("%Y-%m-%d")
    duration_days = (end_date - start_date).days + 1
    logger.info(f"--- æ­£åœ¨ç”Ÿæˆå¤§äº‹ä»¶ ({start_date_str} è‡³ {end_date_str}) ---")

    # 1. æ¨¡æ‹Ÿå¤©æ°”é¢„æŠ¥ (å®é™…åº”è·å–çœŸå®å¤©æ°”)
    weather_forecast = {}
    for i in range(duration_days):
        current_date = start_date + timedelta(days=i)
        weather_forecast[current_date.strftime("%Y-%m-%d")] = get_weather_info(
            current_date.strftime("%Y-%m-%d")
        )
    logger.info(f"æ¨¡æ‹Ÿå¤©æ°”é¢„æŠ¥: {weather_forecast}")

    # 2. è°ƒç”¨AIç”Ÿæˆå¤§äº‹ä»¶
    logger.info("æ­£åœ¨è°ƒç”¨AIç”Ÿæˆå¤§äº‹ä»¶...")
    major_event_data = await generate_major_event(
        duration_days=duration_days,
        event_type=event_type,
        start_date=start_date_str,
        weather_forecast=weather_forecast,
    )

    if "error" in major_event_data:
        logger.error(f"âŒ AIç”Ÿæˆå¤§äº‹ä»¶å¤±è´¥: {major_event_data['error']}")
        return None

    logger.info("âœ… AIå¤§äº‹ä»¶ç”ŸæˆæˆåŠŸã€‚")

    # 3. å­˜å‚¨åˆ°æ•°æ®åº“
    logger.info("æ­£åœ¨å­˜å‚¨å¤§äº‹ä»¶åˆ°æ•°æ®åº“...")
    try:
        event_id = insert_major_event(
            start_date=start_date_str,
            end_date=end_date_str,
            duration_days=duration_days,
            main_content=major_event_data.get("main_objective", "æ— ä¸»è¦å†…å®¹"),
            daily_summaries=major_event_data.get("daily_plans", []),
            event_type=event_type,
            status="active",  # å‡è®¾ç”Ÿæˆåå³ä¸ºæ´»è·ƒçŠ¶æ€
        )
        logger.info(f"âœ… å¤§äº‹ä»¶å·²æ’å…¥ (ID: {event_id})")
        major_event_data["id"] = str(event_id)  # å°†æ•°æ®åº“ç”Ÿæˆçš„IDæ·»åŠ åˆ°æ•°æ®ä¸­
    except Exception as e:
        logger.error(f"âŒ å­˜å‚¨å¤§äº‹ä»¶åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return None

    # 4. å­˜å‚¨åˆ°æ–‡ä»¶
    logger.info("æ­£åœ¨å­˜å‚¨å¤§äº‹ä»¶åˆ°æ–‡ä»¶...")
    os.makedirs(GENERATED_CONTENT_DIR, exist_ok=True)
    file_path = os.path.join(
        GENERATED_CONTENT_DIR,
        f"major_event_{major_event_data.get('event_id', uuid.uuid4())}.json",
    )
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(major_event_data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… å¤§äº‹ä»¶å·²ä¿å­˜åˆ°æ–‡ä»¶: {file_path}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å¤§äº‹ä»¶åˆ°æ–‡ä»¶å¤±è´¥: {e}")

    logger.info(f"--- å¤§äº‹ä»¶ç”Ÿæˆä¸å­˜å‚¨å®Œæˆ ---")
    return major_event_data


async def generate_and_store_micro_experiences(
    schedule_item: dict,
    current_date: date,
    schedule_id: str,
    previous_experiences: Optional[list] = None,
    major_event_context: Optional[dict] = None,
):
    """
    ä¸ºå•ä¸ªæ—¥ç¨‹é¡¹ç›®ç”Ÿæˆå¹¶å­˜å‚¨å¤šä¸ªå¾®è§‚ç»å†é¡¹ï¼ˆ5-30åˆ†é’Ÿé¢—ç²’åº¦ï¼‰
    """
    logger.info(
        f"--- æ­£åœ¨ä¸ºæ—¥ç¨‹é¡¹ç›® '{schedule_item.get('title', 'æœªçŸ¥é¡¹ç›®')}' ç”Ÿæˆå¾®è§‚ç»å†é¡¹ ---"
    )

    # 1. è°ƒç”¨AIç”Ÿæˆå¤šä¸ªå¾®è§‚ç»å†é¡¹
    logger.info("æ­£åœ¨è°ƒç”¨AIç”Ÿæˆå¾®è§‚ç»å†é¡¹ï¼ˆ5-30åˆ†é’Ÿé¢—ç²’åº¦ï¼‰...")
    micro_experiences = await generate_micro_experiences(
        schedule_item=schedule_item,
        current_date=current_date.strftime("%Y-%m-%d"),
        previous_experiences=previous_experiences,
        major_event_context=major_event_context,
    )

    if not micro_experiences or any("error" in exp for exp in micro_experiences):
        errors = [exp["error"] for exp in micro_experiences if "error" in exp]
        logger.error(f"âŒ AIç”Ÿæˆå¾®è§‚ç»å†å¤±è´¥: {', '.join(errors)}")
        return None

    logger.info(f"âœ… AIç”ŸæˆæˆåŠŸï¼Œå…± {len(micro_experiences)} ä¸ªå¾®è§‚ç»å†é¡¹")

    # 2. å­˜å‚¨åˆ°æ•°æ®åº“
    logger.info("æ­£åœ¨å­˜å‚¨å¾®è§‚ç»å†é¡¹åˆ°æ•°æ®åº“...")
    try:
        experience_id = insert_micro_experience(
            date=current_date.strftime("%Y-%m-%d"),
            daily_schedule_id=schedule_id,
            related_item_id=schedule_item.get("id"),
            experiences=micro_experiences,
        )
        logger.info(f"âœ… å¾®è§‚ç»å†å·²å­˜å‚¨ (ID: {experience_id})")
        successful_items = len(micro_experiences)
    except Exception as e:
        logger.error(f"âŒ å­˜å‚¨å¾®è§‚ç»å†å¤±è´¥: {e}")
        successful_items = 0

    logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {successful_items}/{len(micro_experiences)} ä¸ªå¾®è§‚ç»å†é¡¹")

    # 3. å­˜å‚¨åˆ°æ–‡ä»¶
    logger.info("æ­£åœ¨å­˜å‚¨å¾®è§‚ç»å†åˆ°æ–‡ä»¶...")
    os.makedirs(GENERATED_CONTENT_DIR, exist_ok=True)
    title = schedule_item.get("title", "unknown").replace(" ", "_")
    date_str = current_date.strftime("%Y-%m-%d")
    file_path = os.path.join(
        GENERATED_CONTENT_DIR, f"micro_experiences_{date_str}_{title}.json"
    )
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "date": current_date.strftime("%Y-%m-%d"),
                    "schedule_item_id": schedule_item.get("id", ""),
                    "items": micro_experiences,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        logger.info(f"âœ… å¾®è§‚ç»å†é¡¹å·²ä¿å­˜åˆ°æ–‡ä»¶: {file_path}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å¾®è§‚ç»å†é¡¹åˆ°æ–‡ä»¶å¤±è´¥: {e}")

    logger.info(f"--- å¾®è§‚ç»å†é¡¹ç”Ÿæˆä¸å­˜å‚¨å®Œæˆ ---")
    return micro_experiences


# async def get_and_summarize_experiences(
#     daily_schedule_id: str, summary_type: str = "æ•´ä½“"
# ):
#     """
#     ä»æ•°æ®åº“è·å–å¾®è§‚ç»å†å¹¶è¿›è¡Œæ€»ç»“ã€‚
#     """
#     logger.info(f"--- æ­£åœ¨è·å–å¹¶æ€»ç»“æ¯æ—¥è®¡åˆ’ {daily_schedule_id} çš„å¾®è§‚ç»å† ---")
#     experiences = get_micro_experiences_by_daily_schedule_id(daily_schedule_id)
#     if not experiences:
#         logger.info("æ²¡æœ‰æ‰¾åˆ°å¾®è§‚ç»å†ã€‚")
#         return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å¾®è§‚ç»å†ã€‚"

#     logger.info(f"æ‰¾åˆ° {len(experiences)} æ¡å¾®è§‚ç»å†ï¼Œæ­£åœ¨æ€»ç»“...")
#     summary = await summarize_experiences(experiences, summary_type)
#     logger.info("âœ… æ€»ç»“å®Œæˆã€‚")
#     return summary


# ç¤ºä¾‹ç”¨æ³• (åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›ä¼šé€šè¿‡APIæˆ–è°ƒåº¦å™¨è§¦å‘)
class LifeSystemQuery:
    def __init__(self, target_date: Optional[date] = None):
        self.target_date = target_date if target_date else date.today()
        self.date_str = self.target_date.strftime("%Y-%m-%d")

    async def is_in_major_event(self) -> bool:
        major_event = get_major_event_by_date(self.date_str)
        return major_event is not None

    async def get_major_event_info(self) -> Optional[dict]:
        return get_major_event_by_date(self.date_str)

    async def get_major_event_daily_info(self) -> Optional[dict]:
        major_event = await self.get_major_event_info()
        if major_event and "daily_summaries" in major_event:
            # daily_summaries å­—æ®µæ˜¯ä¸€ä¸ªJSONBç±»å‹ï¼Œå­˜å‚¨çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ—¥æœŸå’Œå†…å®¹
            # éå† daily_summaries åˆ—è¡¨ï¼ŒæŸ¥æ‰¾ä¸å½“å‰æ—¥æœŸåŒ¹é…çš„æ¯æ—¥æ‘˜è¦
            for daily_summary in major_event["daily_summaries"]:
                if daily_summary.get("date") == self.date_str:
                    return daily_summary
        return None

    async def get_daily_schedule_info(self) -> Optional[dict]:
        return get_daily_schedule_by_date(self.date_str)

    async def get_schedule_item_at_time(self, target_time: str) -> Optional[dict]:
        logger.debug(
            f"get_schedule_item_at_time called with target_time: {target_time}"
        )
        daily_schedule = await self.get_daily_schedule_info()
        if not daily_schedule:
            logger.debug("No daily schedule found.")
            return None

        if not (
            "schedule_data" in daily_schedule
            and "schedule_items" in daily_schedule["schedule_data"]
        ):
            logger.debug("Daily schedule has no 'schedule_data' or 'schedule_items'.")
            return None

        try:
            target_time_obj = datetime.strptime(target_time, "%H:%M").time()
        except ValueError:
            logger.error(f"Invalid target_time format: {target_time}")
            return None

        for item in daily_schedule["schedule_data"]["schedule_items"]:
            item_start_time_str = item.get("start_time")
            item_end_time_str = item.get("end_time")

            if not (item_start_time_str and item_end_time_str):
                logger.warning(f"Schedule item missing start_time or end_time: {item}")
                continue

            try:
                item_start_time_obj = datetime.strptime(
                    item_start_time_str, "%H:%M"
                ).time()
                item_end_time_obj = datetime.strptime(item_end_time_str, "%H:%M").time()
            except ValueError:
                logger.error(f"Invalid time format in schedule item: {item}")
                continue

            logger.debug(
                f"Checking item: {item.get('title')} from {item_start_time_str} to {item_end_time_str}"
            )
            if item_start_time_obj <= target_time_obj < item_end_time_obj:
                logger.debug(f"Matched schedule item: {item.get('title')}")
                return item

        logger.debug("No matching schedule item found for current time.")
        return None

    async def get_micro_experiences_for_schedule_item(
        self, schedule_item_id: str
    ) -> Optional[list]:
        return get_micro_experiences_by_related_item_id(schedule_item_id)

    async def get_micro_experience_at_time(
        self, schedule_item_id: str, target_time: str
    ) -> Optional[dict]:
        logger.debug(
            f"get_micro_experience_at_time called for schedule_item_id: {schedule_item_id}, target_time: {target_time}"
        )
        micro_experiences_list = await self.get_micro_experiences_for_schedule_item(
            schedule_item_id
        )
        if not micro_experiences_list:
            logger.debug(
                f"No micro experiences found for schedule_item_id: {schedule_item_id}"
            )
            return None

        try:
            target_time_obj = datetime.strptime(target_time, "%H:%M").time()
        except ValueError:
            logger.error(f"Invalid target_time format: {target_time}")
            return None

        for record in micro_experiences_list:
            experiences_in_record = record.get("experiences", [])
            if not experiences_in_record:
                logger.debug(
                    f"Micro experience record {record.get('id')} has no 'experiences'."
                )
                continue

            for exp in experiences_in_record:
                exp_start_time_str = exp.get("start_time")
                exp_end_time_str = exp.get("end_time")

                if not (exp_start_time_str and exp_end_time_str):
                    logger.warning(
                        f"Micro experience item missing start_time or end_time: {exp}"
                    )
                    continue

                try:
                    exp_start_time_obj = datetime.strptime(
                        exp_start_time_str, "%H:%M"
                    ).time()
                    exp_end_time_obj = datetime.strptime(
                        exp_end_time_str, "%H:%M"
                    ).time()
                except ValueError:
                    logger.error(f"Invalid time format in micro experience item: {exp}")
                    continue

                logger.debug(
                    f"Checking micro experience: {exp.get('content')} from {exp_start_time_str} to {exp_end_time_str}"
                )
                if exp_start_time_obj <= target_time_obj < exp_end_time_obj:
                    logger.debug(f"Matched micro experience: {exp.get('content')}")
                    return exp

        logger.debug("No matching micro experience found for current time.")
        return None


# ç¤ºä¾‹ç”¨æ³• (åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›ä¼šé€šè¿‡APIæˆ–è°ƒåº¦å™¨è§¦å‘)
async def main(target_date: date = None):
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼ŒåŒ…å«å¼‚å¸¸å¤„ç†å’Œæ—¥æœŸå‚æ•°"""
    target_date = target_date or date.today()

    try:
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆ {target_date} çš„æ—¥ç¨‹ç³»ç»Ÿ")

        # ç”Ÿæˆä¸»æ—¥ç¨‹
        await generate_and_store_daily_life(target_date)

        # ç¤ºä¾‹æŸ¥è¯¢åŠŸèƒ½éªŒè¯
        logger.info("éªŒè¯ç³»ç»ŸæŸ¥è¯¢åŠŸèƒ½...")
        query = LifeSystemQuery(target_date)
        print(f"\n{target_date} æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­: {await query.is_in_major_event()}")
        print(f"å½“æ—¥æ—¥ç¨‹æ‘˜è¦: {await query.get_daily_schedule_info() or 'æ— æ—¥ç¨‹'}")

    except Exception as e:
        logger.critical(f"â€¼ï¸ ä¸»æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        raise

    print("\n--- LifeSystemQuery ç¤ºä¾‹ ---")
    query_today = LifeSystemQuery()
    print(
        f"ä»Šå¤© ({query_today.date_str}) æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­: {await query_today.is_in_major_event()}"
    )
    print(
        f"ä»Šå¤© ({query_today.date_str}) çš„å¤§äº‹ä»¶ä¿¡æ¯: {await query_today.get_major_event_info()}"
    )
    print(
        f"ä»Šå¤© ({query_today.date_str}) çš„æ—¥ç¨‹ä¿¡æ¯: {await query_today.get_daily_schedule_info()}"
    )

    # å‡è®¾æœ‰ä¸€ä¸ªæ—¥ç¨‹é¡¹ID
    # example_schedule_item_id = "some_uuid_from_db"
    # print(f"æ—¥ç¨‹é¡¹ {example_schedule_item_id} çš„å¾®è§‚ç»å†: {await query_today.get_micro_experiences_for_schedule_item(example_schedule_item_id)}")
    # print(f"æ—¥ç¨‹é¡¹ {example_schedule_item_id} åœ¨ 10:00 çš„å¾®è§‚ç»å†: {await query_today.get_micro_experience_at_time(example_schedule_item_id, '10:00')}")


if __name__ == "__main__":
    import argparse
    from datetime import datetime

    parser = argparse.ArgumentParser(description="å¾·å·ç”Ÿæ´»ç³»ç»Ÿç”Ÿæˆå™¨")
    parser.add_argument(
        "--date", type=str, help="æŒ‡å®šç”Ÿæˆæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)", default=None
    )
    args = parser.parse_args()

    target_date = date.today()
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            logger.error(f"æ— æ•ˆæ—¥æœŸæ ¼å¼: {args.date}, ä½¿ç”¨ä»Šæ—¥æ—¥æœŸ")
            target_date = date.today()

    logger.info(f"ğŸ•’ æ‰§è¡Œæ—¥æœŸ: {target_date}")
    asyncio.run(main(target_date))
