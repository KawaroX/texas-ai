import os
import httpx
import logging
import json
import asyncio
from typing import AsyncGenerator, Optional

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1/chat/completions"

YUNWU_AI_KEY = os.getenv("YUNWU_AI_KEY")
YUNWU_AI_BASE_URL = "https://yunwu.ai/v1/chat/completions"
YUNWU_AI_MODEL = "claude-3-7-sonnet-20250219"

logger = logging.getLogger(__name__)


async def retry_with_backoff(func, max_retries: int = 3, base_delay: float = 1.0):
    """
    é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿
    """
    for attempt in range(max_retries):
        try:
            return await func()
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒé‡è¯•")
                    raise
            else:
                # å…¶ä»–HTTPé”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                raise
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒé‡è¯•")
                raise


async def stream_openrouter(
    messages, model="deepseek/deepseek-chat-v3-0324:free"
) -> AsyncGenerator[str, None]:
    """
    æµå¼è°ƒç”¨OpenRouter APIï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
    """
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ stream_openrouter(): {model}")
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {"model": model, "messages": messages, "stream": True}

    async def _stream_request():
        async with httpx.AsyncClient(timeout=60) as client:
            async with client.stream(
                "POST", OPENROUTER_BASE_URL, headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                async for chunk in response.aiter_lines():
                    chunk = chunk.strip()
                    if chunk == "":
                        continue  # è·³è¿‡ç©ºè¡Œ
                    if chunk.startswith(":"):
                        # è·³è¿‡SSEæ³¨é‡Šè¡Œ
                        continue
                    if chunk.startswith("data:"):
                        data_part = chunk[5:].strip()
                        if data_part == "[DONE]":
                            # è·³è¿‡æµç»“æŸæ ‡è®°
                            continue
                        try:
                            data = json.loads(data_part)
                            if "choices" in data and data["choices"]:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    yield delta["content"]
                        except json.JSONDecodeError as json_err:
                            logger.error(
                                f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: JSONè§£æé”™è¯¯: {json_err}. åŸå§‹æ•°æ®: '{chunk}'"
                            )
                            continue
                    elif chunk.startswith("event:"):
                        # è·³è¿‡å¦‚ event: end ä¹‹ç±»çš„äº‹ä»¶è¡Œ
                        continue
                    else:
                        # è·³è¿‡æœªçŸ¥è¡Œï¼Œä¸å†è®°å½•warning
                        continue

    # å¯¹äºæµå¼è¯·æ±‚ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç†é‡è¯•é€»è¾‘ï¼Œä¸ä½¿ç”¨ retry_with_backoff
    max_retries = 3
    base_delay = 1.0

    for attempt in range(max_retries):
        try:
            async for chunk in _stream_request():
                yield chunk
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
        except httpx.HTTPStatusError as http_err:
            status_code = http_err.response.status_code
            if status_code == 429:
                logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(
                        "âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: APIè°ƒç”¨é¢‘ç‡é™åˆ¶ (429 Too Many Requests)"
                    )
                    yield "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    return
            else:
                try:
                    error_content = await http_err.response.aread()
                    error_text = (
                        error_content.decode("utf-8") if error_content else "æœªçŸ¥é”™è¯¯"
                    )
                except Exception:
                    error_text = "æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…"

                logger.error(
                    f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {error_text}"
                )
                yield f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
                return
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error(f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
                yield ""
                return


async def stream_reply_ai(messages, model=YUNWU_AI_MODEL) -> AsyncGenerator[str, None]:
    """
    æµå¼è°ƒç”¨ Reply AI API (æ”¯æŒ OpenAI åè®®)ï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
    """
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ stream_reply_ai(): {model}")
    headers = {
        "Authorization": f"Bearer {YUNWU_AI_KEY}",
        "Content-Type": "application/json",
    }
    payload = {"model": model, "messages": messages, "stream": True}

    async def _stream_request():
        async with httpx.AsyncClient(timeout=60) as client:
            async with client.stream(
                "POST", YUNWU_AI_BASE_URL, headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                async for chunk in response.aiter_lines():
                    chunk = chunk.strip()
                    if chunk == "":
                        continue  # è·³è¿‡ç©ºè¡Œ
                    if chunk.startswith(":"):
                        # è·³è¿‡SSEæ³¨é‡Šè¡Œ
                        continue
                    if chunk.startswith("data:"):
                        data_part = chunk[5:].strip()
                        if data_part == "[DONE]":
                            # è·³è¿‡æµç»“æŸæ ‡è®°
                            continue
                        try:
                            data = json.loads(data_part)
                            if "choices" in data and data["choices"]:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    yield delta["content"]
                        except json.JSONDecodeError as json_err:
                            logger.error(
                                f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: JSONè§£æé”™è¯¯: {json_err}. åŸå§‹æ•°æ®: '{chunk}'"
                            )
                            continue
                    elif chunk.startswith("event:"):
                        # è·³è¿‡å¦‚ event: end ä¹‹ç±»çš„äº‹ä»¶è¡Œ
                        continue
                    else:
                        # è·³è¿‡æœªçŸ¥è¡Œï¼Œä¸å†è®°å½•warning
                        continue

    # å¯¹äºæµå¼è¯·æ±‚ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç†é‡è¯•é€»è¾‘ï¼Œä¸ä½¿ç”¨ retry_with_backoff
    max_retries = 3
    base_delay = 1.0

    for attempt in range(max_retries):
        try:
            async for chunk in _stream_request():
                yield chunk
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
        except httpx.HTTPStatusError as http_err:
            status_code = http_err.response.status_code
            if status_code == 429:
                logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(
                        "âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: APIè°ƒç”¨é¢‘ç‡é™åˆ¶ (429 Too Many Requests)"
                    )
                    yield "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    return
            else:
                try:
                    error_content = await http_err.response.aread()
                    error_text = (
                        error_content.decode("utf-8") if error_content else "æœªçŸ¥é”™è¯¯"
                    )
                except Exception:
                    error_text = "æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…"

                logger.error(
                    f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {error_text}"
                )
                yield f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
                return
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error(f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
                yield ""
                return


async def stream_ai_chat(messages: list, model: Optional[str] = None):
    """
    æµå¼ç”ŸæˆAIå›å¤ï¼ŒæŒ‰åˆ†éš”ç¬¦åˆ†æ®µè¾“å‡ºã€‚
    åˆ†éš”ç¬¦ä¼˜å…ˆä¸º '==='ï¼Œå…¶æ¬¡ä¸ºæ¢è¡Œç¬¦ã€‚
    """
    # å¦‚ï¿½ï¿½ï¿½æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œæˆ–è€…æŒ‡å®šçš„æ˜¯ DeepSeek V3 æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ Reply AI æ¸ é“
    if model is None or model == "deepseek/deepseek-chat-v3-0324:free":
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ Reply AI æ¸ é“è¿›è¡Œ stream_ai_chat(): {YUNWU_AI_MODEL}")
        stream_func = stream_reply_ai
        actual_model = YUNWU_AI_MODEL
    else:
        # å¦åˆ™ï¼Œä½¿ç”¨ OpenRouter æ¸ é“
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ OpenRouter æ¸ é“è¿›è¡Œ stream_ai_chat(): {model}")
        stream_func = stream_openrouter
        actual_model = model

    buffer = ""
    async for chunk in stream_func(messages, model=actual_model):
        buffer += chunk

        # ä¼˜å…ˆæŒ‰ '===' åˆ†æ®µ
        while True:
            sep_index = buffer.find("===")
            if sep_index != -1:
                segment = buffer[:sep_index].strip()
                if segment:
                    yield segment
                buffer = buffer[sep_index + 3 :]
                continue
            # å…¶æ¬¡æŒ‰æ¢è¡Œç¬¦åˆ†æ®µï¼ˆå¯é€‰ï¼Œé€šå¸¸æµå¼æ¨¡å‹ç›´æ¥æŒ‰===åˆ†ï¼‰
            newline_index = buffer.find("\n")
            if newline_index != -1:
                segment = buffer[:newline_index].strip()
                if segment:
                    yield segment
                buffer = buffer[newline_index + 1 :]
                continue
            break

    # æœ€ç»ˆå‰©ä½™å†…å®¹
    if buffer.strip():
        yield buffer.strip()


async def call_openrouter(
    messages, model="nousresearch/deephermes-3-llama-3-8b-preview:free"
) -> str:
    """
    éæµå¼è°ƒç”¨ï¼ˆç”¨äºæ‘˜è¦ç­‰åœºæ™¯ï¼‰
    """
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model,
        "messages": messages,
    }

    async def _call_request():
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_openrouter(): {model}")
        async with httpx.AsyncClient() as client:
            response = await client.post(
                OPENROUTER_BASE_URL, headers=headers, json=payload
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

    try:
        return await retry_with_backoff(_call_request)
    except httpx.HTTPStatusError as http_err:
        status_code = http_err.response.status_code
        if status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
            return "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            logger.error(
                f"âŒ OpenRouterè°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {http_err.response.text}"
            )
            return f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
    except Exception as e:
        logger.error(f"âŒ OpenRouterè°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
        return ""


async def call_ai_summary(prompt: str) -> str:
    """
    è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦ï¼Œå¯ç”¨äº context_merger.pyã€‚
    """
    messages = [{"role": "user", "content": prompt}]
    model = "nousresearch/deephermes-3-llama-3-8b-preview:free"
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_ai_summary(): {model}")
    # ä½ å¯ä»¥æ ¹æ®éœ€æ±‚è‡ªç”±åˆ‡æ¢æ¨¡å‹å
    return await call_openrouter(messages, model)
