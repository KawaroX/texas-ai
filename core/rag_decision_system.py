import re
import time
import json
import os
import random
from typing import Dict, Optional
from dataclasses import dataclass, asdict
import jieba
import redis

import logging

logger = logging.getLogger(__name__)


@dataclass
class SimpleContext:
    """ç®€å•çš„åŠ¨æ€ç´¯ç§¯ä¸Šä¸‹æ–‡"""

    accumulated_score: float = 0.0
    last_update_time: float = 0.0
    # æ–°å¢ï¼šè¿ç»­æŸ¥è¯¢è®¡æ•°å™¨
    consecutive_queries: int = 0
    # æ–°å¢ï¼šæœ€è¿‘è§¦å‘æ—¶é—´
    last_trigger_time: float = 0.0

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "SimpleContext":
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**data)


class RAGDecisionMaker:
    """ç²¾ç®€ç‰ˆRAGå†³ç­–å™¨ - è¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºå¸ƒå°”å€¼ï¼ˆæ”¯æŒRedisç¼“å­˜ï¼‰"""

    def __init__(
        self,
        user_id: str,  # æ–°å¢ç”¨æˆ·IDå‚æ•°
        search_threshold: float = 0.65,
        accumulation_threshold: float = 0.35,
        min_accumulation_threshold: float = 0.15,
        time_decay_minutes: float = 10.0,
        max_accumulation: float = 0.4,
        random_factor_weight: float = 0.05,
        cache_ttl: int = 3600,
        # æ–°å¢å‚æ•°ï¼šæé«˜è§¦å‘æ¦‚ç‡çš„é…ç½®
        consecutive_boost_factor: float = 0.08,  # è¿ç»­æŸ¥è¯¢åŠ æˆå› å­
        max_consecutive_boost: float = 0.25,  # æœ€å¤§è¿ç»­æŸ¥è¯¢åŠ æˆ
        trigger_cooldown_minutes: float = 2.0,  # è§¦å‘å†·å´æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        context_sensitivity: float = 1.2,  # ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦å€æ•°
        memory_boost_probability: float = 0.15,  # è®°å¿†åŠ æˆè§¦å‘æ¦‚ç‡
    ):
        """
        åˆå§‹åŒ–å‚æ•°

        Args:
            user_id: ç”¨æˆ·å”¯ä¸€æ ‡è¯†ç¬¦
            search_threshold: æœç´¢é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è¿”å›True
            accumulation_threshold: ç´¯ç§¯é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼ä¸”ä½äºæœç´¢é˜ˆå€¼æ—¶ç´¯ç§¯
            min_accumulation_threshold: æœ€ä½ç´¯ç§¯é—¨æ§›ï¼Œä½äºæ­¤å€¼ä¸ç´¯ç§¯
            time_decay_minutes: ç´¯ç§¯åˆ†æ•°è¡°å‡æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            max_accumulation: æœ€å¤§ç´¯ç§¯å€¼
            random_factor_weight: éšæœºå› å­æƒé‡
            cache_ttl: Redisç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            consecutive_boost_factor: è¿ç»­æŸ¥è¯¢åŠ æˆå› å­
            max_consecutive_boost: æœ€å¤§è¿ç»­æŸ¥è¯¢åŠ æˆ
            trigger_cooldown_minutes: è§¦å‘å†·å´æ—¶é—´
            context_sensitivity: ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦å€æ•°
            memory_boost_probability: è®°å¿†åŠ æˆè§¦å‘æ¦‚ç‡
        """
        self.user_id = user_id
        self.search_threshold = search_threshold
        self.accumulation_threshold = accumulation_threshold
        self.min_accumulation_threshold = min_accumulation_threshold
        self.time_decay_minutes = time_decay_minutes
        self.max_accumulation = max_accumulation
        self.random_factor_weight = random_factor_weight
        self.cache_ttl = cache_ttl

        # æ–°å¢å‚æ•°
        self.consecutive_boost_factor = consecutive_boost_factor
        self.max_consecutive_boost = max_consecutive_boost
        self.trigger_cooldown_minutes = trigger_cooldown_minutes
        self.context_sensitivity = context_sensitivity
        self.memory_boost_probability = memory_boost_probability

        # åˆå§‹åŒ–Redisè¿æ¥
        self.redis_client = redis.Redis.from_url(os.getenv("REDIS_URL"))

        # Redisé”®åè®¾è®¡
        self._context_key = f"rag_decision:context:{self.user_id}"
        self._stats_key = f"rag_decision:stats:{self.user_id}"

        # åˆå§‹åŒ–jieba
        jieba.initialize()

        # å¾·å…‹è¨æ–¯è§’è‰²ç›¸å…³è¯æ±‡ - æ‰©å±•è¯æ±‡åº“
        self.texas_keywords = {
            "character_related": [
                "ä¼é¹…ç‰©æµ",
                "æ‹‰æ™®å…°å¾·",
                "èƒ½å¤©ä½¿",
                "å¯é¢‚",
                "ç©º",
                "ç½—å¾·å²›",
                "å¾·å…‹è¨æ–¯",
                "å¿«é€’",
                "é€è´§",
                "ç‰©æµ",
                "é…é€",
                "åŒ…è£¹",
                "ä»»åŠ¡",
                "å·¥ä½œ",
                "åŒä¼´",
            ],
            "emotional_states": [
                "ç´¯äº†",
                "ç–²æƒ«",
                "å‹åŠ›",
                "æ”¾æ¾",
                "ä¼‘æ¯",
                "ç´§å¼ ",
                "æ‹…å¿ƒ",
                "å¼€å¿ƒ",
                "é«˜å…´",
                "éš¾è¿‡",
                "æ²®ä¸§",
                "å…´å¥‹",
                "å¹³é™",
                "ç„¦è™‘",
                "è½»æ¾",
            ],
            "relationships": [
                "æœ‹å‹",
                "ä¼™ä¼´",
                "åŒäº‹",
                "é˜Ÿå‹",
                "ä¿¡ä»»",
                "ä¾èµ–",
                "å…³å¿ƒ",
                "ç…§é¡¾",
                "åˆä½œ",
                "é…åˆ",
                "ç†è§£",
                "æ”¯æŒ",
            ],
            "memories": [
                "è®°å¾—",
                "æƒ³èµ·",
                "å›å¿†",
                "ä»¥å‰",
                "é‚£æ—¶",
                "è¿‡å»",
                "ä¹‹å‰",
                "ä¸Šæ¬¡",
                "æ›¾ç»",
                "æœ€è¿‘",
                "åˆšæ‰",
                "åˆšåˆš",
                "åˆšè¯´",
                "æåˆ°è¿‡",
                "èŠè¿‡",
            ],
            # æ–°å¢ï¼šæ›´å¤šè§¦å‘ç±»åˆ«
            "continuity": [
                "ç»§ç»­",
                "æ¥ç€",
                "ç„¶å",
                "åæ¥",
                "æ¥ä¸‹æ¥",
                "å¦å¤–",
                "è¿˜æœ‰",
                "è€Œä¸”",
            ],
            "uncertainty": [
                "ä¸ç¡®å®š",
                "ä¸çŸ¥é“",
                "å¯èƒ½",
                "ä¹Ÿè®¸",
                "å¤§æ¦‚",
                "ä¼°è®¡",
                "åº”è¯¥",
                "æˆ–è®¸",
            ],
        }

        # è¯­è¨€æ¨¡å¼ - å¢å¼ºæ¨¡å¼åŒ¹é…
        self.patterns = {
            "temporal": [
                r"æ˜¨å¤©",
                r"å‰å¤©",
                r"ä¸Šå‘¨",
                r"ä¹‹å‰",
                r"é‚£æ—¶å€™",
                r"å½“æ—¶",
                r"ä»¥å‰",
                r"åˆšæ‰",
                r"åˆšåˆš",
                r"åˆšè¯´",
                r"æœ€è¿‘",
                r"è¿‘æœŸ",
                r"è¿™å‡ å¤©",
            ],
            "referential": [
                r"é‚£ä¸ª",
                r"è¿™ä¸ª",
                r"å®ƒ",
                r"ä»–",
                r"å¥¹",
                r"é‚£ä»¶äº‹",
                r"è¿™ä»¶äº‹",
                r"é‚£æ ·",
                r"è¿™æ ·",
                r"é‚£ç§",
                r"è¿™ç§",
                r"å¦‚ä½ æ‰€è¯´",
                r"åƒä½ è¯´çš„",
            ],
            "personal": [
                r"æˆ‘çš„",
                r"ä½ çš„",
                r"æˆ‘ä»¬çš„",
                r"ä½ çŸ¥é“æˆ‘",
                r"ä½ äº†è§£æˆ‘",
                r"å¯¹æˆ‘æ¥è¯´",
                r"åœ¨æˆ‘çœ‹æ¥",
                r"æˆ‘è§‰å¾—",
                r"æˆ‘è®¤ä¸º",
                r"æˆ‘æƒ³",
                r"æˆ‘å¸Œæœ›",
            ],
            "questioning": [
                r"å—\?",
                r"å‘¢\?",
                r"å¦‚ä½•",
                r"æ€ä¹ˆ",
                r"ä¸ºä»€ä¹ˆ",
                r"ä»€ä¹ˆæ—¶å€™",
                r"å“ªé‡Œ",
                r"ä»€ä¹ˆ",
                r"è°",
                r"å¤šå°‘",
                r"å“ªä¸ª",
                r"å“ªç§",
            ],
            # æ–°å¢ï¼šå¯¹è¯å»¶ç»­æ¨¡å¼
            "continuation": [
                r"é‚£ä¹ˆ",
                r"æ‰€ä»¥",
                r"å› æ­¤",
                r"ä¸è¿‡",
                r"ä½†æ˜¯",
                r"è€Œä¸”",
                r"å¦å¤–",
                r"è¿˜æœ‰",
                r"é™¤äº†",
                r"å…³äº",
                r"è¯´åˆ°",
            ],
        }

        # åŠ è½½æˆ–åˆå§‹åŒ–ä¸Šä¸‹æ–‡
        self._context = self._load_context()

    def _load_context(self) -> SimpleContext:
        """ä»RedisåŠ è½½ä¸Šä¸‹æ–‡"""
        try:
            context_data = self.redis_client.get(self._context_key)
            if context_data:
                context_dict = json.loads(context_data.decode("utf-8"))
                logger.debug(
                    f"[RAG DECISION] Loaded context from Redis: {context_dict}"
                )
                return SimpleContext.from_dict(context_dict)
            else:
                logger.debug("[RAG DECISION] No context in Redis, using default")
        except (redis.RedisError, json.JSONDecodeError, TypeError) as e:
            logger.error(f"[RAG DECISION] Failed to load context from Redis: {e}")

        # è¿”å›é»˜è®¤ä¸Šä¸‹æ–‡
        return SimpleContext()

    def _save_context(self):
        """ä¿å­˜ä¸Šä¸‹æ–‡åˆ°Redis"""
        try:
            context_data = json.dumps(self._context.to_dict())
            self.redis_client.setex(self._context_key, self.cache_ttl, context_data)
            logger.debug(
                f"[RAG DECISION] Context saved to Redis: {self._context.to_dict()}"
            )
        except (redis.RedisError, json.JSONDecodeError) as e:
            logger.error(f"[RAG DECISION] Failed to save context to Redis: {e}")

    def _update_stats(self, message: str, decision: bool, final_score: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯åˆ°Redis"""
        try:
            # è·å–å½“å‰ç»Ÿè®¡
            stats_data = self.redis_client.get(self._stats_key)
            if stats_data:
                stats = json.loads(stats_data.decode("utf-8"))
                logger.debug("[RAG DECISION] Loaded existing stats from Redis")
            else:
                stats = {
                    "total_queries": 0,
                    "search_count": 0,
                    "no_search_count": 0,
                    "avg_score": 0.0,
                    "last_updated": 0.0,
                }
                logger.debug("[RAG DECISION] Created new stats structure")

            # æ›´æ–°ç»Ÿè®¡
            stats["total_queries"] += 1
            if decision:
                stats["search_count"] += 1
            else:
                stats["no_search_count"] += 1

            # æ›´æ–°å¹³å‡åˆ†æ•°ï¼ˆç®€å•ç§»åŠ¨å¹³å‡ï¼‰
            old_avg = stats["avg_score"]
            stats["avg_score"] = (
                old_avg * (stats["total_queries"] - 1) + final_score
            ) / stats["total_queries"]
            stats["last_updated"] = time.time()

            # ä¿å­˜ç»Ÿè®¡
            stats_json = json.dumps(stats)
            self.redis_client.setex(self._stats_key, self.cache_ttl, stats_json)
            logger.debug(
                f"[RAG DECISION] Updated stats: total_queries={stats['total_queries']}, "
                f"search_count={stats['search_count']}, no_search_count={stats['no_search_count']}, "
                f"avg_score={stats['avg_score']:.3f}"
            )

        except (redis.RedisError, json.JSONDecodeError) as e:
            logger.error(f"[RAG DECISION] Failed to update stats in Redis: {e}")

    def get_user_stats(self) -> Dict:
        """è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats_data = self.redis_client.get(self._stats_key)
            if stats_data:
                return json.loads(stats_data.decode("utf-8"))
        except (redis.RedisError, json.JSONDecodeError) as e:
            print(f"Warning: Failed to get stats from Redis: {e}")

        return {
            "total_queries": 0,
            "search_count": 0,
            "no_search_count": 0,
            "avg_score": 0.0,
            "last_updated": 0.0,
        }

    def _quick_filter(self, message: str) -> Optional[bool]:
        """å¿«é€Ÿè¿‡æ»¤æ˜æ˜¾æƒ…å†µ - å¢å¼ºç‰ˆ"""
        message = message.strip()
        logger.debug(f"[RAG DECISION] Applying quick filter for: {message}")

        # æçŸ­ç®€å•é—®å€™ - ä½†è¦è€ƒè™‘è¿ç»­æŸ¥è¯¢
        if len(message) <= 3:
            simple_greetings = ["ä½ å¥½", "æ—©", "æ™šå®‰", "å—¯", "å“¦", "å¥½çš„", "è°¢è°¢"]
            if any(greeting in message for greeting in simple_greetings):
                # å¦‚æœè¿ç»­æŸ¥è¯¢è¾ƒå¤šï¼Œå³ä½¿æ˜¯ç®€å•é—®å€™ä¹Ÿå¯èƒ½éœ€è¦æœç´¢
                if self._context.consecutive_queries >= 3:
                    logger.debug(
                        "[RAG DECISION] Quick filter: short greeting but consecutive queries -> search"
                    )
                    return True
                logger.debug("[RAG DECISION] Quick filter: short greeting -> no search")
                return False

        # æ˜ç¡®çš„è®°å¿†ç›¸å…³è¯æ±‡
        memory_patterns = [
            "è®°å¾—",
            "è¿˜è®°å¾—",
            "æƒ³èµ·",
            "ä¹‹å‰è¯´è¿‡",
            "ä¸Šæ¬¡æåˆ°",
            "ä»¥å‰èŠè¿‡",
            "åˆšæ‰è¯´",
            "åˆšæåˆ°",
        ]
        if any(pattern in message for pattern in memory_patterns):
            logger.debug("[RAG DECISION] Quick filter: memory pattern -> search")
            return True

        # æ˜ç¡®çš„æ—¶é—´æŒ‡ä»£
        time_patterns = [
            "æ˜¨å¤©æˆ‘ä»¬",
            "ä¸Šæ¬¡ä½ ",
            "ä¹‹å‰çš„",
            "é‚£æ—¶å€™æˆ‘",
            "åˆšæ‰æˆ‘ä»¬",
            "åˆšåˆšä½ ",
        ]
        if any(pattern in message for pattern in time_patterns):
            logger.debug("[RAG DECISION] Quick filter: time pattern -> search")
            return True

        # æ–°å¢ï¼šå¯¹è¯å»¶ç»­æŒ‡ç¤ºè¯
        continuation_patterns = ["é‚£ä¹ˆ", "æ‰€ä»¥", "å› æ­¤", "æ¥ç€", "ç„¶å", "å¦å¤–", "è¿˜æœ‰"]
        if any(pattern in message for pattern in continuation_patterns):
            logger.debug(
                "[RAG DECISION] Quick filter: continuation pattern -> potential search"
            )
            # å»¶ç»­æ€§è¯æ±‡å¢åŠ æœç´¢å€¾å‘ï¼Œä½†ä¸æ˜¯ç»å¯¹
            return None  # éœ€è¦è¯¦ç»†åˆ†æï¼Œä½†ä¼šæœ‰åŠ æˆ

        logger.debug("[RAG DECISION] Quick filter: no match, need detailed analysis")
        return None  # éœ€è¦è¯¦ç»†åˆ†æ

    def _calculate_base_score(self, message: str) -> float:
        """è®¡ç®—åŸºç¡€åˆ†æ•° - å¢å¼ºç‰ˆ"""
        total_score = 0.0
        logger.debug(f"[RAG DECISION] Calculating base score for: {message}")

        # 1. å…³é”®è¯åˆ†æ - æƒé‡è°ƒæ•´
        # å¾·å…‹è¨æ–¯ç›¸å…³è¯æ±‡ (æƒé‡æå‡)
        texas_score = 0
        for category, keywords in self.texas_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in message)
            if matches > 0:
                # ä¸åŒç±»åˆ«ç»™äºˆä¸åŒæƒé‡
                if category in ["memories", "continuity"]:
                    texas_score += matches * 0.15  # è®°å¿†å’Œè¿ç»­æ€§æƒé‡æ›´é«˜
                else:
                    texas_score += matches * 0.12
        total_score += min(texas_score, 0.35)  # ä¸Šé™æé«˜
        logger.debug(f"  - Texas keywords score: {min(texas_score, 0.35):.3f}")

        # è®°å¿†ç›¸å…³è¯æ±‡ (å•ç‹¬åŠ å¼º)
        memory_keywords = self.texas_keywords["memories"]
        memory_score = sum(
            0.18 for keyword in memory_keywords if keyword in message
        )  # æƒé‡æå‡
        total_score += min(memory_score, 0.45)  # ä¸Šé™æé«˜
        logger.debug(f"  - Memory keywords score: {min(memory_score, 0.45):.3f}")

        # 2. è¯­è¨€æ¨¡å¼åˆ†æ - æƒé‡è°ƒæ•´
        pattern_scores = {}
        for pattern_type, patterns in self.patterns.items():
            pattern_score = 0
            for pattern in patterns:
                if re.search(pattern, message):
                    if pattern_type in ["temporal", "referential", "continuation"]:
                        pattern_score += 0.12  # è¿™äº›æ¨¡å¼æƒé‡æ›´é«˜
                    else:
                        pattern_score += 0.10
            pattern_score = min(pattern_score, 0.3)  # ä¸Šé™æé«˜
            total_score += pattern_score
            pattern_scores[pattern_type] = pattern_score

        logger.debug(f"  - Pattern scores: {pattern_scores}")

        # 3. ä¸ªæ€§åŒ–åˆ†æ - å¢å¼º
        personal_indicators = [
            "æˆ‘çš„",
            "ä½ çŸ¥é“æˆ‘",
            "ä½ äº†è§£",
            "å¯¹æˆ‘",
            "æˆ‘è§‰å¾—",
            "æˆ‘æƒ³è¦",
            "åœ¨æˆ‘çœ‹æ¥",
            "æˆ‘è®¤ä¸º",
        ]
        personal_score = sum(
            0.12
            for indicator in personal_indicators
            if indicator in message  # æƒé‡æå‡
        )
        personal_score = min(personal_score, 0.3)  # ä¸Šé™æé«˜
        total_score += personal_score
        logger.debug(f"  - Personalization score: {personal_score:.3f}")

        # æƒ…æ„Ÿè¡¨è¾¾ - å¢å¼º
        emotional_words = [
            "æ„Ÿè§‰",
            "è§‰å¾—",
            "è®¤ä¸º",
            "å¸Œæœ›",
            "æ‹…å¿ƒ",
            "å¼€å¿ƒ",
            "éš¾è¿‡",
            "ç´§å¼ ",
            "å…´å¥‹",
            "å¹³é™",
            "ç„¦è™‘",
            "è½»æ¾",
            "æ»¡æ„",
            "å¤±æœ›",
        ]
        emotion_score = sum(
            0.10 for word in emotional_words if word in message
        )  # æƒé‡æå‡
        emotion_score = min(emotion_score, 0.25)  # ä¸Šé™æé«˜
        total_score += emotion_score
        logger.debug(f"  - Emotion score: {emotion_score:.3f}")

        # 4. æ¶ˆæ¯å¤æ‚åº¦ - è°ƒæ•´
        length_score = min(len(message) / 80, 0.2)  # é•¿åº¦é˜ˆå€¼é™ä½ï¼Œä¸Šé™æé«˜
        total_score += length_score
        logger.debug(f"  - Length score: {length_score:.3f}")

        # 5. æ–°å¢ï¼šé—®å¥æ£€æµ‹
        question_indicators = [
            "ï¼Ÿ",
            "?",
            "å—",
            "å‘¢",
            "å¦‚ä½•",
            "æ€ä¹ˆ",
            "ä¸ºä»€ä¹ˆ",
            "ä»€ä¹ˆ",
            "å“ª",
        ]
        question_score = (
            0.15
            if any(indicator in message for indicator in question_indicators)
            else 0
        )
        total_score += question_score
        logger.debug(f"  - Question score: {question_score:.3f}")

        total_score = min(total_score, 1.0)
        logger.debug(f"[RAG DECISION] Base score calculated: {total_score:.3f}")
        return total_score

    def _generate_memory_spark(self, base_score: float) -> float:
        """ç”Ÿæˆè®°å¿†çªå‘å› å­ - å¢å¼ºç‰ˆ"""
        random_factor = 0.0

        # åŸºç¡€éšæœºå› å­ - èŒƒå›´æ‰©å¤§ï¼Œæ›´å€¾å‘äºæ­£å€¼
        if base_score >= 0.1:  # é™ä½é—¨æ§›
            # ä½¿ç”¨æ›´ç§¯æçš„éšæœºåˆ†å¸ƒ
            base_random = random.gauss(0.03, 0.04)  # å‡å€¼æé«˜ï¼Œæ–¹å·®å¢åŠ 
            base_random = max(-0.03, min(base_random, 0.12))  # èŒƒå›´è°ƒæ•´
            random_factor += base_random

        # è®°å¿†åŠ æˆéšæœºè§¦å‘
        if random.random() < self.memory_boost_probability:
            memory_boost = random.uniform(0.08, 0.15)
            random_factor += memory_boost
            logger.debug(f"[RAG DECISION] Memory boost triggered: +{memory_boost:.4f}")

        # è¿ç»­æŸ¥è¯¢åŠ æˆ
        if self._context.consecutive_queries > 0:
            consecutive_boost = min(
                self._context.consecutive_queries * self.consecutive_boost_factor,
                self.max_consecutive_boost,
            )
            random_factor += consecutive_boost
            logger.debug(
                f"[RAG DECISION] Consecutive queries boost: {consecutive_boost:.4f} (queries: {self._context.consecutive_queries})"
            )

        # å†·å´æ—¶é—´åŠ æˆ - å¦‚æœè·ç¦»ä¸Šæ¬¡è§¦å‘è¾ƒä¹…ï¼Œå¢åŠ è§¦å‘æ¦‚ç‡
        current_time = time.time()
        if self._context.last_trigger_time > 0:
            time_since_trigger = (
                current_time - self._context.last_trigger_time
            ) / 60  # åˆ†é’Ÿ
            if time_since_trigger > self.trigger_cooldown_minutes:
                cooldown_boost = min(time_since_trigger / 60, 0.1)  # æœ€å¤š10%åŠ æˆ
                random_factor += cooldown_boost
                logger.debug(
                    f"[RAG DECISION] Cooldown boost: {cooldown_boost:.4f} (time: {time_since_trigger:.1f}min)"
                )

        random_factor = max(-0.05, min(random_factor, 0.25))  # æ€»ä½“èŒƒå›´é™åˆ¶
        logger.debug(f"[RAG DECISION] Generated memory spark: {random_factor:.4f}")

        return random_factor * self.random_factor_weight * 2  # æ•´ä½“æƒé‡ç¿»å€

    def _update_accumulation(self, current_score: float, should_search: bool):
        """æ›´æ–°ç´¯ç§¯åˆ†æ•° - å¢å¼ºç‰ˆ"""
        current_time = time.time()

        prev_time = self._context.last_update_time

        # æ›´æ–°è¿ç»­æŸ¥è¯¢è®¡æ•°
        if prev_time > 0:
            time_diff = (current_time - prev_time) / 60  # åˆ†é’Ÿ
            if time_diff <= 5:  # 5åˆ†é’Ÿå†…ç®—è¿ç»­
                self._context.consecutive_queries += 1
            else:
                self._context.consecutive_queries = 1  # é‡ç½®ä¸º1
        else:
            self._context.consecutive_queries = 1

        # è®¡ç®—æ—¶é—´è¡°å‡ - è°ƒæ•´è¡°å‡ç­–ç•¥
        if prev_time > 0:
            time_diff = (current_time - prev_time) / 60  # åˆ†é’Ÿ
            if time_diff > self.time_decay_minutes:
                self._context.accumulated_score = 0.0
                logger.debug(
                    f"[RAG DECISION] Accumulation reset due to time decay ({time_diff:.1f}min > {self.time_decay_minutes}min)"
                )
            else:
                # æ›´æ¸©å’Œçš„è¡°å‡
                decay_factor = 1.0 - (
                    time_diff / (self.time_decay_minutes * 1.5)
                )  # è¡°å‡æ›´æ…¢
                self._context.accumulated_score *= max(
                    decay_factor, 0.1
                )  # ä¿ç•™æ›´å¤šç´¯ç§¯å€¼
                logger.debug(
                    f"[RAG DECISION] Time decay applied: decay_factor={decay_factor:.3f}, "
                    f"time_diff={time_diff:.1f}min, accumulated={self._context.accumulated_score:.4f}"
                )

        # å¦‚æœæœç´¢äº†ï¼Œè®°å½•è§¦å‘æ—¶é—´ä½†ä¸å®Œå…¨é‡ç½®ç´¯ç§¯
        if should_search:
            self._context.last_trigger_time = current_time
            self._context.consecutive_queries = 0  # é‡ç½®è¿ç»­æŸ¥è¯¢
            # éƒ¨åˆ†é‡ç½®è€Œä¸æ˜¯å®Œå…¨é‡ç½®
            self._context.accumulated_score *= 0.3  # ä¿ç•™30%
            logger.debug("[RAG DECISION] Search triggered, partial accumulation reset")
        else:
            # ç´¯ç§¯ç­–ç•¥è°ƒæ•´ - æ›´å®¹æ˜“ç´¯ç§¯
            if current_score >= self.min_accumulation_threshold:  # é™ä½ç´¯ç§¯é—¨æ§›
                if current_score >= self.accumulation_threshold:
                    # æ­£å¸¸ç´¯ç§¯
                    accumulation_boost = (
                        current_score - self.accumulation_threshold
                    ) * 0.6  # ç³»æ•°æé«˜
                else:
                    # å³ä½¿ä½äºç´¯ç§¯é˜ˆå€¼ä¹Ÿæœ‰å°å¹…ç´¯ç§¯
                    accumulation_boost = (
                        current_score - self.min_accumulation_threshold
                    ) * 0.3

                # åº”ç”¨ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦
                accumulation_boost *= self.context_sensitivity

                self._context.accumulated_score += accumulation_boost
                self._context.accumulated_score = min(
                    self._context.accumulated_score,
                    self.max_accumulation * 1.2,  # ä¸Šé™æé«˜
                )
                logger.debug(
                    f"[RAG DECISION] Accumulation updated: "
                    f"boost={accumulation_boost:.4f}, "
                    f"new_accumulated={self._context.accumulated_score:.4f}"
                )

        self._context.last_update_time = current_time
        logger.debug(
            f"[RAG DECISION] Context updated: accumulated_score={self._context.accumulated_score:.4f}, "
            f"consecutive_queries={self._context.consecutive_queries}, "
            f"last_update_time={current_time}"
        )

        # ä¿å­˜åˆ°Redis
        self._save_context()

    def should_search(self, message: str) -> bool:
        """
        ä¸»è¦æ¥å£ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢RAG

        Args:
            message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯

        Returns:
            bool: Trueè¡¨ç¤ºéœ€è¦æœç´¢ï¼ŒFalseè¡¨ç¤ºä¸éœ€è¦
        """
        logger.info(f"[RAG DECISION] should_search() å¼€å§‹, message={message}")

        # é˜¶æ®µ1ï¼šå¿«é€Ÿè¿‡æ»¤
        quick_result = self._quick_filter(message)
        if quick_result is not None:
            self._update_accumulation(0.9 if quick_result else 0.1, quick_result)
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(message, quick_result, 0.9 if quick_result else 0.1)
            logger.info(
                f"[RAG DECISION] Quick decision: {'SEARCH' if quick_result else 'NO SEARCH'}"
            )
            return quick_result

        # é˜¶æ®µ2ï¼šè¯¦ç»†åˆ†æ
        base_score = self._calculate_base_score(message)

        # æ·»åŠ è®°å¿†çªå‘å› å­ï¼ˆå¢å¼ºç‰ˆï¼‰
        memory_spark = self._generate_memory_spark(base_score)

        # è®¡ç®—å½“å‰ç´¯ç§¯åŠ æˆï¼ˆå¢å¼ºç‰ˆï¼‰
        current_time = time.time()
        if self._context.last_update_time > 0:
            time_diff = (current_time - self._context.last_update_time) / 60
            if time_diff <= self.time_decay_minutes:
                decay_factor = 1.0 - (
                    time_diff / (self.time_decay_minutes * 1.5)
                )  # æ›´æ…¢çš„è¡°å‡
                accumulated_boost = self._context.accumulated_score * max(
                    decay_factor, 0.1
                )
            else:
                accumulated_boost = 0.0
        else:
            accumulated_boost = 0.0

        # åº”ç”¨ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦å€æ•°
        accumulated_boost *= self.context_sensitivity

        # æœ€ç»ˆåˆ†æ•°
        final_score = min(base_score + memory_spark + accumulated_boost, 1.0)

        # å†³ç­–
        should_search_result = final_score >= self.search_threshold

        # æ›´æ–°ç´¯ç§¯çŠ¶æ€
        self._update_accumulation(final_score, should_search_result)

        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(message, should_search_result, final_score)

        # è¯¦ç»†æ—¥å¿—è®°å½•å†³ç­–è¿‡ç¨‹
        logger.info(
            f"[RAG DECISION] Final decision: {'SEARCH' if should_search_result else 'NO SEARCH'} | "
            f"Score: {final_score:.3f} = "
            f"base({base_score:.3f}) + "
            f"spark({memory_spark:.3f}) + "
            f"accumulated({accumulated_boost:.3f}) * sensitivity({self.context_sensitivity}) | "
            f"Threshold: {self.search_threshold} | "
            f"Consecutive: {self._context.consecutive_queries}"
        )

        return should_search_result

    def get_debug_info(self, message: str) -> Dict:
        """
        è·å–è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰- å¢å¼ºç‰ˆ

        Args:
            message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯

        Returns:
            Dict: åŒ…å«è¯¦ç»†åˆ†æä¿¡æ¯çš„å­—å…¸
        """
        # è¿™ä¸ªæ–¹æ³•ç”¨äºè°ƒè¯•ï¼Œè¿”å›è¯¦ç»†çš„åˆ†æè¿‡ç¨‹
        quick_result = self._quick_filter(message)
        if quick_result is not None:
            return {
                "user_id": self.user_id,
                "quick_filter_result": quick_result,
                "base_score": 0.9 if quick_result else 0.1,
                "memory_spark": 0.0,
                "accumulated_boost": 0.0,
                "final_score": 0.9 if quick_result else 0.1,
                "decision": quick_result,
                "context_from_redis": True,
                "consecutive_queries": self._context.consecutive_queries,
                "enhancements_applied": "quick_filter",
            }

        base_score = self._calculate_base_score(message)
        memory_spark = self._generate_memory_spark(base_score)

        current_time = time.time()
        if self._context.last_update_time > 0:
            time_diff = (current_time - self._context.last_update_time) / 60
            if time_diff <= self.time_decay_minutes:
                decay_factor = 1.0 - (time_diff / (self.time_decay_minutes * 1.5))
                accumulated_boost = self._context.accumulated_score * max(
                    decay_factor, 0.1
                )
            else:
                accumulated_boost = 0.0
        else:
            accumulated_boost = 0.0

        accumulated_boost *= self.context_sensitivity
        final_score = min(base_score + memory_spark + accumulated_boost, 1.0)
        decision = final_score >= self.search_threshold

        return {
            "user_id": self.user_id,
            "quick_filter_result": None,
            "base_score": base_score,
            "memory_spark": memory_spark,
            "accumulated_boost": accumulated_boost,
            "final_score": final_score,
            "decision": decision,
            "current_accumulated": self._context.accumulated_score,
            "consecutive_queries": self._context.consecutive_queries,
            "context_sensitivity": self.context_sensitivity,
            "last_trigger_time": self._context.last_trigger_time,
            "context_from_redis": True,
            "enhancements_applied": "full_analysis",
        }

    def clear_user_data(self):
        """æ¸…é™¤ç”¨æˆ·çš„æ‰€æœ‰ç¼“å­˜æ•°æ®"""
        try:
            self.redis_client.delete(self._context_key)
            self.redis_client.delete(self._stats_key)
            self._context = SimpleContext()
            print(f"Cleared all data for user: {self.user_id}")
        except redis.RedisError as e:
            logger.error(f"[RAG DECISION] Failed to clear user data from Redis: {e}")

    def get_cache_info(self) -> Dict:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        try:
            context_exists = self.redis_client.exists(self._context_key)
            stats_exists = self.redis_client.exists(self._stats_key)

            context_ttl = (
                self.redis_client.ttl(self._context_key) if context_exists else -1
            )
            stats_ttl = self.redis_client.ttl(self._stats_key) if stats_exists else -1

            return {
                "user_id": self.user_id,
                "context_key": self._context_key,
                "stats_key": self._stats_key,
                "context_exists": bool(context_exists),
                "stats_exists": bool(stats_exists),
                "context_ttl": context_ttl,
                "stats_ttl": stats_ttl,
                "cache_ttl_setting": self.cache_ttl,
                "enhancements": {
                    "consecutive_boost_factor": self.consecutive_boost_factor,
                    "max_consecutive_boost": self.max_consecutive_boost,
                    "trigger_cooldown_minutes": self.trigger_cooldown_minutes,
                    "context_sensitivity": self.context_sensitivity,
                    "memory_boost_probability": self.memory_boost_probability,
                },
            }
        except redis.RedisError as e:
            logger.error(f"[RAG DECISION] Failed to get cache info: {e}")
            return {"error": f"Failed to get cache info: {e}", "user_id": self.user_id}

    def adjust_sensitivity(self, new_sensitivity: float):
        """åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦"""
        old_sensitivity = self.context_sensitivity
        self.context_sensitivity = max(0.5, min(new_sensitivity, 3.0))  # é™åˆ¶åœ¨åˆç†èŒƒå›´
        logger.info(
            f"[RAG DECISION] Context sensitivity adjusted: {old_sensitivity:.2f} -> {self.context_sensitivity:.2f}"
        )

    def get_performance_metrics(self) -> Dict:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        stats = self.get_user_stats()
        if stats["total_queries"] == 0:
            return {"message": "No queries processed yet"}

        search_rate = stats["search_count"] / stats["total_queries"]
        current_context = self._context.to_dict()

        return {
            "search_trigger_rate": f"{search_rate:.1%}",
            "total_queries": stats["total_queries"],
            "average_score": f"{stats['avg_score']:.3f}",
            "current_accumulated_score": f"{current_context['accumulated_score']:.4f}",
            "consecutive_queries": current_context["consecutive_queries"],
            "enhancement_impact": {
                "context_sensitivity_multiplier": self.context_sensitivity,
                "consecutive_boost_available": f"{min(current_context['consecutive_queries'] * self.consecutive_boost_factor, self.max_consecutive_boost):.4f}",
                "memory_boost_probability": f"{self.memory_boost_probability:.1%}",
            },
        }


def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹ - å±•ç¤ºä¼˜åŒ–æ•ˆæœ"""
    # åˆ›å»ºä¼˜åŒ–åçš„å†³ç­–å™¨
    user_id = "texas_user_enhanced_001"
    rag_decision = RAGDecisionMaker(
        user_id=user_id,
        search_threshold=0.65,  # é˜ˆå€¼ä¸å˜
        accumulation_threshold=0.35,
        min_accumulation_threshold=0.15,
        time_decay_minutes=10.0,
        max_accumulation=0.4,
        random_factor_weight=0.05,
        cache_ttl=3600,
        # æ–°å¢ä¼˜åŒ–å‚æ•°
        consecutive_boost_factor=0.08,  # è¿ç»­æŸ¥è¯¢åŠ æˆ
        max_consecutive_boost=0.25,  # æœ€å¤§è¿ç»­åŠ æˆ
        trigger_cooldown_minutes=2.0,  # è§¦å‘å†·å´
        context_sensitivity=1.3,  # ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦æå‡
        memory_boost_probability=0.18,  # è®°å¿†åŠ æˆæ¦‚ç‡
    )

    # æµ‹è¯•æ¶ˆæ¯ - åŒ…å«æ›´å¤šè¾¹ç•Œæƒ…å†µ
    test_messages = [
        "æ™šä¸Šå¥½ï¼",
        "ä»Šå¤©å¤©æ°”ä¸é”™",
        "ä½ è§‰å¾—ä¼é¹…ç‰©æµæ€ä¹ˆæ ·ï¼Ÿ",
        "é‚£ä¸ªé—®é¢˜ä½ æ€ä¹ˆçœ‹ï¼Ÿ",
        "å—¯å—¯ï¼Œæˆ‘æ˜ç™½äº†",
        "å¾·å…‹è¨æ–¯ä½ è¯´å‘¢ï¼Ÿ",
        "æˆ‘è®°å¾—ä½ ä¹‹å‰è¯´è¿‡ä»€ä¹ˆæ¥ç€ï¼Ÿ",
        "é‚£ä¹ˆæˆ‘ä»¬ç»§ç»­èŠå§",
        "å¦å¤–ï¼Œæˆ‘æƒ³é—®ä¸ªé—®é¢˜",
        "ä½ äº†è§£æˆ‘çš„æƒ³æ³•å—ï¼Ÿ",
        "åˆšæ‰æåˆ°çš„é‚£ä»¶äº‹...",
        "æ‰€ä»¥ä½ çš„å»ºè®®æ˜¯ï¼Ÿ",
    ]

    print("=== ä¼˜åŒ–ç‰ˆRAGå†³ç­–å™¨æµ‹è¯•ï¼ˆæé«˜è§¦å‘æ¦‚ç‡ï¼‰===\n")

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    cache_info = rag_decision.get_cache_info()
    print("é…ç½®ä¿¡æ¯:")
    print(f"  - æœç´¢é˜ˆå€¼: {rag_decision.search_threshold}")
    print(f"  - ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦: {cache_info['enhancements']['context_sensitivity']}")
    print(
        f"  - è¿ç»­æŸ¥è¯¢åŠ æˆå› å­: {cache_info['enhancements']['consecutive_boost_factor']}"
    )
    print(
        f"  - è®°å¿†åŠ æˆæ¦‚ç‡: {cache_info['enhancements']['memory_boost_probability']:.1%}"
    )
    print(
        f"  - è§¦å‘å†·å´æ—¶é—´: {cache_info['enhancements']['trigger_cooldown_minutes']}åˆ†é’Ÿ\n"
    )

    trigger_count = 0
    for i, message in enumerate(test_messages, 1):
        # ä¸»è¦æ¥å£ï¼šç›´æ¥è·å–å¸ƒå°”ç»“æœ
        result = rag_decision.should_search(message)
        if result:
            trigger_count += 1

        # è·å–è°ƒè¯•ä¿¡æ¯
        debug_info = rag_decision.get_debug_info(message)

        print(f"æ¶ˆæ¯ {i}: {message}")
        print(f"ç»“æœ: {'ğŸ” éœ€è¦æœç´¢' if result else 'ğŸ’¬ ä¸éœ€è¦æœç´¢'}")

        if debug_info.get("enhancements_applied") == "quick_filter":
            print(f"åˆ†æ: å¿«é€Ÿè¿‡æ»¤ -> {debug_info['final_score']:.3f}")
        else:
            print(
                f"åˆ†æ: åŸºç¡€{debug_info['base_score']:.3f} + "
                f"çªå‘{debug_info['memory_spark']:.3f} + "
                f"ç´¯ç§¯{debug_info['accumulated_boost']:.3f} = "
                f"{debug_info['final_score']:.3f}"
            )
            if debug_info["consecutive_queries"] > 0:
                print(f"      è¿ç»­æŸ¥è¯¢: {debug_info['consecutive_queries']} æ¬¡")

        print("-" * 60)

    # æ˜¾ç¤ºä¼˜åŒ–æ•ˆæœç»Ÿè®¡
    print("\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
    print(f"æ€»æ¶ˆæ¯æ•°: {len(test_messages)}")
    print(f"è§¦å‘æœç´¢: {trigger_count} æ¬¡")
    print(f"è§¦å‘ç‡: {trigger_count / len(test_messages):.1%}")

    # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
    metrics = rag_decision.get_performance_metrics()
    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    for key, value in metrics.items():
        if isinstance(value, dict):
            print(f"{key}:")
            for sub_key, sub_value in value.items():
                print(f"  - {sub_key}: {sub_value}")
        else:
            print(f"  - {key}: {value}")

    print("\nğŸ’¡ ä¼˜åŒ–è¯´æ˜:")
    print("1. æ‰©å±•äº†å…³é”®è¯åº“ï¼Œå¢åŠ äº†æ›´å¤šè§¦å‘è¯æ±‡")
    print("2. æé«˜äº†å„ç±»è¯„åˆ†çš„æƒé‡å’Œä¸Šé™")
    print("3. å¢åŠ äº†è¿ç»­æŸ¥è¯¢åŠ æˆæœºåˆ¶")
    print("4. æ·»åŠ äº†è®°å¿†åŠ æˆéšæœºè§¦å‘")
    print("5. ä¼˜åŒ–äº†ç´¯ç§¯ç­–ç•¥ï¼Œæ›´å®¹æ˜“ç´¯ç§¯åˆ†æ•°")
    print("6. å¢åŠ äº†ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦å€æ•°")
    print("7. æ”¹è¿›äº†æ—¶é—´è¡°å‡ç­–ç•¥ï¼Œä¿ç•™æ›´å¤šå†å²ç´¯ç§¯")


if __name__ == "__main__":
    example_usage()
