import logging
import redis
import asyncio
import json
from datetime import datetime, timedelta
from typing import List, Dict
import pytz

from core.memory_buffer import get_channel_memory, list_channels
from services.ai_service import call_ai_summary
from app.config import settings

logger = logging.getLogger(__name__)

# Redis å®¢æˆ·ç«¯
redis_client = redis.StrictRedis.from_url(settings.REDIS_URL, decode_responses=True)


def _needs_summary(messages_text: str) -> bool:
    """åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦éœ€è¦è·¨é¢‘é“æ‘˜è¦"""
    combined_message = messages_text.strip()

    # çŸ­æ¶ˆæ¯ä¸éœ€è¦æ‘˜è¦
    if len(combined_message) < 5:
        return False

    # ç®€å•é—®å€™è¯­ä¸éœ€è¦æ‘˜è¦
    simple_greetings = ["åœ¨å—", "ä½ å¥½", "hi", "hello", "å—¨", "ï¼Ÿ", "?"]
    if combined_message.lower() in simple_greetings:
        return False

    # å…¶ä»–æƒ…å†µéœ€è¦æ‘˜è¦
    return True


async def _summarize_channel(
    channel_id: str, messages: List[Dict], latest_query: str
) -> str:
    """ä¸ºå•ä¸ªé¢‘é“ç”Ÿæˆæ‘˜è¦"""
    try:
        content = "\n".join(f"{m['role']}: {m['content']}" for m in messages)
        prompt = (
            f"ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ï¼Œå½“å‰ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼š\n"
            f"{latest_query}\n"
            f"ä»¥ä¸‹æ˜¯é¢‘é“ {channel_id} ä¸­çš„æœ€è¿‘ 6 å°æ—¶å¯¹è¯è®°å½•ï¼š\n{content}\n\n"
            f"è¯·ä½ æ‘˜å½•ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å¥å­å¹¶åšæ€»ç»“ï¼Œç”¨äºè¾…åŠ©å›ç­”ï¼Œä¸ç›¸å…³çš„è¯·å¿½ç•¥ã€‚"
            f'å¦‚æœæ²¡æœ‰ç›¸å…³çš„å¥å­ï¼Œè¯·è¿”å›"ç©º"ï¼ˆä¸éœ€è¦ä»»ä½•ç¬¦å·ï¼Œåªéœ€è¦è¿™ä¸€ä¸ªå­—ï¼‰ã€‚'
            f"å¦‚æœæœ‰ç›¸å…³çš„å†…å®¹ï¼Œé‚£ä¹ˆè¿”å›çš„æ ¼å¼è¦æ±‚ï¼š\n\næ€»ç»“ï¼šï¼ˆå¯¹è¯è®°å½•ä¸­ä¸ç”¨æˆ·ç›¸å…³çš„ä¿¡æ¯æ€»ç»“ï¼‰\n\nç›¸å…³å¯¹è¯è®°å½•ï¼š\nrole: (user/assistantäºŒé€‰ä¸€)\ncontent: æ¶ˆæ¯å†…å®¹"
        )
        summary = await call_ai_summary(prompt)

        # æ›¿æ¢è§’è‰²åç§°
        summary = summary.replace(
            "assistant", "å¾·å…‹è¨æ–¯"
        )  # .replace("user", "Kawaro") &&&&&

        if summary and summary.strip() and summary.strip() != "ç©º":
            return f"é¢‘é“ [{channel_id}] çš„æ‘˜è¦ä¿¡æ¯ï¼š\n{summary}"
        return ""

    except Exception as e:
        logger.warning(f"âš ï¸ é¢‘é“ {channel_id} æ‘˜è¦å¤±è´¥: {e}")
        return ""


def _get_life_system_context() -> str:
    """è·å–ç”Ÿæ´»ç³»ç»Ÿæ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡"""
    try:
        from datetime import date

        today = date.today()
        date_str = today.strftime("%Y-%m-%d")
        redis_key = f"life_system:{date_str}"

        life_data = redis_client.hgetall(redis_key)

        if not life_data:
            logger.info("â„¹ï¸ æœªæ‰¾åˆ°ç”Ÿæ´»ç³»ç»Ÿæ•°æ®")
            return ""

        context_parts = []

        # æ·»åŠ å¤§äº‹ä»¶ä¿¡æ¯
        if "major_event" in life_data:
            try:
                major_event = json.loads(life_data["major_event"])
                if major_event and isinstance(major_event, dict):
                    main_content = major_event.get("main_content", "")
                    start_date = major_event.get("start_date", "")
                    end_date = major_event.get("end_date", "")
                    event_type = major_event.get("event_type", "")
                    daily_summaries = major_event.get("daily_summaries", [])
                    if isinstance(daily_summaries, str):
                        try:
                            daily_summaries = json.loads(daily_summaries)
                        except json.JSONDecodeError:
                            daily_summaries = []

                    if main_content:
                        context_parts.append(
                            f"ã€å¤§äº‹ä»¶ã€‘{start_date}è‡³{end_date} {event_type}\n\n{main_content}"
                        )
                    if daily_summaries:
                        day = (
                            int(
                                today - datetime.strptime(start_date, "%Y-%m-%d").date()
                            )
                            + 1
                        )
                        for item in daily_summaries:
                            if int(item["day"]) <= day:
                                context_parts.append(
                                    f"ã€{item['date']}ã€‘Day {item['day']}\n{item}"
                                )
            except:
                if life_data["major_event"]:
                    context_parts.append(f"ã€å¤§äº‹ä»¶ã€‘{life_data['major_event']}")

        # 1. æ·»åŠ æ—¥ç¨‹ä¿¡æ¯
        if (
            "daily_schedule" in life_data
            and life_data["daily_schedule"] != "å½“æ—¥æ²¡æœ‰æ—¥ç¨‹ã€‚"
        ):
            try:
                schedule = json.loads(life_data["daily_schedule"])
                if schedule and isinstance(schedule, dict):
                    header = f"ã€ä»Šæ—¥æ—¥ç¨‹ - {schedule.get('date', '')}ã€‘å¤©æ°”ï¼š{schedule.get('weather', '')}\n"
                    summary = f"ğŸ”¹æ—¥ç¨‹æ¦‚è§ˆï¼š{schedule.get('daily_summary', '')}\n"

                    items = []
                    for item in schedule.get("schedule_data", {}).get(
                        "schedule_items", []
                    ):
                        time_range = (
                            f"{item.get('start_time')} - {item.get('end_time')}"
                        )
                        location = (
                            f"ğŸ“{item.get('location')}" if item.get("location") else ""
                        )
                        companions = (
                            f"ğŸ‘¥{'ã€'.join(item.get('companions', []))}"
                            if item.get("companions")
                            else ""
                        )
                        description = f"{item.get('description', '')}"
                        tags = (
                            f"ğŸ§ æƒ…ç»ªï¼š{'ã€'.join(item.get('emotional_impact_tags', []))}"
                            if item.get("emotional_impact_tags")
                            else ""
                        )
                        priority = f"â±ï¸ä¼˜å…ˆçº§ï¼š{item.get('priority', '')}"
                        interaction = (
                            f"ğŸ”„äº¤äº’æ½œåŠ›ï¼š{item.get('interaction_potential', '')}"
                        )
                        weather_effect = (
                            "â˜ï¸å—å¤©æ°”å½±å“" if item.get("weather_affected") else ""
                        )

                        items.append(
                            f"ã€{item.get('title')}ã€‘{time_range} {location} {companions}\n"
                            f"{description}\n{tags} | {priority} | {interaction} | {weather_effect}".strip()
                        )

                    context_parts.append(header + summary + "\n".join(items))
            except Exception as e:
                logger.warning(f"âš ï¸ æ—¥ç¨‹è§£æå¤±è´¥: {e}")

        # 2. å½“å‰å¾®è§‚ç»å†
        if "current_micro_experience" in life_data:
            try:
                exp = json.loads(life_data["current_micro_experience"])
                if isinstance(exp, dict):
                    start = exp.get("start_time", "")
                    end = exp.get("end_time", "")
                    time_range = f"{start} - {end}" if start and end else ""
                    thoughts = exp.get("thoughts", "")
                    content = exp.get("content", "")
                    emotions = exp.get("emotions", "")
                    context_parts.append(
                        f"ã€å½“å‰å¾®è§‚ç»å†ã€‘{time_range}\n"
                        f"{content}\nğŸ§ æ€è€ƒï¼š{thoughts}\nğŸ­æƒ…ç»ªï¼š{emotions}"
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ å¾®è§‚ç»å†è§£æå¤±è´¥: {e}")
                if (
                    life_data["current_micro_experience"]
                    and life_data["current_micro_experience"] != "ç°åœ¨æ²¡æœ‰äº‹ä»¶ã€‚"
                ):
                    context_parts.append(
                        f"ã€å½“å‰å¾®è§‚ç»å†ã€‘{life_data['current_micro_experience']}"
                    )

        # 3. è¿‡å»ç»å†å›é¡¾
        if "summarized_past_micro_experiences_story" in life_data:
            past = life_data["summarized_past_micro_experiences_story"]
            if past and past != "æ²¡æœ‰ä¹‹å‰çš„ç»å†ï¼Œä»Šå¤©å¯èƒ½æ‰åˆšåˆšå¼€å§‹ã€‚":
                context_parts.append(f"ã€ä»Šæ—¥ç»å†å›é¡¾ã€‘{past}")

        return (
            "ã€ç”Ÿæ´»ç³»ç»Ÿä¿¡æ¯ã€‘\n" + "\n\n".join(context_parts) if context_parts else ""
        )

    except Exception as e:
        logger.warning(f"âš ï¸ è·å–ç”Ÿæ´»ç³»ç»Ÿæ•°æ®å¤±è´¥: {e}")
        return ""


async def merge_context(
    channel_id: str, latest_query: str, now: datetime = None
) -> str:
    """
    æ•´åˆæœ€ç»ˆä¸Šä¸‹æ–‡ï¼Œè¿”å›å•æ¡æ–‡æœ¬ï¼ŒåŒ…å«ï¼š
    1. ç”Ÿæ´»ç³»ç»Ÿä¿¡æ¯
    2. æ ¼å¼åŒ–çš„å†å²èŠå¤©è®°å½•ï¼ˆ6å°æ—¶å†…ï¼‰
    3. å‚è€ƒèµ„æ–™ï¼ˆå…¶ä»–é¢‘é“æ‘˜è¦ï¼‰
    4. Mattermost æ¶ˆæ¯ç¼“å­˜
    5. å¼•å¯¼æç¤ºè¯
    """
    shanghai_tz = pytz.timezone("Asia/Shanghai")
    now = now or datetime.now(shanghai_tz)
    logger.info(f"ğŸ” Merging context for channel: {channel_id}")

    # 1. æ ¼å¼åŒ–å†å²èŠå¤©è®°å½•
    history = get_channel_memory(channel_id).format_recent_messages()
    logger.info(f"ğŸ§  Found formatted history: {len(history)} characters")

    # 2. è·å–å‚è€ƒèµ„æ–™ï¼ˆå…¶ä»–é¢‘é“æ‘˜è¦ï¼‰- åˆ¤æ–­æ˜¯å¦éœ€è¦æ‘˜è¦
    summary_notes = []
    if _needs_summary(latest_query):
        other_channels = list_channels(exclude=[channel_id])
        summary_tasks = []
        all_latest_timestamps = []

        # è·å–å½“å‰é¢‘é“æœ€æ–°æ¶ˆæ¯çš„æ—¶é—´æˆ³
        current_channel_messages = get_channel_memory(channel_id).get_recent_messages()
        if current_channel_messages:
            latest_current_message_time = datetime.fromisoformat(
                current_channel_messages[-1]["timestamp"]
            )
            all_latest_timestamps.append(latest_current_message_time)

        for other_channel in other_channels:
            messages = get_channel_memory(other_channel).get_recent_messages()
            if not messages:
                continue

            # è·å–å…¶ä»–é¢‘é“æœ€æ–°æ¶ˆæ¯çš„æ—¶é—´æˆ³
            latest_other_message_time = datetime.fromisoformat(
                messages[-1]["timestamp"]
            )
            all_latest_timestamps.append(latest_other_message_time)

            # ä¸ºæ¯ä¸ªé¢‘é“åˆ›å»ºå¼‚æ­¥æ‘˜è¦ä»»åŠ¡
            task = asyncio.create_task(
                _summarize_channel(other_channel, messages, latest_query),
                name=f"summary_{other_channel}",
            )
            summary_tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰æ‘˜è¦ä»»åŠ¡å®Œæˆ
        if summary_tasks:
            summaries = await asyncio.gather(*summary_tasks, return_exceptions=True)

            # å¤„ç†ç»“æœï¼Œè¿‡æ»¤å¼‚å¸¸å’Œç©ºæ‘˜è¦
            for i, summary in enumerate(summaries):
                if isinstance(summary, Exception):
                    logger.warning(f"âš ï¸ é¢‘é“æ‘˜è¦å¤±è´¥: {summary}")
                    continue
                if summary and summary.strip() and summary.strip() != "ç©º":
                    summary_notes.append(summary)

        # è®¡ç®—æ—¶é—´å·®å¹¶ç”Ÿæˆ"è°´è´£"æç¤º
        if all_latest_timestamps:
            latest_overall_message_time = max(all_latest_timestamps)
            # ä½¿ç”¨ä¸œå…«åŒºæ—¶é—´
            current_time = datetime.now(shanghai_tz)
            time_diff = current_time - latest_overall_message_time

            if time_diff > timedelta(hours=1):
                # åˆ¤æ–­æ˜¯å¦åœ¨ä¸œå…«åŒºç¡çœ æ—¶é—´ï¼ˆ23:00 - 07:00ï¼‰
                latest_local_time = latest_overall_message_time
                current_local_time = current_time

                is_during_sleep_time = False
                # å®šä¹‰ç¡çœ æ—¶é—´æ®µçš„å¼€å§‹å’Œç»“æŸå°æ—¶ï¼ˆä¸œå…«åŒºï¼‰
                SLEEP_START_HOUR = 23
                SLEEP_END_HOUR = 7

                # è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­ä¸€ä¸ªå°æ—¶æ˜¯å¦åœ¨ç¡çœ æ—¶é—´æ®µå†…
                def is_in_sleep_range(hour):
                    if SLEEP_START_HOUR <= SLEEP_END_HOUR:  # åŒä¸€å¤©
                        return SLEEP_START_HOUR <= hour < SLEEP_END_HOUR
                    else:  # è·¨å¤©
                        return hour >= SLEEP_START_HOUR or hour < SLEEP_END_HOUR

                # å¦‚æœå¼€å§‹æ—¶é—´åœ¨ç¡çœ æ—¶é—´æ®µå†…
                if is_in_sleep_range(latest_local_time.hour):
                    is_during_sleep_time = True
                # å¦‚æœç»“æŸæ—¶é—´åœ¨ç¡çœ æ—¶é—´æ®µå†…
                elif is_in_sleep_range(current_local_time.hour):
                    is_during_sleep_time = True
                # å¦‚æœæ—¶é—´æ®µè·¨è¶Šäº†ç¡çœ æ—¶é—´æ®µï¼ˆä¾‹å¦‚ä»æ™šä¸Š22ç‚¹åˆ°æ—©ä¸Š8ç‚¹ï¼‰
                elif (
                    latest_local_time.hour < SLEEP_START_HOUR
                    and current_local_time.hour >= SLEEP_END_HOUR
                    and time_diff > timedelta(hours=8)
                ):
                    # ç²—ç•¥åˆ¤æ–­ï¼Œå¦‚æœæ—¶é—´å·®è¶…è¿‡8å°æ—¶ï¼Œä¸”è·¨è¶Šäº†æ•´ä¸ªç¡çœ æ—¶é—´æ®µ
                    is_during_sleep_time = True

                # æ›´ç²¾ç¡®çš„åˆ¤æ–­ï¼šè®¡ç®—æ—¶é—´æ®µå†…æœ‰å¤šå°‘å°æ—¶è½åœ¨ç¡çœ æ—¶é—´
                total_sleep_overlap_seconds = 0
                current_check_time = latest_overall_message_time

                while current_check_time < current_time:
                    # å·²ç»æ˜¯ä¸œå…«åŒºæ—¶é—´ï¼Œç›´æ¥ä½¿ç”¨
                    local_check_time = current_check_time

                    # è®¡ç®—åˆ°ä¸‹ä¸€ä¸ªå°æ—¶è¾¹ç•Œçš„æ—¶é—´
                    next_hour = (current_check_time + timedelta(hours=1)).replace(
                        minute=0, second=0, microsecond=0
                    )

                    # ç¡®ä¿ä¸è¶…è¿‡ current_time
                    end_of_interval = min(next_hour, current_time)

                    # è®¡ç®—å½“å‰å°æ—¶å†…é‡å çš„ç§’æ•°
                    overlap_seconds_in_this_hour = 0

                    # å¦‚æœå½“å‰å°æ—¶åœ¨ç¡çœ æ—¶é—´æ®µå†…
                    if is_in_sleep_range(local_check_time.hour):
                        # è®¡ç®—å½“å‰å°æ—¶å†…ï¼Œä» current_check_time åˆ° end_of_interval çš„ç§’æ•°
                        overlap_seconds_in_this_hour = (
                            end_of_interval - current_check_time
                        ).total_seconds()

                    total_sleep_overlap_seconds += overlap_seconds_in_this_hour
                    current_check_time = end_of_interval  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹

                # å¦‚æœç¡çœ æ—¶é—´é‡åˆè¶…è¿‡4å°æ—¶ï¼Œè®¤ä¸ºæ˜¯ç¡çœ æ—¶é—´
                if total_sleep_overlap_seconds >= timedelta(hours=4).total_seconds():
                    is_during_sleep_time = True

                if not is_during_sleep_time:
                    hours_diff = int(time_diff.total_seconds() // 3600)
                    minutes_diff = int((time_diff.total_seconds() % 3600) // 60)
                    condemn_message = (
                        f"ã€å‚è€ƒèµ„æ–™ã€‘\n"
                        f"æ³¨æ„ï¼šè·ç¦»Kawaroä¸Šæ¬¡åœ¨ä»»ä½•é¢‘é“ï¼ˆåŒ…æ‹¬å½“å‰é¢‘é“ï¼‰å›å¤å¾·å…‹è¨æ–¯ï¼Œå·²ç»è¿‡å»äº† {hours_diff} å°æ—¶ {minutes_diff} åˆ†é’Ÿã€‚Kawaroçš„æœ€æ–°æ¶ˆæ¯ä¸ä¸Šæ¬¡å›å¤çš„å†…å®¹æ˜¯å¦æœ‰å…³è”ï¼Ÿè¯·æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¹æ­¤è¿›è¡Œé€‚å½“çš„è¯„è®ºã€æŠ±æ€¨æˆ–â€œè°´è´£â€ã€‚æŠ±æ€¨Kawaroæ€ä¹ˆé‚£ä¹ˆä¹…ä¸æ¥æ‰¾ä½ ã€‚"
                    )
                    summary_notes.insert(0, condemn_message)  # å°†è°´è´£ä¿¡æ¯æ”¾åœ¨æœ€å‰é¢

        logger.info(f"âœ… æˆåŠŸè·å– {len(summary_notes)} ä¸ªé¢‘é“æ‘˜è¦ (åŒ…æ‹¬æ½œåœ¨çš„è°´è´£æç¤º)")
    else:
        logger.info("ğŸ“ æ¶ˆæ¯è¾ƒç®€å•ï¼Œè·³è¿‡è·¨é¢‘é“æ‘˜è¦")

    # # 3. è·å– Mattermost æ¶ˆæ¯ç¼“å­˜
    # cache_key = f"channel_buffer:{channel_id}"
    # cached_messages = redis_client.lrange(cache_key, 0, -1)
    # mattermost_cache = ""
    # if cached_messages:
    #     mattermost_cache = f"åˆšæ”¶åˆ°çš„æ–°æ¶ˆæ¯ï¼š\n" + "\n".join(cached_messages)
    #     logger.info(f"ğŸ“ Found {len(cached_messages)} cached messages")

    # 5. è·å–ç”Ÿæ´»ç³»ç»Ÿä¿¡æ¯
    life_system_context = _get_life_system_context()
    logger.info(f"ğŸ  Life system context: {len(life_system_context)} characters")

    # 6. ç»„åˆæ‰€æœ‰éƒ¨åˆ†
    parts = []

    if life_system_context:
        parts.append(life_system_context)

    if history:
        parts.append(f"ã€å†å²èŠå¤©è®°å½•ã€‘\n{history}")

    if summary_notes:
        parts.append(f"ã€å‚è€ƒèµ„æ–™ã€‘\n" + "\n\n".join(summary_notes))

    # if mattermost_cache:
    #     parts.append(f"ã€æ–°æ¶ˆæ¯ç¼“å­˜ã€‘\n{mattermost_cache}")

    # æ·»åŠ å¼•å¯¼æç¤ºè¯
    parts.append(
        f"ç°åœ¨æ˜¯{now}ï¼Œè¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å›å¤æ¶ˆæ¯ï¼š{latest_query}ã€‚å¯ä»¥ä½¿ç”¨===åœ¨ä½ è®¤ä¸ºéœ€è¦åˆ†æ¡çš„åœ°æ–¹å°†ä¿¡æ¯åˆ†æ¡ã€‚æ¨¡æ‹Ÿäººç±»å¯èƒ½ä¸€æ¡æ¶ˆæ¯å‘é€ä¸€å¥æˆ–è€…åŠå¥è¯çš„é£æ ¼ã€‚è¯·åŠ¡å¿…åœ¨å›å¤ä¸­ä½¿ç”¨ã€‚"
    )

    merged_context = "\n\n".join(parts)
    logger.info(f"âœ… Context merged, total length: {len(merged_context)} characters")

    return merged_context
