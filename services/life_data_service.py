import json
from utils.logging_config import get_logger

logger = get_logger(__name__)
from datetime import date, datetime  # ç¡®ä¿ datetime ç±»è¢«æ­£ç¡®å¯¼å…¥
import redis
from app.config import settings
from app.life_system import LifeSystemQuery
from services.ai_service import summarize_past_micro_experiences  # å¯¼å…¥æ–°çš„AIæœåŠ¡
from core.state_manager import state_manager

# å¤ç”¨é¡¹ç›®ç°æœ‰çš„Redisè¿æ¥æ± 
from utils.redis_manager import get_redis_client
redis_client = get_redis_client()


class LifeDataService:
    def __init__(self):
        self.redis = redis_client

    async def _generate_summary_with_status_tracking(
        self,
        all_past_micro_experiences,
        current_exp_json,
        prev_past_micro_experiences_key,
        summary_generation_status_key,
        date_str,
    ):
        """ç”Ÿæˆæ±‡æ€»å¹¶è·Ÿè¸ªçŠ¶æ€"""
        logger.info("[LIFE_DATA] å¼€å§‹ç”Ÿæˆå¾®è§‚ç»å†æ±‡æ€»")

        # è®°å½•å¼€å§‹å°è¯•çš„çŠ¶æ€
        attempt_status = {
            "last_attempt_time": datetime.now().isoformat(),
            "last_attempt_data": current_exp_json,
            "last_success": "false",
            "attempt_count": str(
                int(
                    self.redis.hget(summary_generation_status_key, "attempt_count")
                    or "0"
                )
                + 1
            ),
        }
        self.redis.hset(summary_generation_status_key, mapping=attempt_status)
        self.redis.expire(summary_generation_status_key, 86400)

        try:
            # æ±‡æ€»è¿‡å»çš„å¾®è§‚ç»å†
            summarized_story = await summarize_past_micro_experiences(
                all_past_micro_experiences
            )

            # éªŒè¯ç”Ÿæˆç»“æœ
            if not summarized_story or summarized_story.strip() == "":
                logger.warning("AIæ±‡æ€»ç”Ÿæˆç»“æœä¸ºç©ºï¼Œä¿æŒé‡è¯•çŠ¶æ€")
                # æ›´æ–°å¤±è´¥çŠ¶æ€ï¼Œä½†ä¸æ›´æ–°æ•°æ®åŸºå‡†
                failure_status = {
                    "last_success": "false",
                    "last_error": "ç”Ÿæˆç»“æœä¸ºç©º",
                    "last_failure_time": datetime.now().isoformat(),
                }
                self.redis.hset(summary_generation_status_key, mapping=failure_status)
                return "æ±‡æ€»ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™..."
            else:
                logger.debug("[LIFE_DATA] AIæ±‡æ€»ç”ŸæˆæˆåŠŸ")
                # è®°å½•æˆåŠŸçŠ¶æ€
                success_status = {
                    "last_success": "true",
                    "last_success_time": datetime.now().isoformat(),
                    "last_error": "",  # æ¸…é™¤é”™è¯¯ä¿¡æ¯
                }
                self.redis.hset(summary_generation_status_key, mapping=success_status)

                # åªæœ‰åœ¨æˆåŠŸç”Ÿæˆåæ‰æ›´æ–°æ¯”è¾ƒåŸºå‡†
                self.redis.set(
                    prev_past_micro_experiences_key, current_exp_json, ex=86400
                )
                return summarized_story

        except Exception as e:
            logger.error(f"AIæ±‡æ€»ç”Ÿæˆå¤±è´¥: {str(e)}")
            # è®°å½•å¤±è´¥çŠ¶æ€ï¼Œä½†ä¸æ›´æ–°æ•°æ®åŸºå‡†
            failure_status = {
                "last_success": "false",
                "last_error": str(e),
                "last_failure_time": datetime.now().isoformat(),
            }
            self.redis.hset(summary_generation_status_key, mapping=failure_status)
            return f"æ±‡æ€»ç”Ÿæˆå¤±è´¥ï¼Œå°†åœ¨ä¸‹æ¬¡é‡è¯• (é”™è¯¯: {str(e)[:50]}...)"

    async def fetch_and_store_today_data(self):
        """è·å–å¹¶å­˜å‚¨å½“å¤©ç”Ÿæ´»ç³»ç»Ÿæ•°æ®åˆ°Redis"""
        try:
            # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
            today = date.today()
            date_str = today.strftime("%Y-%m-%d")
            current_time = datetime.now().strftime("%H:%M")

            logger.info(f"[LIFE_DATA] å¼€å§‹è·å–ç”Ÿæ´»ç³»ç»Ÿæ•°æ® date={date_str}")
            logger.debug(f"[LIFE_DATA] ç›®æ ‡æ—¥æœŸ: {date_str}, å½“å‰æ—¶é—´: {current_time}")

            # åˆå§‹åŒ–æŸ¥è¯¢å¯¹è±¡
            query = LifeSystemQuery(today)

            # è·å–å½“å¤©çš„å¤§äº‹ä»¶
            major_event = await query.get_major_event_info()

            # è·å–å½“å¤©çš„æ—¥ç¨‹
            daily_schedule = await query.get_daily_schedule_info()

            # è·å–å½“å‰æ—¶åˆ»çš„å¾®è§‚ç»å†
            current_micro_experience = None
            schedule_item = None  # åˆå§‹åŒ–schedule_itemå˜é‡
            logger.debug("[LIFE_DATA] è·å–å½“å‰æ—¶åˆ»çš„æ—¥ç¨‹é¡¹")

            # å…ˆè·å–å½“å‰æ—¶åˆ»çš„æ—¥ç¨‹é¡¹
            if (
                daily_schedule
                and "schedule_data" in daily_schedule
                and "schedule_items" in daily_schedule["schedule_data"]
            ):
                logger.debug("[LIFE_DATA] éå†æ—¥ç¨‹é¡¹")
                for item in daily_schedule["schedule_data"]["schedule_items"]:
                    logger.debug(f"[LIFE_DATA] æ—¥ç¨‹é¡¹ç»“æŸæ—¶é—´: {item.get('end_time')}")
                    item_start_time = item["start_time"]
                    item_end_time = item["end_time"]
                    item_start_time_obj = datetime.strptime(
                        item_start_time, "%H:%M"
                    ).time()
                    item_end_time_obj = datetime.strptime(item_end_time, "%H:%M").time()
                    current_time_obj = datetime.strptime(current_time, "%H:%M").time()
                    if item_start_time_obj <= current_time_obj <= item_end_time_obj:
                        schedule_item = item
                        logger.debug(f"[LIFE_DATA] åŒ¹é…çš„æ—¥ç¨‹é¡¹: {schedule_item}")
                        break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹å³å¯é€€å‡ºå¾ªç¯

            if schedule_item:
                logger.debug(f"[LIFE_DATA] æ‰¾åˆ°åŒ¹é…çš„æ—¥ç¨‹é¡¹: {schedule_item}")
                
                # [Stats Update] æ›´æ–°çŠ¶æ€ç®¡ç†å™¨çš„æ´»åŠ¨æ¶ˆè€—ç‡
                try:
                    metadata = schedule_item.get("metadata", {})
                    # è·å–é¢„ä¼°æ¶ˆè€— (é»˜è®¤5)
                    stamina_cost = float(metadata.get("stamina_cost", 5))
                    
                    # è®¡ç®—æŒç»­æ—¶é—´
                    fmt = "%H:%M"
                    try:
                        start_dt = datetime.strptime(schedule_item["start_time"], fmt)
                        end_dt = datetime.strptime(schedule_item["end_time"], fmt)
                        # ç®€å•çš„æ—¶é•¿è®¡ç®— (å¿½ç•¥è·¨å¤©å¸¦æ¥çš„è´Ÿæ•°é—®é¢˜ï¼Œå‡è®¾ schedule_item å†…éƒ¨å·²å¤„ç†æˆ–åœ¨æ­¤ä»…åšå·®å€¼ç»å¯¹å€¼)
                        # å¦‚æœ end < startï¼Œé€šå¸¸æ„å‘³ç€è·¨å¤©ï¼Œä¾‹å¦‚ 23:00 - 01:00ã€‚
                        # è¿™é‡Œæˆ‘ä»¬ç®€å•å¤„ç†ï¼šå¦‚æœ end < startï¼ŒåŠ  24 å°æ—¶
                        if end_dt < start_dt:
                             duration_hours = (end_dt - start_dt).total_seconds() / 3600.0 + 24
                        else:
                             duration_hours = (end_dt - start_dt).total_seconds() / 3600.0
                    except Exception:
                        duration_hours = 1.0

                    if duration_hours < 0.1: duration_hours = 0.5 # é¿å…æç«¯å€¼
                    
                    # è®¡ç®—æ¯å°æ—¶æ¶ˆè€—ç‡
                    rate = stamina_cost / duration_hours
                    
                    # åˆ¤æ–­æ˜¯å¦åœ¨ç¡è§‰
                    is_sleeping = False
                    category = schedule_item.get("category", "")
                    title = schedule_item.get("title", "")
                    if category == "rest" or "ç¡" in title:
                        is_sleeping = True
                        
                    state_manager.update_current_activity(rate, is_sleeping)
                    logger.debug(f"[LIFE_DATA] æ›´æ–°ç”Ÿç†çŠ¶æ€: rate={rate:.2f}/h, sleep={is_sleeping}")
                    
                except Exception as e:
                    logger.warning(f"[LIFE_DATA] æ›´æ–°ç”Ÿç†çŠ¶æ€å¤±è´¥: {e}")

                # è·å–è¯¥æ—¥ç¨‹é¡¹çš„å¾®è§‚ç»å†
                logger.debug("[LIFE_DATA] è·å–è¯¥æ—¥ç¨‹é¡¹çš„å¾®è§‚ç»å†")
                schedule_item_id = schedule_item.get("id")
                if schedule_item_id:
                    # è·å–è¯¥æ—¥ç¨‹é¡¹åœ¨å½“å‰æ—¶åˆ»çš„å¾®è§‚ç»å†
                    logger.debug("[LIFE_DATA] è·å–è¯¥æ—¥ç¨‹é¡¹åœ¨å½“å‰æ—¶åˆ»çš„å¾®è§‚ç»å†")
                    current_micro_experience = await query.get_micro_experience_at_time(
                        schedule_item_id, current_time
                    )
            else:
                # å½“å‰æ²¡æœ‰åŒ¹é…çš„æ—¥ç¨‹é¡¹ (é—²æš‡æ—¶é—´)
                # é»˜è®¤æ¶ˆè€—æä½ï¼Œä¸”ä¸ºæ¸…é†’çŠ¶æ€
                state_manager.update_current_activity(0.5, False)


            # è·å–å½“å‰æ—¶åˆ»ä¹‹å‰çš„æ‰€æœ‰å¾®è§‚ç»å†ï¼ˆä¸åŒ…æ‹¬å½“å‰æ—¶åˆ»ï¼‰
            all_past_micro_experiences = []
            if (
                daily_schedule
                and "schedule_data" in daily_schedule
                and "schedule_items" in daily_schedule["schedule_data"]
            ):
                logger.debug("[LIFE_DATA] è·å–å½“å‰æ—¶åˆ»ä¹‹å‰æ‰€æœ‰å¾®è§‚ç»å†")
                for item in daily_schedule["schedule_data"]["schedule_items"]:
                    item_time = item["start_time"]
                    item_start_time_obj = datetime.strptime(item_time, "%H:%M").time()
                    current_time_obj = datetime.strptime(current_time, "%H:%M").time()
                    if (
                        item_start_time_obj <= current_time_obj
                    ):  # åªåŒ…æ‹¬å½“å‰æ—¶åˆ»åŠä¹‹å‰çš„æ—¥ç¨‹é¡¹
                        # logger.info("[LIFE_DATA] æ—¥ç¨‹é¡¹å¼€å§‹æ—¶é—´å°äºç­‰äºå½“å‰æ—¶é—´!!!!!")
                        schedule_item_id = item.get("id")
                        if schedule_item_id:
                            # è·å–è¯¥æ—¥ç¨‹é¡¹çš„æ‰€æœ‰å¾®è§‚ç»å†
                            micro_experiences = (
                                await query.get_micro_experiences_for_schedule_item(
                                    schedule_item_id
                                )
                            )
                            # logger.info(
                            #     f"è·å–è¯¥æ—¥ç¨‹é¡¹çš„æ‰€æœ‰å¾®è§‚ç»å†: {micro_experiences}"
                            # )
                            if micro_experiences:
                                # è¿‡æ»¤å‡ºåœ¨å½“å‰æ—¶åˆ»ä¹‹å‰ç»“æŸçš„å¾®è§‚ç»å†
                                for exp in micro_experiences:
                                    for exp_item in exp.get("experiences", []):
                                        # ç¡®ä¿ç»å†æœ‰ç»“æŸæ—¶é—´
                                        if "end_time" in exp_item:
                                            # åªåŒ…æ‹¬åœ¨å½“å‰æ—¶åˆ»ä¹‹å‰ç»“æŸçš„ç»å†
                                            if exp_item["end_time"] <= current_time:
                                                all_past_micro_experiences.append(
                                                    exp_item
                                                )

            # Redis é”®å®šä¹‰
            prev_past_micro_experiences_key = (
                f"life_system:prev_past_micro_experiences:{date_str}"
            )
            summary_generation_status_key = f"life_system:summary_status:{date_str}"

            # è·å–ä¹‹å‰å­˜å‚¨çš„æ•°æ®å’ŒçŠ¶æ€
            prev_past_micro_experiences = self.redis.get(
                prev_past_micro_experiences_key
            )
            summary_status = self.redis.hgetall(summary_generation_status_key)

            # åºåˆ—åŒ–å½“å‰ç»å†ç”¨äºæ¯”è¾ƒ
            current_exp_json = (
                json.dumps(
                    all_past_micro_experiences, sort_keys=True, ensure_ascii=False
                )
                if all_past_micro_experiences
                else ""
            )

            logger.debug(f"[LIFE_DATA] curr: ...{current_exp_json[-100:]}")
            logger.debug(f"[LIFE_DATA] summary_status: {summary_status}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆæ±‡æ€»
            data_changed = prev_past_micro_experiences != current_exp_json
            last_generation_success = (
                summary_status.get("last_success", "false") == "true"
            )
            last_attempt_data = summary_status.get("last_attempt_data", "")

            if not current_exp_json:
                # æ²¡æœ‰å½“å‰ç»å†æ•°æ®
                summarized_past_micro_experiences_story = ""
                # æ¸…ç†çŠ¶æ€
                self.redis.delete(summary_generation_status_key)
                self.redis.set(
                    prev_past_micro_experiences_key, current_exp_json, ex=86400
                )

            elif data_changed:
                # æ•°æ®æœ‰å˜åŒ–ï¼Œæ— è®ºä¹‹å‰æ˜¯å¦æˆåŠŸéƒ½éœ€è¦é‡æ–°ç”Ÿæˆ
                logger.debug("[LIFE_DATA] å‘ç°æ•°æ®å·®å¼‚ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆæ±‡æ€»")
                summarized_past_micro_experiences_story = (
                    await self._generate_summary_with_status_tracking(
                        all_past_micro_experiences,
                        current_exp_json,
                        prev_past_micro_experiences_key,
                        summary_generation_status_key,
                        date_str,
                    )
                )

            elif not last_generation_success and last_attempt_data == current_exp_json:
                # æ•°æ®æ²¡å˜ä½†ä¸Šæ¬¡ç”Ÿæˆå¤±è´¥ï¼Œéœ€è¦é‡è¯•
                logger.debug("[LIFE_DATA] æ•°æ®æœªå˜åŒ–ä½†ä¸Šæ¬¡ç”Ÿæˆå¤±è´¥ï¼Œè¿›è¡Œé‡è¯•")
                summarized_past_micro_experiences_story = (
                    await self._generate_summary_with_status_tracking(
                        all_past_micro_experiences,
                        current_exp_json,
                        prev_past_micro_experiences_key,
                        summary_generation_status_key,
                        date_str,
                    )
                )

            else:
                # æ•°æ®æ²¡å˜åŒ–ä¸”ä¹‹å‰ç”ŸæˆæˆåŠŸï¼Œä½¿ç”¨ç°æœ‰æ±‡æ€»
                logger.debug("[LIFE_DATA] æ•°æ®æ— å˜åŒ–ä¸”ä¹‹å‰ç”ŸæˆæˆåŠŸï¼Œä½¿ç”¨ç°æœ‰æ±‡æ€»")
                main_data = self.redis.hgetall(f"life_system:{date_str}")
                existing_story = main_data.get(
                    "summarized_past_micro_experiences_story", ""
                )

                if not existing_story or existing_story in [
                    "",
                    "æ²¡æœ‰ä¹‹å‰çš„ç»å†ï¼Œä»Šå¤©å¯èƒ½æ‰åˆšåˆšå¼€å§‹ã€‚",
                ]:
                    # æ²¡æœ‰æœ‰æ•ˆæ±‡æ€»ä½†çŠ¶æ€æ˜¾ç¤ºæˆåŠŸï¼Œå¯èƒ½æ˜¯æ•°æ®ä¸¢å¤±ï¼Œé‡æ–°ç”Ÿæˆ
                    logger.debug("[LIFE_DATA] çŠ¶æ€æ˜¾ç¤ºæˆåŠŸä½†æœªæ‰¾åˆ°æœ‰æ•ˆæ±‡æ€»ï¼Œé‡æ–°ç”Ÿæˆ")
                    summarized_past_micro_experiences_story = (
                        await self._generate_summary_with_status_tracking(
                            all_past_micro_experiences,
                            current_exp_json,
                            prev_past_micro_experiences_key,
                            summary_generation_status_key,
                            date_str,
                        )
                    )
                else:
                    # è§£æç°æœ‰æ±‡æ€»
                    if existing_story.startswith('"'):
                        try:
                            summarized_past_micro_experiences_story = json.loads(
                                existing_story
                            )
                        except json.JSONDecodeError:
                            summarized_past_micro_experiences_story = existing_story
                    else:
                        summarized_past_micro_experiences_story = existing_story

            # å­˜å‚¨åˆ°Redis
            redis_key = f"life_system:{date_str}"
            data = {
                "major_event": (
                    json.dumps(major_event, ensure_ascii=False)
                    if major_event
                    else "ç°åœ¨æ²¡æœ‰ä»€ä¹ˆå¤§äº‹ä»¶ï¼Œåœ¨å¹³é™çš„é¾™é—¨ã€‚"
                ),
                "daily_schedule": (
                    json.dumps(daily_schedule, ensure_ascii=False)
                    if daily_schedule
                    else "å½“æ—¥æ²¡æœ‰æ—¥ç¨‹ã€‚"
                ),
                "current_micro_experience": (
                    json.dumps(current_micro_experience, ensure_ascii=False)
                    if current_micro_experience
                    else "ç°åœ¨æ²¡æœ‰äº‹ä»¶ã€‚"
                ),
                "past_micro_experiences": (
                    json.dumps(all_past_micro_experiences, ensure_ascii=False)
                    if all_past_micro_experiences
                    else "æ²¡æœ‰ä¹‹å‰çš„ç»å†ï¼Œä»Šå¤©å¯èƒ½æ‰åˆšåˆšå¼€å§‹ã€‚"
                ),
                "summarized_past_micro_experiences_story": (
                    json.dumps(
                        summarized_past_micro_experiences_story, ensure_ascii=False
                    )
                    if summarized_past_micro_experiences_story
                    else "æ²¡æœ‰ä¹‹å‰çš„ç»å†ï¼Œä»Šå¤©å¯èƒ½æ‰åˆšåˆšå¼€å§‹ã€‚"
                ),
            }

            # ä½¿ç”¨HSETå­˜å‚¨å“ˆå¸Œæ•°æ®
            self.redis.hset(redis_key, mapping=data)
            # è®¾ç½®24å°æ—¶è¿‡æœŸæ—¶é—´
            self.redis.expire(redis_key, 86400)

            logger.info(f"[LIFE_DATA] ç”Ÿæ´»ç³»ç»Ÿæ•°æ®å·²å­˜å‚¨åˆ° Redis: {redis_key}")

            return True

        except Exception as e:
            import traceback

            logger.error(f"è·å–å’Œå­˜å‚¨ç”Ÿæ´»æ•°æ®å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return False


# å•ä¾‹å®ä¾‹
life_data_service = LifeDataService()


async def main():
    """ç›´æ¥è¿è¡Œå…¥å£"""
    logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    # æ‰§è¡Œæ•°æ®è·å–å’Œå­˜å‚¨
    result = await life_data_service.fetch_and_store_today_data()
    if result:
        logger.info("[LIFE_DATA] ç”Ÿæ´»ç³»ç»Ÿæ•°æ®è·å–å’Œå­˜å‚¨æˆåŠŸ")
    else:
        logger.error("ç”Ÿæ´»ç³»ç»Ÿæ•°æ®è·å–å’Œå­˜å‚¨å¤±è´¥")

    # æ‰“å°å­˜å‚¨åœ¨Redisä¸­çš„æ•°æ®å’ŒçŠ¶æ€
    today = datetime.date.today().strftime("%Y-%m-%d")
    redis_key = f"life_system:{today}"
    status_key = f"life_system:summary_status:{today}"

    stored_data = redis_client.hgetall(redis_key)
    status_data = redis_client.hgetall(status_key)

    if stored_data:
        logger.debug(f"[LIFE_DATA] Rediså­˜å‚¨çš„æ•°æ® ({redis_key}):")
        for key, value in stored_data.items():
            # å°è¯•è§£æJSONå€¼
            try:
                parsed_value = json.loads(value)
            except Exception:
                logger.debug(f"[LIFE_DATA] {key}: {value}")
    else:
        logger.warning(f"â„¹ï¸ æœªæ‰¾åˆ°Redisé”®: {redis_key}")

    if status_data:
        logger.debug(f"[LIFE_DATA] ğŸ“Š ç”ŸæˆçŠ¶æ€ä¿¡æ¯ ({status_key}):")
        for key, value in status_data.items():
            logger.debug(f"[LIFE_DATA] {key}: {value}")
    else:
        logger.debug("[LIFE_DATA] ğŸ“Š æœªæ‰¾åˆ°ç”ŸæˆçŠ¶æ€ä¿¡æ¯")


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
