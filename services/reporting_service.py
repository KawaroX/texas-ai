"""
ç‹¬ç«‹å›¾ç‰‡ç”Ÿæˆç›‘æ§æŠ¥å‘ŠæœåŠ¡

ä¸“æ³¨äºç›‘æ§å’Œåˆ†æå¢å¼ºå›¾ç‰‡ç”ŸæˆåŠŸèƒ½çš„æ•ˆæœï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŠ¥å‘Šå±•ç¤ºè¿è¡ŒçŠ¶æ€å’ŒAIåˆ†æå»ºè®®ã€‚
è®¾è®¡ä¸ºå®Œå…¨ç‹¬ç«‹çš„æ¨¡å—ï¼Œå¯é€šè¿‡Redisé…ç½®åŠ¨æ€æ§åˆ¶ï¼Œå¤±è´¥ä¸å½±å“ä¸»ä¸šåŠ¡æµç¨‹ã€‚
"""
import json
import os
import logging
import httpx
import redis
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class ReportingConfig:
    """æŠ¥å‘Šç³»ç»Ÿé…ç½®"""
    enabled: bool = True
    frequency: str = "daily"  # daily, weekly, manual
    ai_analysis: bool = True
    
    # å›ºå®šé…ç½®
    target_channel: str = "eqgikba1opnpupiy3w16icdxoo"
    mattermost_token: str = "8or4yqexc3r6brji6s4acp1ycr"
    mattermost_url: str = "https://prts.kawaro.space"
    
    @property
    def openrouter_api_key(self) -> str:
        """ä»ç¯å¢ƒå˜é‡è·å–OpenRouter APIå¯†é’¥"""
        return os.getenv('OPENROUTER_API_KEY', '')

@dataclass 
class EnhancementMetrics:
    """å¢å¼ºåŠŸèƒ½æŒ‡æ ‡"""
    total_events: int = 0
    enhanced_data_used: int = 0
    fallback_to_original: int = 0
    companions_detection: int = 0
    string_detection: int = 0
    prompt_enhancement_success: int = 0
    prompt_enhancement_failed: int = 0

@dataclass
class GenerationMetrics:
    """å›¾ç‰‡ç”Ÿæˆæ€»ä½“æŒ‡æ ‡"""
    total_attempts: int = 0
    successful_generations: int = 0
    failed_generations: int = 0
    average_duration: float = 0.0
    selfie_count: int = 0
    scene_count: int = 0
    scene_with_characters_count: int = 0
    error_types: Dict[str, int] = None
    
    def __post_init__(self):
        if self.error_types is None:
            self.error_types = {}

class ImageGenerationReportingService:
    """å›¾ç‰‡ç”ŸæˆæŠ¥å‘ŠæœåŠ¡"""
    
    def __init__(self):
        from app.config import settings
        self.redis_url = settings.REDIS_URL
        self.redis_client = redis.StrictRedis.from_url(self.redis_url, decode_responses=True)
        self.monitoring_logs_dir = Path("/app/image_generation_logs")
        self.process_tracking_key = "image_generation_process_tracking"
        
    def _get_config(self) -> ReportingConfig:
        """ä»Redisè·å–æŠ¥å‘Šé…ç½®ï¼ˆåŠ¨æ€æ§åˆ¶ï¼‰"""
        try:
            config_str = self.redis_client.get("reporting_config")
            config = ReportingConfig()  # ä½¿ç”¨é»˜è®¤å€¼ï¼ˆåŒ…æ‹¬å›ºå®šé…ç½®ï¼‰
            
            if config_str:
                # åªæ›´æ–°åŠ¨æ€é…ç½®é¡¹
                config_data = json.loads(config_str)
                config.enabled = config_data.get('enabled', True)
                config.frequency = config_data.get('frequency', 'daily')
                config.ai_analysis = config_data.get('ai_analysis', True)
            else:
                # åˆå§‹åŒ–åŠ¨æ€é…ç½®
                dynamic_config = {
                    'enabled': True,
                    'frequency': 'daily',
                    'ai_analysis': True
                }
                self.redis_client.set("reporting_config", json.dumps(dynamic_config), ex=86400*7)
                
            return config
        except Exception as e:
            logger.warning(f"[reporting] è·å–é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return ReportingConfig()
    
    def _save_config(self, config: ReportingConfig):
        """ä¿å­˜åŠ¨æ€é…ç½®åˆ°Redis"""
        try:
            # åªä¿å­˜åŠ¨æ€é…ç½®é¡¹
            dynamic_config = {
                'enabled': config.enabled,
                'frequency': config.frequency,
                'ai_analysis': config.ai_analysis
            }
            self.redis_client.set("reporting_config", json.dumps(dynamic_config), ex=86400*7)
            logger.info("[reporting] åŠ¨æ€é…ç½®å·²ä¿å­˜åˆ°Redis")
        except Exception as e:
            logger.error(f"[reporting] ä¿å­˜é…ç½®å¤±è´¥: {e}")
    
    async def collect_enhancement_data(self, target_date: str = None) -> EnhancementMetrics:
        """æ”¶é›†å¢å¼ºåŠŸèƒ½ç›¸å…³æ•°æ®"""
        if target_date is None:
            target_date = datetime.now().strftime('%Y-%m-%d')
            
        metrics = EnhancementMetrics()
        
        try:
            # 1. ç»Ÿè®¡Redisä¸­çš„æ•°æ®é‡å¯¹æ¯”
            original_key = f"interaction_needed:{target_date}"
            enhanced_key = f"interaction_needed_enhanced:{target_date}"
            
            original_count = self.redis_client.zcard(original_key) if self.redis_client.exists(original_key) else 0
            enhanced_count = self.redis_client.zcard(enhanced_key) if self.redis_client.exists(enhanced_key) else 0
            
            metrics.total_events = max(original_count, enhanced_count)  # åº”è¯¥ç›¸ç­‰ï¼Œå–è¾ƒå¤§å€¼ä½œä¸ºæ€»æ•°
            
            # 2. æ”¶é›†å®æ—¶è¿‡ç¨‹è¿½è¸ªæ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            tracking_data = self._get_process_tracking_data(target_date)
            if tracking_data:
                metrics.enhanced_data_used = tracking_data.get("enhanced_data_used", 0)
                metrics.fallback_to_original = tracking_data.get("fallback_to_original", 0)
                metrics.companions_detection = tracking_data.get("companions_detection", 0)
                metrics.string_detection = tracking_data.get("string_detection", 0)
                metrics.prompt_enhancement_success = tracking_data.get("prompt_enhancement_success", 0)
                metrics.prompt_enhancement_failed = tracking_data.get("prompt_enhancement_failed", 0)
            
            logger.info(f"[reporting] å¢å¼ºåŠŸèƒ½æ•°æ®æ”¶é›†å®Œæˆ: {target_date}")
            return metrics
            
        except Exception as e:
            logger.error(f"[reporting] æ”¶é›†å¢å¼ºåŠŸèƒ½æ•°æ®å¤±è´¥: {e}")
            return metrics
    
    async def collect_generation_data(self, target_date: str = None) -> GenerationMetrics:
        """æ”¶é›†å›¾ç‰‡ç”Ÿæˆæ€»ä½“æ•°æ®"""
        if target_date is None:
            target_date = datetime.now().strftime('%Y-%m-%d')
            
        metrics = GenerationMetrics()
        
        try:
            # ä»ç›‘æ§æ–‡ä»¶è¯»å–æ•°æ®
            daily_log_path = self.monitoring_logs_dir / target_date / "generation_log.jsonl"
            daily_summary_path = self.monitoring_logs_dir / target_date / "daily_summary.json"
            
            # è¯»å–è¯¦ç»†è®°å½•
            if daily_log_path.exists():
                with open(daily_log_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            record = json.loads(line.strip())
                            metrics.total_attempts += 1
                            
                            if record.get("success"):
                                metrics.successful_generations += 1
                            else:
                                metrics.failed_generations += 1
                                # ç»Ÿè®¡é”™è¯¯ç±»å‹
                                error = record.get("error", "Unknown")
                                metrics.error_types[error] = metrics.error_types.get(error, 0) + 1
                            
                            # ç»Ÿè®¡ç”Ÿæˆç±»å‹
                            gen_type = record.get("generation_type", "unknown")
                            if gen_type == "selfie":
                                metrics.selfie_count += 1
                            elif gen_type == "scene":
                                metrics.scene_count += 1
                            elif gen_type == "scene_with_characters":
                                metrics.scene_with_characters_count += 1
                                
                        except json.JSONDecodeError:
                            continue
            
            # è¯»å–æ±‡æ€»æ•°æ®
            if daily_summary_path.exists():
                with open(daily_summary_path, 'r', encoding='utf-8') as f:
                    summary = json.load(f)
                    metrics.average_duration = summary.get("average_duration", 0.0)
            
            logger.info(f"[reporting] å›¾ç‰‡ç”Ÿæˆæ•°æ®æ”¶é›†å®Œæˆ: {target_date}")
            return metrics
            
        except Exception as e:
            logger.error(f"[reporting] æ”¶é›†å›¾ç‰‡ç”Ÿæˆæ•°æ®å¤±è´¥: {e}")
            return metrics
    
    def _get_process_tracking_data(self, target_date: str) -> Dict:
        """è·å–è¿‡ç¨‹è¿½è¸ªæ•°æ®"""
        try:
            tracking_key = f"{self.process_tracking_key}:{target_date}"
            data_str = self.redis_client.get(tracking_key)
            if data_str:
                return json.loads(data_str)
            return {}
        except Exception as e:
            logger.warning(f"[reporting] è·å–è¿‡ç¨‹è¿½è¸ªæ•°æ®å¤±è´¥: {e}")
            return {}
    
    def generate_enhancement_report(self, metrics: EnhancementMetrics, target_date: str) -> str:
        """ç”Ÿæˆå¢å¼ºåŠŸèƒ½æ•ˆæœæŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰"""
        report = f"""# ğŸš€ å›¾ç‰‡ç”Ÿæˆå¢å¼ºåŠŸèƒ½æ•ˆæœæŠ¥å‘Š

**æ—¥æœŸ**: {target_date}  
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š æ•°æ®æºä½¿ç”¨æƒ…å†µ

### æ€»ä½“ç»Ÿè®¡
- **æ€»äº¤äº’äº‹ä»¶æ•°**: {metrics.total_events}
- **å¢å¼ºæ•°æ®ä½¿ç”¨**: {metrics.enhanced_data_used} æ¬¡
- **å›é€€åˆ°åŸå§‹æ•°æ®**: {metrics.fallback_to_original} æ¬¡
- **æ•°æ®æºæˆåŠŸç‡**: {(metrics.enhanced_data_used / max(metrics.total_events, 1) * 100):.1f}%

### ä½¿ç”¨åˆ†å¸ƒ
```
å¢å¼ºæ•°æ®: {'â–ˆ' * min(20, max(1, metrics.enhanced_data_used))}
åŸå§‹æ•°æ®: {'â–ˆ' * min(20, max(1, metrics.fallback_to_original))}
```

---

## ğŸ¯ è§’è‰²æ£€æµ‹å‡†ç¡®æ€§åˆ†æ

### æ£€æµ‹æ–¹å¼å¯¹æ¯”
- **Companionsæ•°ç»„æ£€æµ‹**: {metrics.companions_detection} æ¬¡ âœ¨
- **å­—ç¬¦ä¸²åŒ¹é…æ£€æµ‹**: {metrics.string_detection} æ¬¡ ğŸ“¦
- **æ–°æ£€æµ‹æ–¹å¼å æ¯”**: {(metrics.companions_detection / max(metrics.companions_detection + metrics.string_detection, 1) * 100):.1f}%

### å‡†ç¡®æ€§æå‡
```
æ–°æ–¹æ³•(companions): {'â–ˆ' * min(20, max(1, metrics.companions_detection))}
æ—§æ–¹æ³•(å­—ç¬¦ä¸²):     {'â–ˆ' * min(20, max(1, metrics.string_detection))}
```

---

## âœ¨ æç¤ºè¯å¢å¼ºæ•ˆæœ

### å¢å¼ºæ„å»ºç»Ÿè®¡
- **å¢å¼ºæˆåŠŸ**: {metrics.prompt_enhancement_success} æ¬¡ âœ…
- **å¢å¼ºå¤±è´¥**: {metrics.prompt_enhancement_failed} æ¬¡ âš ï¸
- **å¢å¼ºæˆåŠŸç‡**: {(metrics.prompt_enhancement_success / max(metrics.prompt_enhancement_success + metrics.prompt_enhancement_failed, 1) * 100):.1f}%

---

## ğŸ”’ å‘åå…¼å®¹æ€§éªŒè¯

### å…¼å®¹æ€§çŠ¶æ€
- **å›é€€æœºåˆ¶è§¦å‘**: {metrics.fallback_to_original} æ¬¡
- **ç³»ç»Ÿç¨³å®šæ€§**: {'ğŸŸ¢ ä¼˜ç§€' if metrics.fallback_to_original < metrics.total_events * 0.1 else 'ğŸŸ¡ éœ€å…³æ³¨' if metrics.fallback_to_original < metrics.total_events * 0.5 else 'ğŸ”´ éœ€æ£€æŸ¥'}
- **å…¼å®¹æ€§è¯„åˆ†**: {max(0, min(100, 100 - (metrics.fallback_to_original / max(metrics.total_events, 1) * 100))):.0f}/100

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### åŠŸèƒ½é‡‡ç”¨æƒ…å†µ
{'ğŸ‰ å¢å¼ºåŠŸèƒ½è¿è¡Œè‰¯å¥½ï¼æ•°æ®æºä¸»è¦ä½¿ç”¨å¢å¼ºç‰ˆæœ¬ã€‚' if metrics.enhanced_data_used > metrics.fallback_to_original else 'âš ï¸ å¢å¼ºæ•°æ®ä½¿ç”¨ç‡è¾ƒä½ï¼Œéœ€è¦æ£€æŸ¥æ•°æ®ç”Ÿæˆæµç¨‹ã€‚'}

### è§’è‰²æ£€æµ‹æ”¹è¿›
{'âœ¨ æ–°çš„è§’è‰²æ£€æµ‹æ–¹å¼å·¥ä½œæ­£å¸¸ï¼' if metrics.companions_detection > 0 else 'ğŸ“¦ ä¸»è¦ä½¿ç”¨ä¼ ç»Ÿå­—ç¬¦ä¸²åŒ¹é…ï¼Œå¢å¼ºæ£€æµ‹å¯èƒ½æœªç”Ÿæ•ˆã€‚'}

### æç¤ºè¯å¢å¼ºçŠ¶æ€  
{'ğŸš€ æç¤ºè¯å¢å¼ºåŠŸèƒ½è¡¨ç°è‰¯å¥½ï¼' if metrics.prompt_enhancement_success > metrics.prompt_enhancement_failed else 'ğŸ”§ æç¤ºè¯å¢å¼ºéœ€è¦ä¼˜åŒ–ã€‚'}

---

*æŠ¥å‘Šç”± Texas AI å¢å¼ºç›‘æ§ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        return report
    
    def generate_generation_report(self, metrics: GenerationMetrics, target_date: str) -> str:
        """ç”Ÿæˆå›¾ç‰‡ç”Ÿæˆæ€»ä½“è¡¨ç°æŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰"""
        success_rate = (metrics.successful_generations / max(metrics.total_attempts, 1)) * 100
        
        report = f"""# ğŸ“¸ å›¾ç‰‡ç”Ÿæˆæ€»ä½“è¡¨ç°æŠ¥å‘Š

**æ—¥æœŸ**: {target_date}  
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“ˆ æ€»ä½“æ€§èƒ½æŒ‡æ ‡

### ç”Ÿæˆç»Ÿè®¡
- **æ€»å°è¯•æ¬¡æ•°**: {metrics.total_attempts}
- **æˆåŠŸç”Ÿæˆ**: {metrics.successful_generations}
- **å¤±è´¥æ¬¡æ•°**: {metrics.failed_generations}
- **æˆåŠŸç‡**: {success_rate:.1f}%
- **å¹³å‡è€—æ—¶**: {metrics.average_duration:.1f}ç§’

### æˆåŠŸç‡è¶‹åŠ¿
```
æˆåŠŸ: {'â–ˆ' * min(20, max(1, int(success_rate/5)))}
å¤±è´¥: {'â–ˆ' * min(20, max(1, int((100-success_rate)/5)))}
```

---

## ğŸ¨ ç”Ÿæˆç±»å‹åˆ†å¸ƒ

### ç±»å‹ç»Ÿè®¡
- **è‡ªæ‹å›¾ç‰‡**: {metrics.selfie_count} å¼  ğŸ“¸
- **åœºæ™¯å›¾ç‰‡**: {metrics.scene_count} å¼  ğŸ¨  
- **å¤šè§’è‰²åœºæ™¯**: {metrics.scene_with_characters_count} å¼  ğŸ‘¥

### åˆ†å¸ƒå¯è§†åŒ–
```
è‡ªæ‹:     {'â–ˆ' * min(15, max(1, metrics.selfie_count))} ({metrics.selfie_count})
åœºæ™¯:     {'â–ˆ' * min(15, max(1, metrics.scene_count))} ({metrics.scene_count})
å¤šè§’è‰²:   {'â–ˆ' * min(15, max(1, metrics.scene_with_characters_count))} ({metrics.scene_with_characters_count})
```

---

## âŒ é”™è¯¯ç±»å‹åˆ†æ

### ä¸»è¦é”™è¯¯ç±»å‹
"""
        
        if metrics.error_types:
            for error_type, count in sorted(metrics.error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                report += f"- **{error_type}**: {count} æ¬¡\n"
        else:
            report += "- ğŸ‰ å½“æ—¥æ— é”™è¯¯è®°å½•\n"
            
        report += f"""
### é”™è¯¯ç‡åˆ†æ
- **é”™è¯¯ç‡**: {((metrics.failed_generations / max(metrics.total_attempts, 1)) * 100):.1f}%
- **ç³»ç»Ÿå¥åº·åº¦**: {'ğŸŸ¢ ä¼˜ç§€' if success_rate >= 90 else 'ğŸŸ¡ è‰¯å¥½' if success_rate >= 70 else 'ğŸŸ  éœ€å…³æ³¨' if success_rate >= 50 else 'ğŸ”´ éœ€æ£€æŸ¥'}

---

## ğŸ“Š æ€§èƒ½åˆ†æ

### å¹³å‡è€—æ—¶è¯„ä¼°
- **å½“å‰è€—æ—¶**: {metrics.average_duration:.1f}ç§’
- **æ€§èƒ½ç­‰çº§**: {'ğŸš€ æå¿«' if metrics.average_duration < 30 else 'âš¡ å¿«é€Ÿ' if metrics.average_duration < 60 else 'ğŸ”„ æ­£å¸¸' if metrics.average_duration < 120 else 'â³ è¾ƒæ…¢'}

### ç”Ÿæˆæ•ˆç‡
- **æ¯å°æ—¶äº§èƒ½**: {(3600 / max(metrics.average_duration, 1)):.0f} å¼ ï¼ˆç†è®ºå€¼ï¼‰
- **å®é™…äº§èƒ½**: å–å†³äºäº¤äº’äº‹ä»¶é¢‘ç‡

---

## ğŸ’¡ æ€»ç»“ä¸å»ºè®®

### ç³»ç»ŸçŠ¶æ€
{'ğŸ‰ ç³»ç»Ÿè¿è¡ŒçŠ¶å†µè‰¯å¥½ï¼' if success_rate >= 80 else 'âš ï¸ ç³»ç»Ÿéœ€è¦å…³æ³¨å’Œä¼˜åŒ–ã€‚'}

### å…³é”®æŒ‡æ ‡
- æˆåŠŸç‡: {success_rate:.1f}%
- å¹³å‡è€—æ—¶: {metrics.average_duration:.1f}s
- é”™è¯¯ç±»å‹: {len(metrics.error_types)} ç§

---

*æŠ¥å‘Šç”± Texas AI ç›‘æ§ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        return report
    
    async def get_ai_analysis(self, report_content: str, report_type: str) -> str:
        """è°ƒç”¨OpenRouter APIè¿›è¡ŒAIåˆ†æ"""
        config = self._get_config()
        if not config.openrouter_api_key:
            logger.warning("[reporting] OpenRouter APIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡AIåˆ†æ")
            return "âš ï¸ AIåˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼ˆAPIå¯†é’¥æœªé…ç½®ï¼‰"
            
        try:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç³»ç»Ÿåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å›¾ç‰‡ç”Ÿæˆç³»ç»Ÿçš„{report_type}æŠ¥å‘Šï¼Œæä¾›ä¸“ä¸šçš„è§è§£å’Œå»ºè®®ã€‚

æŠ¥å‘Šå†…å®¹ï¼š
{report_content}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªè§’åº¦è¿›è¡Œåˆ†æï¼š
1. ç³»ç»Ÿæ€§èƒ½è¡¨ç°è¯„ä¼°
2. æ½œåœ¨é—®é¢˜è¯†åˆ«
3. ä¼˜åŒ–å»ºè®®
4. é£é™©é¢„è­¦
5. æ”¹è¿›ä¼˜å…ˆçº§

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼Œç»™å‡ºå…·ä½“å¯è¡Œçš„å»ºè®®ã€‚"""

            headers = {
                "Authorization": f"Bearer {config.openrouter_api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://prts.kawaro.space",
                "X-Title": "Texas AI Reporting System"
            }
            
            payload = {
                "model": "deepseek/deepseek-chat-v3.1:free",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=payload
                )
                response.raise_for_status()
                
                result = response.json()
                ai_analysis = result["choices"][0]["message"]["content"]
                
                logger.info(f"[reporting] AIåˆ†æå®Œæˆï¼Œé•¿åº¦: {len(ai_analysis)}")
                return ai_analysis
                
        except Exception as e:
            logger.error(f"[reporting] AIåˆ†æå¤±è´¥: {e}")
            return f"âš ï¸ AIåˆ†ææš‚æ—¶ä¸å¯ç”¨: {str(e)[:100]}..."
    
    async def send_to_mattermost(self, content: str, config: ReportingConfig) -> bool:
        """å‘é€æŠ¥å‘Šåˆ°Mattermost"""
        try:
            url = f"{config.mattermost_url}/api/v4/posts"
            headers = {
                "Authorization": f"Bearer {config.mattermost_token}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "channel_id": config.target_channel,
                "message": content
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(url, headers=headers, json=payload)
                response.raise_for_status()
                
                logger.info("[reporting] æŠ¥å‘Šå·²æˆåŠŸå‘é€åˆ°Mattermost")
                return True
                
        except Exception as e:
            logger.error(f"[reporting] å‘é€Mattermostæ¶ˆæ¯å¤±è´¥: {e}")
            return False
    
    async def generate_and_send_reports(self, target_date: str = None) -> Dict[str, bool]:
        """ç”Ÿæˆå¹¶å‘é€å®Œæ•´æŠ¥å‘Š"""
        if target_date is None:
            target_date = datetime.now().strftime('%Y-%m-%d')
            
        config = self._get_config()
        if not config.enabled:
            logger.info("[reporting] æŠ¥å‘ŠåŠŸèƒ½å·²ç¦ç”¨")
            return {"enabled": False}
        
        results = {}
        
        try:
            logger.info(f"[reporting] å¼€å§‹ç”Ÿæˆ {target_date} çš„æŠ¥å‘Š...")
            
            # 1. æ”¶é›†æ•°æ®
            enhancement_metrics = await self.collect_enhancement_data(target_date)
            generation_metrics = await self.collect_generation_data(target_date)
            
            # 2. ç”Ÿæˆå¢å¼ºåŠŸèƒ½æŠ¥å‘Š
            enhancement_report = self.generate_enhancement_report(enhancement_metrics, target_date)
            
            # 3. AIåˆ†æå¢å¼ºåŠŸèƒ½æŠ¥å‘Š
            if config.ai_analysis:
                enhancement_ai_analysis = await self.get_ai_analysis(enhancement_report, "å¢å¼ºåŠŸèƒ½")
                enhancement_full_report = f"{enhancement_report}\n\n---\n\n# ğŸ¤– AI æ™ºèƒ½åˆ†æ\n\n{enhancement_ai_analysis}"
            else:
                enhancement_full_report = enhancement_report
            
            # 4. å‘é€å¢å¼ºåŠŸèƒ½æŠ¥å‘Š
            results["enhancement_report"] = await self.send_to_mattermost(enhancement_full_report, config)
            
            # 5. ç”Ÿæˆæ€»ä½“æ€§èƒ½æŠ¥å‘Š
            generation_report = self.generate_generation_report(generation_metrics, target_date)
            
            # 6. AIåˆ†ææ€»ä½“æ€§èƒ½æŠ¥å‘Š
            if config.ai_analysis:
                generation_ai_analysis = await self.get_ai_analysis(generation_report, "æ€»ä½“æ€§èƒ½")
                generation_full_report = f"{generation_report}\n\n---\n\n# ğŸ¤– AI æ™ºèƒ½åˆ†æ\n\n{generation_ai_analysis}"
            else:
                generation_full_report = generation_report
            
            # 7. å‘é€æ€»ä½“æ€§èƒ½æŠ¥å‘Š
            results["generation_report"] = await self.send_to_mattermost(generation_full_report, config)
            
            logger.info("[reporting] æŠ¥å‘Šç”Ÿæˆå’Œå‘é€å®Œæˆ")
            return results
            
        except Exception as e:
            logger.error(f"[reporting] æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return {"error": str(e)}

# å…¨å±€å®ä¾‹
reporting_service = ImageGenerationReportingService()