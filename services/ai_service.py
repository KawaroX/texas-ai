import os
import asyncio
import httpx
import logging

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1/chat/completions"

logger = logging.getLogger(__name__)


async def call_openrouter(
    messages, model="cognitivecomputations/dolphin-mistral-24b-venice-edition:free"
) -> str:
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": model,
        "messages": messages,
    }

    try:
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹: {model}")
        async with httpx.AsyncClient() as client:
            response = await client.post(
                OPENROUTER_BASE_URL, headers=headers, json=payload
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
        else:
            logger.error(f"âŒ OpenRouterè°ƒç”¨å¤±è´¥ (çŠ¶æ€ç  {e.response.status_code}): {e}")
        return ""
    except Exception as e:
        logger.error(f"âŒ OpenRouterè°ƒç”¨å¤±è´¥: {e}")
        return ""


async def stream_ai_chat(messages: list) -> str:
    """
    æµå¼è°ƒç”¨ AI ç”Ÿæˆå¯¹è¯å›å¤ã€‚
    """
    # å‡è®¾ call_openrouter å·²ç»æ”¯æŒæµå¼è¾“å‡ºï¼Œæˆ–è€…è¿™é‡Œè¿›è¡Œæ¨¡æ‹Ÿ
    # ç›®å‰ call_openrouter è¿”å›çš„æ˜¯å®Œæ•´å›å¤ï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦æ¨¡æ‹Ÿæµå¼
    full_reply = await call_openrouter(messages)
    
    logger.info(f"ğŸ¤–ï¸ OpenRouterå›å¤: {full_reply}")

    if not full_reply.strip():
        yield "[è‡ªåŠ¨å›å¤]æ­£åœ¨å¼€è½¦ï¼Œç­‰ä¼šå›å¤"
        return

    # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ŒæŒ‰å­—ç¬¦æˆ–å°æ®µè¿”å›
    for char in full_reply:
        yield char
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå»¶è¿Ÿ


async def call_ai_summary(prompt: str) -> str:
    """
    è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦ï¼Œé»˜è®¤ä½¿ç”¨ llama-4-maverick æ¨¡å‹ã€‚
    """
    messages = [{"role": "user", "content": prompt}]
    return await call_openrouter(messages, model="meta-llama/llama-4-maverick")
