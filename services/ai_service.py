"""
AIæœåŠ¡ç»Ÿä¸€è°ƒåº¦æ¨¡å—

é‡æ„åçš„AIæœåŠ¡å…¥å£ï¼Œé€šè¿‡ç»Ÿä¸€æ¥å£è°ƒåº¦å„ä¸ªAIæœåŠ¡æä¾›å•†ã€‚
ä¿æŒä¸åŸæœ‰ä»£ç çš„APIå…¼å®¹æ€§ã€‚
"""

from utils.logging_config import get_logger

logger = get_logger(__name__)
from typing import AsyncGenerator, Optional, Dict, Any

from .ai_providers import OpenRouterProvider, GeminiProvider, OpenAIProvider


class AIService:
    """AIæœåŠ¡ç»Ÿä¸€è°ƒåº¦å™¨"""

    def __init__(self):
        self.openrouter = OpenRouterProvider()
        self.gemini = GeminiProvider()
        self.openai = OpenAIProvider()

        # é»˜è®¤è·¯ç”±é…ç½®
        self._default_routes = {
            "stream": "gemini",  # é»˜è®¤æµå¼å¯¹è¯ä½¿ç”¨Gemini
            "summary": "openrouter",  # æ‘˜è¦ä½¿ç”¨OpenRouter
            "structured": "openai",  # ç»“æ„åŒ–ç”Ÿæˆä½¿ç”¨OpenAI
        }

    def _get_provider(self, provider_name: str):
        """æ ¹æ®åç§°è·å–æä¾›å•†å®ä¾‹"""
        providers = {
            "openrouter": self.openrouter,
            "gemini": self.gemini,
            "openai": self.openai,
        }
        return providers.get(provider_name)

    async def stream_ai_chat(self, messages: list, model: Optional[str] = None) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆAIå›å¤ï¼ŒæŒ‰åˆ†éš”ç¬¦åˆ†æ®µè¾“å‡º - å®Œæ•´æ¢å¤åŸæœ‰åŠŸèƒ½
        åŒ…å«ï¼šæ¨¡å‹è·¯ç”±ã€æ–‡æœ¬æ¸…ç†ã€åˆ†æ®µå¤„ç†ã€å›é€€æœºåˆ¶ã€Barké€šçŸ¥
        """
        import re
        # === DEBUG_CONTEXT_SAVE_START === ä¸´æ—¶è°ƒè¯•ä»£ç ï¼Œç”¨äºä¿å­˜AIä¸Šä¸‹æ–‡
        import os
        import json
        from datetime import datetime
        # === DEBUG_CONTEXT_SAVE_END ===
        from .ai_providers.utils import send_bark_notification
        
        # === DEBUG_CONTEXT_SAVE_START === ä¿å­˜å‘é€ç»™AIçš„å®Œæ•´ä¸Šä¸‹æ–‡åˆ°æœ¬åœ°æ–‡ä»¶ç”¨äºè°ƒè¯•
        # ä¿®æ”¹è¿™é‡Œçš„ True/False æ¥å¯ç”¨/ç¦ç”¨è°ƒè¯•åŠŸèƒ½ï¼Œæ— éœ€é‡å¯æœåŠ¡
        DEBUG_SAVE_CONTEXT = False
        if DEBUG_SAVE_CONTEXT:
            try:
                debug_dir = "/app/debug_output"
                os.makedirs(debug_dir, exist_ok=True)
                
                timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # åŒ…å«æ¯«ç§’
                
                # 1. ä¿å­˜åŸå§‹messages JSON
                messages_json_file = f"{debug_dir}/ai_context_messages_{timestamp_str}.json"
                with open(messages_json_file, 'w', encoding='utf-8') as f:
                    json.dump(messages, f, ensure_ascii=False, indent=2)
                
                # 2. ä¿å­˜äººç±»å¯è¯»æ ¼å¼
                messages_readable_file = f"{debug_dir}/ai_context_readable_{timestamp_str}.txt"
                with open(messages_readable_file, 'w', encoding='utf-8') as f:
                    f.write("=" * 80 + "\n")
                    f.write("AI CONTEXT - å‘é€ç»™AIçš„å®Œæ•´ä¸Šä¸‹æ–‡\n")
                    f.write("=" * 80 + "\n")
                    f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"æ¨¡å‹: {model}\n")
                    f.write(f"æ¶ˆæ¯æ€»æ•°: {len(messages)}\n")
                    f.write("=" * 80 + "\n\n")
                    
                    for i, msg in enumerate(messages):
                        f.write(f"[æ¶ˆæ¯ {i+1}] è§’è‰²: {msg['role']}\n")
                        f.write("-" * 40 + "\n")
                        f.write(msg['content'])
                        f.write("\n" + "=" * 40 + "\n\n")
                
                logger.debug(f"ä¸Šä¸‹æ–‡å·²ä¿å­˜: {messages_json_file}")
                
            except Exception as e:
                logger.warning(f"ä¿å­˜ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        # === DEBUG_CONTEXT_SAVE_END ===

        # æ ¹æ®æ¨¡å‹é€‰æ‹©æä¾›å•†
        if model and "gemini" in model.lower():
            provider = self.gemini  # æœ€å¸¸ç”¨ï¼Œä¼˜å…ˆæ£€æŸ¥
        elif model and "/" in model:
            provider = self.openrouter  # OpenRouteræ¨¡å‹æ ¼å¼ï¼švendor/model
        else:
            provider = self.openai  # å…¶ä»–æƒ…å†µä½¿ç”¨OpenAI

        logger.info(f"ä½¿ç”¨ {provider.get_provider_name()} è¿›è¡Œæµå¼å¯¹è¯")

        def clean_segment(text):
            """æ¸…ç†æ–‡æœ¬ä¸­çš„æ—¶é—´æˆ³å’Œå‘è¨€äººæ ‡è¯†"""
            return re.sub(
                r"^\(è·ç¦»ä¸Šä¸€æ¡æ¶ˆæ¯è¿‡å»äº†ï¼š(\d+[hms]( \d+[hms])?)*\) \[\d{2}:\d{2}:\d{2}\] [^:]+:\s*",
                "",
                text,
            ).strip()

        buffer = ""
        total_processed = 0  # è·Ÿè¸ªå·²å¤„ç†çš„å­—ç¬¦æ•°

        # ç‰¹æ®Šå¤„ç†ï¼šGeminiæ¨¡å‹éœ€è¦å›é€€æœºåˆ¶
        if model and "gemini" in model.lower():
            # ç¬¬ä¸€æ¬¡å°è¯•ï¼šGemini
            gemini_failed = False
            yielded_any = False
            try:
                async for chunk in provider.stream_chat(messages, model):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªåŠ¨å›å¤ï¼ˆè¯´æ˜Geminiå¤±è´¥äº†ï¼‰
                    if chunk.startswith("[è‡ªåŠ¨å›å¤]") or not chunk.strip():
                        gemini_failed = True
                        break
                    if chunk.strip():
                        yielded_any = True
                    buffer += chunk

                    # åº”ç”¨æ–‡æœ¬åˆ†æ®µå¤„ç†é€»è¾‘
                    while True:
                        # ä¼˜å…ˆæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ‡åˆ†
                        indices = []
                        for sep in ["ã€‚", "ï¼Ÿ", "ï¼"]:
                            idx = buffer.find(sep)
                            if idx != -1:
                                indices.append(idx)

                        if indices:
                            earliest_index = min(indices)
                            # å¦‚æœå¥æœ«æ ‡ç‚¹åœ¨æœ«å°¾ï¼Œæš‚ä¸åˆ‡åˆ†ï¼Œç­‰å¾…æ”¶å°¾ç¬¦å·
                            if earliest_index == len(buffer) - 1:
                                break

                            # å°†ç´§éšå…¶åçš„æ”¶å°¾å­—ç¬¦ä¸€å¹¶åŒ…å«
                            closers = set([
                                "â€", "â€™", "ã€‘", "ã€", "ã€", "ï¼‰", "ã€‹", "ã€‰",
                                ")", "]", "'", '"',
                            ])
                            end_index = earliest_index + 1
                            while end_index < len(buffer) and buffer[end_index] in closers:
                                end_index += 1

                            segment = buffer[:end_index].strip()
                            cleaned_segment = clean_segment(segment)
                            if cleaned_segment:
                                logger.debug(f"stream_ai_chat: yield sentence='{cleaned_segment[:50]}'")
                                yield cleaned_segment
                            buffer = buffer[end_index:]
                            total_processed += end_index
                            continue

                        # å†å°è¯•æŒ‰æ¢è¡Œç¬¦åˆ‡åˆ†
                        newline_index = buffer.find("\n")
                        if newline_index != -1:
                            if newline_index == len(buffer) - 1:
                                buffer = buffer[:newline_index]
                                break
                            segment = buffer[:newline_index].strip()
                            cleaned_segment = clean_segment(segment)
                            if cleaned_segment:
                                logger.debug(f"stream_ai_chat: yield line='{cleaned_segment[:50]}'")
                                yield cleaned_segment
                            buffer = buffer[newline_index + 1:]
                            total_processed += newline_index + 1
                            continue

                        break

                # å¤„ç†æœ€ç»ˆå‰©ä½™å†…å®¹
                if buffer.strip():
                    final_segment = clean_segment(buffer)
                    if final_segment:
                        logger.debug(f"stream_ai_chat: yield final='{final_segment[:80]}'")
                        yield final_segment

                # å¦‚æœGeminiå¤±è´¥æˆ–æ— è¾“å‡ºï¼Œç«‹å³å°è¯•OpenAIåè®®
                if gemini_failed or not yielded_any:
                    fallback_message = f"Geminiå¤±è´¥ï¼Œç«‹å³å°è¯•OpenAIåè®®({model})"
                    logger.warning(fallback_message)
                    await send_bark_notification(
                        title="Gemini API å›é€€",
                        content=fallback_message,
                        group="AI_Service_Alerts",
                    )

                    # é‡ç½®bufferï¼Œç¬¬äºŒæ¬¡å°è¯•ï¼šOpenAIåè®®
                    buffer = ""
                    openai_yielded = False
                    try:
                        async for chunk in self.openai.stream_chat(messages, model):
                            if chunk.strip():
                                openai_yielded = True
                            buffer += chunk


                        # OpenAIåˆ†æ®µå¤„ç†é€»è¾‘
                        while True:
                            indices = []
                            for sep in ["ã€‚", "ï¼Ÿ", "ï¼"]:
                                idx = buffer.find(sep)
                                if idx != -1:
                                    indices.append(idx)
                            if indices:
                                earliest_index = min(indices)
                                if earliest_index == len(buffer) - 1:
                                    break
                                closers = set(["â€", "â€™", "ã€‘", "ã€", "ã€", "ï¼‰", "ã€‹", "ã€‰", ")", "]", "'", '"'])
                                end_index = earliest_index + 1
                                while end_index < len(buffer) and buffer[end_index] in closers:
                                    end_index += 1
                                segment = buffer[:end_index].strip()
                                cleaned_segment = clean_segment(segment)
                                if cleaned_segment:
                                    yield cleaned_segment
                                buffer = buffer[end_index:]
                                continue
                            newline_index = buffer.find("\n")
                            if newline_index != -1:
                                if newline_index == len(buffer) - 1:
                                    buffer = buffer[:newline_index]
                                    break
                                segment = buffer[:newline_index].strip()
                                cleaned_segment = clean_segment(segment)
                                if cleaned_segment:
                                    yield cleaned_segment
                                buffer = buffer[newline_index + 1:]
                                continue
                            break

                        # å¤„ç†OpenAIå‰©ä½™å†…å®¹
                        if buffer.strip():
                            final_segment = clean_segment(buffer)
                            if final_segment:
                                yield final_segment

                    except Exception as openai_e:
                        logger.error(f"OpenAIä¹Ÿå¤±è´¥: {openai_e}")
                        openai_yielded = False

                    # å¦‚æœOpenAIä¹Ÿæ²¡æœ‰è¾“å‡ºï¼Œè¿”å›è‡ªåŠ¨å›å¤
                    if not openai_yielded:
                        await send_bark_notification(
                            title="æ‰€æœ‰AIæœåŠ¡å¤±è´¥",
                            content=f"Geminiå’ŒOpenAIéƒ½å¤±è´¥ï¼Œè¿”å›è‡ªåŠ¨å›å¤",
                            group="AI_Service_Alerts",
                        )
                        yield "[è‡ªåŠ¨å›å¤] åœ¨å¿™ï¼Œæœ‰äº‹è¯·ç•™è¨€"
                    return

            except Exception as e:
                # Geminiå¼‚å¸¸ä¹Ÿå°è¯•OpenAI
                logger.error(f"Geminiå¼‚å¸¸: {e}ï¼Œå°è¯•OpenAI")
                gemini_failed = True
        else:
            # å…¶ä»–æä¾›å•†ä¹Ÿåº”ç”¨ç›¸åŒçš„æ–‡æœ¬åˆ†æ®µå¤„ç†é€»è¾‘
            async for chunk in provider.stream_chat(messages, model):
                buffer += chunk

                while True:
                    # ä¼˜å…ˆæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ‡åˆ†
                    indices = []
                    for sep in ["ã€‚", "ï¼Ÿ", "ï¼"]:
                        idx = buffer.find(sep)
                        if idx != -1:
                            indices.append(idx)

                    if indices:
                        earliest_index = min(indices)
                        if earliest_index == len(buffer) - 1:
                            break

                        closers = set(["â€", "â€™", "ã€‘", "ã€", "ã€", "ï¼‰", "ã€‹", "ã€‰", ")", "]", "'", '"'])
                        end_index = earliest_index + 1
                        while end_index < len(buffer) and buffer[end_index] in closers:
                            end_index += 1

                        segment = buffer[:end_index].strip()
                        cleaned_segment = clean_segment(segment)
                        if cleaned_segment:
                            logger.debug(f"stream_ai_chat: yield sentence='{cleaned_segment[:50]}'")
                            yield cleaned_segment
                        buffer = buffer[end_index:]
                        total_processed += end_index
                        continue

                    newline_index = buffer.find("\n")
                    if newline_index != -1:
                        if newline_index == len(buffer) - 1:
                            buffer = buffer[:newline_index]
                            break
                        segment = buffer[:newline_index].strip()
                        cleaned_segment = clean_segment(segment)
                        if cleaned_segment:
                            logger.debug(f"stream_ai_chat: yield line='{cleaned_segment[:50]}'")
                            yield cleaned_segment
                        buffer = buffer[newline_index + 1:]
                        total_processed += newline_index + 1
                        continue

                    break

            # å¤„ç†æœ€ç»ˆå‰©ä½™å†…å®¹
            if buffer.strip():
                final_segment = clean_segment(buffer)
                if final_segment:
                    logger.debug(f"stream_ai_chat: yield final='{final_segment[:80]}'")
                    yield final_segment

    async def call_ai_summary(self, prompt: str) -> str:
        """
        AIæ‘˜è¦è°ƒç”¨æ¥å£
        å…¼å®¹åŸæœ‰çš„call_ai_summaryå‡½æ•°
        """
        messages = [{"role": "user", "content": prompt}]
        model = "mistralai/mistral-7b-instruct:free"
        logger.info(f"å¼€å§‹AIæ‘˜è¦ï¼Œæ¨¡å‹={model}")
        return await self.openrouter.call_chat(messages, model)

    async def call_structured_generation(self, messages: list, max_retries: int = 3) -> dict:
        """
        ç»“æ„åŒ–ç”Ÿæˆæ¥å£
        å…¼å®¹åŸæœ‰çš„call_structured_generationå‡½æ•°
        """
        return await self.openai.call_structured_generation(messages, max_retries)


# åˆ›å»ºå…¨å±€AIæœåŠ¡å®ä¾‹
ai_service = AIService()


# ===== å…¼å®¹æ€§å‡½æ•°ï¼šä¿æŒåŸæœ‰APIä¸å˜ =====

async def stream_ai_chat(messages: list, model: Optional[str] = None) -> AsyncGenerator[str, None]:
    """å…¼å®¹åŸæœ‰æ¥å£çš„æµå¼å¯¹è¯å‡½æ•°"""
    async for chunk in ai_service.stream_ai_chat(messages, model):
        yield chunk


async def stream_reply_ai_by_gemini(
    messages, model="gemini-2.5-pro"
) -> AsyncGenerator[str, None]:
    """å…¼å®¹åŸæœ‰æ¥å£çš„Geminiæµå¼å¯¹è¯å‡½æ•°"""
    async for chunk in ai_service.gemini.stream_chat(messages, model):
        yield chunk


async def stream_openrouter(
    messages, model="z-ai/glm-4.5-air:free"
) -> AsyncGenerator[str, None]:
    """å…¼å®¹åŸæœ‰æ¥å£çš„OpenRouteræµå¼å¯¹è¯å‡½æ•°"""
    async for chunk in ai_service.openrouter.stream_chat(messages, model):
        yield chunk


async def stream_reply_ai(
    messages, model="claude-3-7-sonnet-20250219"
) -> AsyncGenerator[str, None]:
    """å…¼å®¹åŸæœ‰æ¥å£çš„OpenAIæµå¼å¯¹è¯å‡½æ•°"""
    async for chunk in ai_service.openai.stream_chat(messages, model):
        yield chunk


async def call_openrouter(messages, model="mistralai/mistral-7b-instruct:free") -> str:
    """å…¼å®¹åŸæœ‰æ¥å£çš„OpenRouterè°ƒç”¨å‡½æ•°"""
    return await ai_service.openrouter.call_chat(messages, model)


async def call_gemini(messages, model="gemini-2.5-flash") -> str:
    """å…¼å®¹åŸæœ‰æ¥å£çš„Geminiè°ƒç”¨å‡½æ•°"""
    return await ai_service.gemini.call_chat(messages, model)


async def call_openai(messages, model="gpt-4o-mini") -> str:
    """å…¼å®¹åŸæœ‰æ¥å£çš„OpenAIè°ƒç”¨å‡½æ•°"""
    return await ai_service.openai.call_chat(messages, model, use_summary=True)


async def call_ai_summary(prompt: str) -> str:
    """å…¼å®¹åŸæœ‰æ¥å£çš„AIæ‘˜è¦å‡½æ•°"""
    return await ai_service.call_ai_summary(prompt)


async def call_structured_generation(messages: list, max_retries: int = 3) -> dict:
    """å…¼å®¹åŸæœ‰æ¥å£çš„ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°"""
    return await ai_service.call_structured_generation(messages, max_retries)


# ===== å…¶ä»–åŸæœ‰å‡½æ•°ä¿æŒä¸å˜ =====

import os
import json
import hashlib
import random
import httpx
from typing import Optional
import uuid

def get_weather_info(date: str, location: str = "") -> str:
    """
    è·å–æŒ‡å®šæ—¥æœŸå’Œåœ°ç‚¹çš„å¤©æ°”ä¿¡æ¯ï¼ˆæ¥å…¥å’Œé£å¤©æ°”APIï¼Œå¤±è´¥æ—¶é€€å›ä¼ªéšæœºç”Ÿæˆï¼‰

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        location: ä½ç½®ï¼ˆä»…ç”¨äºç§å­ï¼‰

    Returns:
        str: ç»¼åˆå¤©æ°”æè¿°
    """
    # é»˜è®¤locationåˆ—è¡¨
    default_locations = [
        "101320101", "101320103", "14606", "1B6D3", "1D255", "1DC87", "275A5",
        "28FE1", "2BBD1", "2BC09", "39CD9", "407DA", "4622E", "55E7E",
        "8A9CA", "8E1C5", "9173", "D5EC3", "DD9B5", "E87DC",
    ]
    if not location:
        location = random.choice(default_locations)
        logger.debug(f"ai.weather ä½¿ç”¨éšæœºä½ç½®ID: {location} æŸ¥è¯¢ {date} å¤©æ°”")

    try:
        logger.info(f"ai.weather å¼€å§‹è·å–å¤©æ°” date={date} location={location}")
        url = (
            "https://"
            + os.getenv("HEFENG_API_HOST", "have_no_api_host")
            + "/v7/weather/7d"
        )
        params = {
            "location": location,
            "key": os.getenv("HEFENG_API_KEY"),
            "lang": "zh",
        }
        logger.debug(f"ai.weather è¯·æ±‚å‚æ•°: {params}")

        response = httpx.get(url, params=params, timeout=10)
        response.raise_for_status()

        data = response.json()
        logger.debug(f"ai.weather å“åº”: {data}")

        if data.get("code") != "200":
            error_msg = f"APIé”™è¯¯ä»£ç : {data.get('code')}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        for day in data.get("daily", []):
            if day.get("fxDate") == date:
                result = (
                    f"ç™½å¤©{day.get('textDay')}ï¼Œå¤œæ™š{day.get('textNight')}ã€‚"
                    f"æ°”æ¸©{day.get('tempMin')}Â°C~{day.get('tempMax')}Â°Cï¼Œ"
                    f"ç™½å¤©é£ï¼š{day.get('windDirDay')} {day.get('windScaleDay')}çº§ï¼Œ"
                    f"å¤œæ™šé£ï¼š{day.get('windDirNight')} {day.get('windScaleNight')}çº§ï¼Œ"
                    f"æ¹¿åº¦ï¼š{day.get('humidity')}%ï¼Œ"
                    f"é™æ°´ï¼š{day.get('precip')}mmï¼Œ"
                    f"ç´«å¤–çº¿æŒ‡æ•°ï¼š{day.get('uvIndex')}ï¼Œ"
                    f"æœˆç›¸ï¼š{day.get('moonPhase')}ï¼Œ"
                    f"æ—¥å‡ºï¼š{day.get('sunrise')}ï¼Œæ—¥è½ï¼š{day.get('sunset')}ï¼Œ"
                    f"æœˆå‡ï¼š{day.get('moonrise')}ï¼Œæœˆè½ï¼š{day.get('moonset')}ã€‚"
                )
                logger.info(f"ai.weather æˆåŠŸè·å– {date} å¤©æ°”")
                return result

        logger.warning(f"æœªæ‰¾åˆ° {date} çš„å¤©æ°”æ•°æ®ï¼Œä½¿ç”¨æœ€åä¸€å¤©æ•°æ®æ›¿ä»£")
        day = data["daily"][-1]
        result = (
            f"ç™½å¤©{day.get('textDay')}ï¼Œå¤œæ™š{day.get('textNight')}ã€‚"
            f"æ°”æ¸©{day.get('tempMin')}Â°C~{day.get('tempMax')}Â°Cï¼Œ"
            f"ç™½å¤©é£ï¼š{day.get('windDirDay')} {day.get('windScaleDay')}çº§ï¼Œ"
            f"å¤œæ™šé£ï¼š{day.get('windDirNight')} {day.get('windScaleNight')}çº§ï¼Œ"
            f"æ¹¿åº¦ï¼š{day.get('humidity')}%ï¼Œ"
            f"é™æ°´ï¼š{day.get('precip')}mmï¼Œ"
            f"ç´«å¤–çº¿æŒ‡æ•°ï¼š{day.get('uvIndex')}ï¼Œ"
            f"æœˆç›¸ï¼š{day.get('moonPhase')}ï¼Œ"
            f"æ—¥å‡ºï¼š{day.get('sunrise')}ï¼Œæ—¥è½ï¼š{day.get('sunset')}ï¼Œ"
            f"æœˆå‡ï¼š{day.get('moonrise')}ï¼Œæœˆè½ï¼š{day.get('moonset')}ã€‚"
        )
        logger.debug(f"ai.weather ä½¿ç”¨æœ€åä¸€å¤©æ•°æ®ä½œä¸º {date} å¤©æ°”: {result[:50]}...")
        return result
    except httpx.HTTPError as e:
        logger.error(f"HTTPè¯·æ±‚å¤±è´¥: {e}")
    except httpx.Timeout:
        logger.error("å¤©æ°”APIè¯·æ±‚è¶…æ—¶")
    except ValueError as e:
        logger.error(f"APIè¿”å›æ•°æ®é”™è¯¯: {e}")
    except Exception as e:
        logger.error(f"è·å–å¤©æ°”å¼‚å¸¸: {str(e)}", exc_info=True)

    # å›é€€ï¼šä½¿ç”¨ä¼ªéšæœºå¤©æ°”
    seed = int(hashlib.md5(f"{date}-{location}".encode()).hexdigest()[:8], 16)
    random.seed(seed)
    logger.warning(f"å›é€€åˆ°ä¼ªéšæœºå¤©æ°” (ç§å­: {seed})")

    weather_options = ["æ™´å¤©", "é˜´å¤©", "é›¨å¤©", "é›ªå¤©", "é›¾å¤©"]
    weather_weights = [0.4, 0.25, 0.2, 0.05, 0.1]

    result = random.choices(weather_options, weights=weather_weights)[0]
    logger.debug(f"ai.weather ç”Ÿæˆä¼ªéšæœºå¤©æ°”: {result}")
    return result


async def generate_daily_schedule(
    date: str,
    day_type: str,
    weather: str,
    is_in_major_event: bool,
    major_event_context: Optional[dict] = None,
    special_flags: Optional[list] = None,
) -> dict:
    """åŠŸèƒ½ï¼šç”Ÿæˆä¸»æ—¥ç¨‹"""
    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”ŸæˆçœŸå®ã€è¿è´¯çš„æ—¥å¸¸ç”Ÿæ´»å®‰æ’ã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åä¿¡ä½¿ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å½“å‰æƒ…å†µ
- æ—¥æœŸ: {date}
- æ—¥æœŸç±»å‹: {day_type} ({"å·¥ä½œæ—¥" if day_type == "weekday" else "å‘¨æœ«"})
- å¤©æ°”çŠ¶å†µ: {weather}
- æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­: {"æ˜¯" if is_in_major_event else "å¦"}"""

    if is_in_major_event and major_event_context:
        prompt += f"\n- å¤§äº‹ä»¶èƒŒæ™¯: {json.dumps(major_event_context, ensure_ascii=False)}"
    if special_flags:
        prompt += f"\n- ç‰¹æ®Šæƒ…å†µ: {', '.join(special_flags)}"

    prompt += f"""

## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œå½“å‰æƒ…å†µï¼Œç”Ÿæˆä¸€ä»½ç¬¦åˆé€»è¾‘çš„æ—¥ç¨‹å®‰æ’ã€‚æ³¨æ„ï¼š
1. **å¿…é¡»æ˜ç¡®èµ·åºŠå’Œç¡è§‰æ—¶é—´**ï¼šæ—¥ç¨‹çš„ç¬¬ä¸€é¡¹åº”è¯¥æ˜¯èµ·åºŠï¼ˆä¾‹å¦‚ 06:30-07:00ï¼‰ï¼Œæœ€åä¸€é¡¹åº”è¯¥æ˜¯ç¡è§‰ï¼ˆä¾‹å¦‚ 23:00-23:59ï¼‰
2. å·¥ä½œæ—¥é€šå¸¸åŒ…å«å¿«é€’é…é€ä»»åŠ¡ï¼Œå‘¨æœ«å¯èƒ½æœ‰åŠ ç­æˆ–ä¼‘é—²æ´»åŠ¨
3. å¤©æ°”ä¼šå½±å“æˆ·å¤–æ´»åŠ¨å’Œé…é€éš¾åº¦
4. ä¸åŒäº‹çš„äº’åŠ¨è¦ç¬¦åˆè§’è‰²å…³ç³»
5. æ—¶é—´å®‰æ’è¦åˆç†ï¼Œæ´»åŠ¨ä¹‹é—´è¦æœ‰é€»è¾‘è¿æ¥
6. èµ·åºŠæ—¶é—´ä¸€èˆ¬åœ¨ 06:00-08:00 ä¹‹é—´ï¼Œç¡è§‰æ—¶é—´ä¸€èˆ¬åœ¨ 22:00-23:59 ä¹‹é—´ï¼Œæ ¹æ®å½“å¤©çš„æ´»åŠ¨å®‰æ’é€‚å½“è°ƒæ•´

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "date": "{date}",
  "day_type": "{day_type}",
  "weather": "{weather}",
  "is_overtime": false,
  "daily_summary": "ç®€è¦æè¿°è¿™ä¸€å¤©çš„æ•´ä½“å®‰æ’å’Œä¸»è¦æ´»åŠ¨",
  "schedule_items": [
    {{
      "start_time": "HH:MM",
      "end_time": "HH:MMï¼ˆå¦‚æœåˆ°æ¬¡æ—¥ï¼Œåˆ™å†™23:59ã€‚æœ€å¤šä¸å¾—è¶…è¿‡23:59ï¼‰",
      "duration_minutes": æ•°å­—,
      "title": "æ´»åŠ¨æ ‡é¢˜",
      "category": "personal|work|social|rest",
      "priority": "high|medium|low",
      "location": "å…·ä½“åœ°ç‚¹",
      "description": "è¯¦ç»†çš„æ´»åŠ¨æè¿°",
      "weather_affected": trueæˆ–false,
      "companions": ["å‚ä¸çš„å…¶ä»–è§’è‰²"],
      "emotional_impact_tags": ["ç›¸å…³æƒ…ç»ªæ ‡ç­¾"],
      "interaction_potential": "low|medium|high",
      "metadata": {
        "stamina_cost": æ•°å­—(0-30),  // é¢„ä¼°ä½“åŠ›æ¶ˆè€—ï¼Œå¸¸è§„å·¥ä½œ5-10ï¼Œé«˜å¼ºåº¦15-25ï¼Œä¼‘æ¯ä¸ºè´Ÿå€¼(æ¢å¤)
        "stress_impact": æ•°å­—(0-20), // å‹åŠ›/åˆºæ¿€ç¨‹åº¦
        "mood_modifier": {"P": 0, "A": 0, "D": 0} // åŸºç¡€æƒ…ç»ªä¿®æ­£å€¾å‘
      }
    }}
  ]
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨éæµå¼è°ƒç”¨ï¼ŒæŒ‡å®šClaudeæ¨¡å‹
    try:
        # ä½¿ç”¨ä¸“ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
        response = await call_structured_generation(messages)
        if "error" in response:
            return response  # ç›´æ¥è¿”å›é”™è¯¯

        result = response  # å·²ç»æ˜¯è§£æå¥½çš„å­—å…¸

        # ä¸ºæ¯ä¸ªschedule_itemæ·»åŠ UUID
        for item in result.get("schedule_items", []):
            item["id"] = str(uuid.uuid4())

        return result
    except json.JSONDecodeError:
        logger.error(f"generate_daily_schedule: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response}")
        return {"error": "AIè¿”å›æ ¼å¼é”™è¯¯", "raw_response": response}
    except Exception as e:
        logger.error(f"generate_daily_schedule: è°ƒç”¨å¤±è´¥: {e}")
        return {"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}


async def generate_major_event(
    duration_days: int,
    event_type: str,
    start_date: str,
    weather_forecast: Optional[dict] = None,
) -> dict:
    """åŠŸèƒ½ï¼šç”Ÿæˆå¤§äº‹ä»¶"""
    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”Ÿæˆé‡è¦çš„ç”Ÿæ´»äº‹ä»¶ã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åä¿¡ä½¿ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å¤§äº‹ä»¶å®šä¹‰
å¤§äº‹ä»¶æ˜¯æŒ‡æŒç»­å¤šå¤©ã€å¯¹å¾·å…‹è¨æ–¯ç”Ÿæ´»äº§ç”Ÿé‡è¦å½±å“çš„äº‹ä»¶ï¼Œå¦‚ï¼š
- é‡è¦çš„é…é€ä»»åŠ¡ï¼ˆè·¨åŸå¸‚ã€é«˜ä»·å€¼è´§ç‰©ï¼‰
- ä¼é¹…ç‰©æµçš„å›¢é˜Ÿæ´»åŠ¨æˆ–åŸ¹è®­
- ä¸ªäººé‡è¦äº‹åŠ¡ï¼ˆæ¬å®¶ã€ä¼‘å‡ã€åŒ»ç–—ç­‰ï¼‰
- é¾™é—¨åŸå¸‚äº‹ä»¶ï¼ˆèŠ‚æ—¥ã€ç´§æ€¥çŠ¶å†µç­‰ï¼‰

## å½“å‰å¤§äº‹ä»¶å‚æ•°
- äº‹ä»¶ç±»å‹: {event_type}
- å¼€å§‹æ—¥æœŸ: {start_date}
- æŒç»­å¤©æ•°: {duration_days}å¤©"""

    if weather_forecast:
        prompt += f"\n- æœŸé—´å¤©æ°”é¢„æŠ¥: {json.dumps(weather_forecast, ensure_ascii=False)}"

    prompt += f"""

## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œäº‹ä»¶å‚æ•°ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å¤§äº‹ä»¶è®¡åˆ’ã€‚æ³¨æ„ï¼š
1. äº‹ä»¶å†…å®¹è¦ç¬¦åˆå¾·å…‹è¨æ–¯çš„èŒä¸šå’Œæ€§æ ¼ç‰¹ç‚¹
2. æ¯æ—¥è®¡åˆ’è¦æœ‰é€»è¾‘è¿è´¯æ€§å’Œæ¸è¿›æ€§
3. è€ƒè™‘å¤©æ°”å¯¹äº‹ä»¶æ‰§è¡Œçš„å½±å“
4. åŒ…å«åˆç†çš„æŒ‘æˆ˜å’Œé£é™©å› ç´ 

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "event_title": "äº‹ä»¶çš„ç®€æ´æ ‡é¢˜",
  "event_type": "{event_type}",
  "main_objective": "è¿™ä¸ªå¤§äº‹ä»¶çš„ä¸»è¦ç›®æ ‡å’Œæ„ä¹‰",
  "total_days": {duration_days},
  "daily_plans": [
    {{
      "day": 1,
      "date": "YYYY-MM-DD",
      "phase": "äº‹ä»¶çš„å½“å‰é˜¶æ®µï¼ˆå¦‚ï¼šå‡†å¤‡é˜¶æ®µã€æ‰§è¡Œé˜¶æ®µã€æ”¶å°¾é˜¶æ®µï¼‰",
      "summary": "å½“æ—¥çš„ä¸»è¦å®‰æ’å’Œç›®æ ‡",
      "key_activities": ["å…·ä½“æ´»åŠ¨1", "å…·ä½“æ´»åŠ¨2"],
      "expected_challenges": ["å¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜"],
      "emotional_state": "å¾·å…‹è¨æ–¯åœ¨è¿™ä¸€å¤©çš„æƒ…ç»ªçŠ¶æ€",
      "location_start": "ä¸€å¤©å¼€å§‹çš„åœ°ç‚¹",
      "location_end": "ä¸€å¤©ç»“æŸçš„åœ°ç‚¹"
    }}
  ],
  "success_criteria": ["åˆ¤æ–­äº‹ä»¶æˆåŠŸçš„æ ‡å‡†"],
  "risk_factors": ["å¯èƒ½å½±å“äº‹ä»¶çš„é£é™©å› ç´ "]
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
    try:
        response = await call_structured_generation(messages)
        if "error" in response:
            return response  # ç›´æ¥è¿”å›é”™è¯¯

        result = response  # å·²ç»æ˜¯è§£æå¥½çš„å­—å…¸

        # æ·»åŠ UUID
        result["event_id"] = str(uuid.uuid4())

        return result
    except json.JSONDecodeError:
        logger.error(f"generate_major_event: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response}")
        return {"error": "AIè¿”å›æ ¼å¼é”™è¯¯", "raw_response": response}
    except Exception as e:
        logger.error(f"generate_major_event: è°ƒç”¨å¤±è´¥: {e}")
        return {"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}


async def generate_micro_experiences(
    schedule_item: dict,
    current_date: str,
    previous_experiences: Optional[list] = None,
    major_event_context: Optional[dict] = None,
) -> list:
    """åŠŸèƒ½ï¼šä¸ºå•ä¸ªæ—¥ç¨‹é¡¹ç›®ç”Ÿæˆå¤šä¸ªå¾®è§‚ç»å†é¡¹ï¼ˆ5-30åˆ†é’Ÿé¢—ç²’åº¦ï¼‰"""
    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„å¾®è§‚ç»å†ç”Ÿæˆæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”ŸæˆçœŸå®ã€ç»†è…»çš„ç”Ÿæ´»ç‰‡æ®µã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åå‘˜å·¥ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å½“å‰æƒ…å†µ
- å½“å‰æ—¥æœŸ: {current_date}
- æ—¥ç¨‹é¡¹ç›®: {schedule_item.get("title", "æœªçŸ¥æ´»åŠ¨")}
- é¡¹ç›®å¼€å§‹æ—¶é—´: {schedule_item.get("start_time", "æœªçŸ¥")}
- é¡¹ç›®ç»“æŸæ—¶é—´: {schedule_item.get("end_time", "æœªçŸ¥")}
- æ´»åŠ¨åœ°ç‚¹: {schedule_item.get("location", "æœªçŸ¥åœ°ç‚¹")}
- æ´»åŠ¨æè¿°: {schedule_item.get("description", "æ— æè¿°")}
- åŒä¼´: {", ".join(schedule_item.get("companions", [])) if schedule_item.get("companions") else "ç‹¬è‡ªä¸€äºº"}"""

    # âœ… æ”¹è¿›ï¼šå¤„ç†å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    if previous_experiences:
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ ¼å¼ï¼ˆå­—å…¸åŒ…å«previous_experiencesåˆ—è¡¨ï¼‰
        if isinstance(previous_experiences, dict) and "previous_experiences" in previous_experiences:
            prev_exps = previous_experiences.get("previous_experiences", [])

            # æ„å»ºè¯¦ç»†çš„ä¸Šä¸‹æ–‡æç¤º
            if prev_exps:
                prompt += f"\n\n## ä»Šå¤©å·²ç»å‘ç”Ÿçš„ç»å†\n"
                # æ˜¾ç¤ºæœ€è¿‘çš„8æ¡ç»å†ï¼Œæä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
                recent_exps = prev_exps[-8:] if len(prev_exps) > 8 else prev_exps
                for exp in recent_exps:
                    prompt += f"- [{exp.get('time', '')}] {exp.get('content', '')}\n"

                if len(prev_exps) > 8:
                    prompt += f"ï¼ˆå…±{len(prev_exps)}æ¡ç»å†ï¼Œæ­¤å¤„æ˜¾ç¤ºæœ€è¿‘{len(recent_exps)}æ¡ï¼‰\n"
        else:
            # å…¼å®¹æ—§æ ¼å¼
            prompt += f"\n- ä¹‹å‰çš„ç»å†æ‘˜è¦: {json.dumps(previous_experiences, ensure_ascii=False)}"

    if major_event_context:
        prompt += f"\n- å¤§äº‹ä»¶èƒŒæ™¯: {json.dumps(major_event_context, ensure_ascii=False)}"

    prompt += f"""

## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œå½“å‰æƒ…å†µï¼Œå°†æ—¥ç¨‹é¡¹ç›®æ‹†è§£æˆå¤šä¸ª5-30åˆ†é’Ÿé¢—ç²’åº¦çš„å¾®è§‚ç»å†é¡¹ã€‚æ³¨æ„ï¼š
1. æ¯ä¸ªç»å†é¡¹åº”åŒ…å«å…·ä½“çš„æ—¶é—´æ®µï¼ˆå¼€å§‹å’Œç»“æŸæ—¶é—´ï¼‰å¹¶ä¸”æ‰€æœ‰å¾®è§‚ç»å†è¿ç»­èµ·æ¥æ•´ä½“ä¸Šè¦ä»å¤´åˆ°åˆ°å°¾è¦†ç›–æ•´ä¸ªæ—¥ç¨‹é¡¹ç›®
2. å†…å®¹è¦ç¬¦åˆå¾·å…‹è¨æ–¯çš„æ€§æ ¼ç‰¹ç‚¹ï¼ˆå†·é™ã€ä¸“ä¸šã€å†…æ•›ï¼‰
3. æƒ…ç»ªè¡¨è¾¾è¦ç»†è…»ä½†ä¸å¤¸å¼ 
4. æ€è€ƒè¦ç¬¦åˆå¥¹çš„èŒä¸šèƒŒæ™¯å’Œç»å†
5. å¦‚æœéœ€è¦äº¤äº’ï¼Œè¦ç¬¦åˆè§’è‰²å…³ç³»å’Œæƒ…å¢ƒ
6. **ğŸ”´ æ—¶é—´è¿è´¯æ€§çº¦æŸ**ï¼šä»”ç»†æŸ¥çœ‹"ä»Šå¤©å·²ç»å‘ç”Ÿçš„ç»å†"ï¼Œç¡®ä¿æƒ…èŠ‚è‡ªç„¶å»¶ç»­ï¼Œ**é¿å…é‡å¤å·²ç»å®Œæˆè¿‡çš„åŠ¨ä½œå’Œäº’åŠ¨**

## ä¸»åŠ¨äº¤äº’é¡»çŸ¥
è¿™æ˜¯ä¸€ä¸ª AI è§’è‰²æ‰®æ¼”çš„ä¸€éƒ¨åˆ†ã€‚è¿™é‡Œæ˜¯æ¨¡æ‹Ÿè§’è‰²çš„æ—¥å¸¸ç”Ÿæ´»ã€‚æ‰€è°“ä¸»åŠ¨äº¤äº’ï¼Œæ˜¯æŒ‡è§’è‰²ï¼ˆå¾·å…‹è¨æ–¯ï¼ˆæ˜æ—¥æ–¹èˆŸçš„è§’è‰²ï¼‰æ˜¯å¦è¦ä¸ç”¨æˆ·è¿›è¡Œäº’åŠ¨ã€‚
å¦‚æœå¾·å…‹è¨æ–¯è®¤ä¸ºè¿™ä»¶äº‹å€¼å¾—åˆ†äº«ç»™ç”¨æˆ·ï¼Œåˆ™è®¾ç½®ä¸ºtrueï¼Œäº¤äº’å†…å®¹æ˜¯å¾·å…‹è¨æ–¯å¯¹è¿™ä»¶äº‹æƒ³è¦å’Œç”¨æˆ·åˆ†äº«çš„ç»å†å’Œæ„Ÿå—ã€‚
ç”¨æˆ·æ˜¯å’Œå¥¹åªèƒ½é€šè¿‡ç½‘ç»œè¿›è¡Œäº¤æµï¼Œä½†æ˜¯æ˜¯å…³ç³»æœ€å¥½çš„æœ‹å‹ã€‚

**ç‰¹åˆ«æ³¨æ„**ï¼š
- **åªæœ‰å½“æ—¥ç¨‹æ ‡é¢˜åŒ…å«"èµ·åºŠ"æˆ–"ç¡è§‰"æ—¶**ï¼Œæ‰éœ€è¦åœ¨æŸä¸ªåˆé€‚çš„itemä¸­åŒ…å«æ—©å®‰æˆ–æ™šå®‰çš„é—®å€™
- å¦‚æœæ˜¯èµ·åºŠç›¸å…³æ—¥ç¨‹ï¼Œåœ¨ç¬¬ä¸€ä¸ªitemä¸­è®¾ç½®need_interactionä¸ºtrueï¼Œäº¤äº’å†…å®¹åŒ…å«é“æ—©å®‰
- å¦‚æœæ˜¯ç¡è§‰ç›¸å…³æ—¥ç¨‹ï¼Œåœ¨æœ€åä¸€ä¸ªitemä¸­è®¾ç½®need_interactionä¸ºtrueï¼Œäº¤äº’å†…å®¹åŒ…å«é“æ™šå®‰
- å…¶ä»–æ—¥ç¨‹é¡¹ä¸éœ€è¦æ—©å®‰/æ™šå®‰é—®å€™

ä¸»åŠ¨äº¤äº’ä¸ºtrueå¤§æ¦‚è¦å æ®40%å·¦å³ï¼Œä¸è¦è¿‡ä½ï¼Œè‡³å°‘éœ€è¦æœ‰ä¸€ä¸ªï¼Œä½†ä¸è¦è¶…è¿‡ä¸€åŠã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "date": "{current_date}",
  "schedule_item_id": "{schedule_item.get("id", "")}",
  "items": [
    {{
      "start_time": "HH:MM",
      "end_time": "HH:MM",
      "content": "è¯¦ç»†æè¿°è¿™æ®µç»å†",
      "emotions": "æƒ…ç»ªçŠ¶æ€",
      "thoughts": "å†…å¿ƒçš„æƒ³æ³•",
      "need_interaction": trueæˆ–false,
      "interaction_content": "äº¤äº’å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰",
      "stats_modifier": {
        "stamina_delta": 0, // é¢å¤–çš„ä½“åŠ›å˜åŒ– (è´Ÿæ•°ä¸ºæ¶ˆè€—)
        "mood_delta": {"P": 0, "A": 0, "D": 0}, // æƒ…ç»ªæ³¢åŠ¨
        "lust_delta": 0 // æ¬²æœ›æ³¢åŠ¨ (ä»…åœ¨ç‰¹å®šäº²å¯†æˆ–åˆºæ¿€åœºæ™¯ä¸‹)
      }
    }},
    // æ›´å¤šç»å†é¡¹...
  ],
  "created_at": "è‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€å¡«å†™"
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
    try:
        response = await call_structured_generation(messages)
        if "error" in response:
            return [response]  # è¿”å›é”™è¯¯åˆ—è¡¨

        # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
        if "items" not in response or not isinstance(response["items"], list):
            return [{"error": "AIè¿”å›æ ¼å¼é”™è¯¯: ç¼ºå°‘itemsåˆ—è¡¨", "raw_response": response}]

        # ä¸ºæ¯ä¸ªç»å†é¡¹æ·»åŠ å”¯ä¸€ID
        for item in response["items"]:
            item["id"] = str(uuid.uuid4())
            item["schedule_item_id"] = schedule_item.get("id", "")

        return response["items"]
    except json.JSONDecodeError:
        logger.error("generate_micro_experiences: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON")
        return [{"error": "AIè¿”å›æ ¼å¼é”™è¯¯"}]
    except Exception as e:
        logger.error(f"generate_micro_experiences: è°ƒç”¨å¤±è´¥: {e}")
        return [{"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}]


async def summarize_past_micro_experiences(experiences: list) -> str:
    """åŠŸèƒ½ï¼šå°†è¿‡å»çš„å¾®è§‚ç»å†æ•´ç†æˆæ•…äº‹åŒ–çš„æ–‡æœ¬"""
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯ï¼ˆæ˜æ—¥æ–¹èˆŸè§’è‰²ï¼‰ã€‚
ç°åœ¨è¯·ä½ ä»¥ç¬¬ä¸€äººç§°å›é¡¾åˆšåˆšç»å†çš„å¾®è§‚äº‹ä»¶ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä»½å®Œæ•´ã€çœŸå®ã€æœ‰æ¡ç†çš„è‡ªæˆ‘è®°å½•ã€‚

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
	1.	æŒ‰ç…§æ—¶é—´é¡ºåºï¼Œé€æ¡ç”¨è‡ªç„¶è¯­è¨€æµç•…åœ°é™ˆè¿°æ¯ä¸€æ®µç»å†çš„å‘ç”Ÿå†…å®¹ã€æ‰€è§æ‰€é—»ã€å†…å¿ƒæƒ³æ³•ã€æƒ…ç»ªå˜åŒ–ï¼›
	2.	ä¸å¾—é—æ¼ä»»ä½•ç»å†é¡¹ï¼Œæ¯æ®µç»å†éƒ½è¦è¦†ç›–åŸºæœ¬è¦ç´ ï¼ˆåšäº†ä»€ä¹ˆã€æƒ³äº†ä»€ä¹ˆã€å½“æ—¶çš„æƒ…ç»ªï¼‰ï¼›
	3.	ä¸è¿›è¡Œæ–‡å­¦åŒ–åŠ å·¥ï¼Œä¹Ÿä¸ç¼–é€ æœªåœ¨ç»å†ä¸­å‡ºç°çš„å†…å®¹ï¼›
	4.	å¦‚æœæŸäº›ç»å†ä¹‹é—´å­˜åœ¨å‰åå…³è”ï¼Œå¯ä»¥æŒ‡å‡ºï¼Œè®©è¡”æ¥æµç•…ã€‚

ä½ æ­£åœ¨ç”Ÿæˆçš„æ–‡æœ¬ç›®çš„åœ¨äºå®Œæ•´è®°å½•å½“å¤©ç”Ÿæ´»ç»†èŠ‚ã€‚
æ³¨æ„è¯­è¨€è¦è¿è´¯è‡ªç„¶ï¼Œè®©å…¶ä»–äººé˜…è¯»çš„æ—¶å€™ï¼Œèƒ½ç†è§£ä½ çš„æƒ³æ³•ï¼Œäº†è§£ä½ ä»Šå¤©ä¸ºæ­¢çš„å…¨éƒ¨ç»å†ã€‚
æ³¨æ„è¯¦ç•¥å¾—å½“ï¼ŒæŠŠä½ è®¤ä¸ºå°è±¡æ·±åˆ»çš„å†…å®¹è¯¦ç»†åœ°è®°å½•ä¸‹æ¥ã€‚å…¶ä»–çš„å¯ä»¥ç®€è¦ä¸€äº›ã€‚
æœ‰ç‚¹ç±»ä¼¼äºæ—¥è®°ï¼Œæˆ–è€…æ˜¯ä½ ç»å†è¿™äº›äº‹æƒ…åçš„å›å¿†è¿‡ç¨‹ã€‚

ä»¥ä¸‹æ˜¯ä½ ä»Šå¤©çš„å¾®è§‚ç»å†æ•°æ®ï¼š
{json.dumps(experiences, ensure_ascii=False, indent=2)}

è¯·å¼€å§‹è®°å½•ï¼š
"""

    messages = [{"role": "user", "content": prompt}]

    try:
        # ä½¿ç”¨éæµå¼è°ƒç”¨ï¼Œè·å–æ•…äº‹åŒ–æ–‡æœ¬
        response = await call_openrouter(messages, model="z-ai/glm-4.5-air:free")
        # response = await call_openai(messages, model="gpt-4o-mini")
        return response
    except Exception as e:
        logger.error(f"summarize_past_micro_experiences: è°ƒç”¨å¤±è´¥: {e}")
        return f"æ•…äº‹ç”Ÿæˆå¤±è´¥: {str(e)}"


async def ai_decide_images_globally(
    candidates: list,
    target_count_range: tuple = (5, 15)
) -> list:
    """
    AIå…¨å±€è¯„ä¼°å“ªäº›å¾®è§‚ç»å†éœ€è¦ç”Ÿæˆå›¾ç‰‡

    Args:
        candidates: å€™é€‰çš„å¾®è§‚ç»å†åˆ—è¡¨ï¼ˆneed_interaction=trueï¼‰
                   æ¯ä¸ªå…ƒç´ æ ¼å¼ï¼š{
                       "id": "å¾®è§‚ç»å†ID",
                       "time": "07:00-07:05",
                       "content": "å†…å®¹",
                       "emotions": "æƒ…ç»ª",
                       "interaction_content": "äº¤äº’å†…å®¹"
                   }
        target_count_range: ç›®æ ‡æ•°é‡èŒƒå›´ (min, max)ï¼Œé»˜è®¤5-15å¼ 

    Returns:
        [
            {
                "micro_experience_id": "uuid",
                "need_image": true,
                "image_type": "selfie" | "scene",
                "image_reason": "åŸå› "
            },
            ...
        ]
    """
    min_count, max_count = target_count_range

    logger.info(f"[AIå›¾ç‰‡å†³ç­–] å¼€å§‹è¯„ä¼° {len(candidates)} ä¸ªå€™é€‰å¾®è§‚ç»å†ï¼Œç›®æ ‡é€‰æ‹© {min_count}-{max_count} å¼ ")

    if not candidates:
        logger.warning("[AIå›¾ç‰‡å†³ç­–] æ²¡æœ‰å€™é€‰é¡¹ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []

    # å¦‚æœå€™é€‰é¡¹å°‘äºæœ€å°æ•°é‡ï¼Œè°ƒæ•´ç›®æ ‡èŒƒå›´
    if len(candidates) < min_count:
        logger.warning(f"[AIå›¾ç‰‡å†³ç­–] å€™é€‰é¡¹({len(candidates)})å°‘äºæœ€å°æ•°é‡({min_count})ï¼Œè°ƒæ•´èŒƒå›´")
        min_count = max(1, len(candidates))
        max_count = len(candidates)

    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„å›¾ç‰‡ç”Ÿæˆå†³ç­–æ¨¡å—ã€‚

## ä»»åŠ¡
ä»ä»Šå¤©æ‰€æœ‰å€¼å¾—åˆ†äº«çš„å¾®è§‚ç»å†ä¸­ï¼ŒæŒ‘é€‰ {min_count}-{max_count} ä¸ªæœ€å€¼å¾—ç”Ÿæˆå›¾ç‰‡çš„æ—¶åˆ»ã€‚

## å€™é€‰å¾®è§‚ç»å†
å…± {len(candidates)} ä¸ªå€™é€‰é¡¹ï¼ˆå‡ä¸º need_interaction=trueï¼Œå³å€¼å¾—ä¸ç”¨æˆ·åˆ†äº«çš„æ—¶åˆ»ï¼‰ï¼š

"""

    for i, cand in enumerate(candidates, 1):
        prompt += f"""
{i}. ID: {cand['id']}
   æ—¶é—´: {cand['time']}
   å†…å®¹: {cand['content']}
   æƒ…ç»ª: {cand['emotions']}
   äº¤äº’å†…å®¹: {cand.get('interaction_content', 'æ— ')}
---
"""

    prompt += f"""

## å›¾ç‰‡ç”Ÿæˆå†³ç­–æ ‡å‡†

**âš ï¸ æ•°é‡æ§åˆ¶**ï¼š
- å¿…é¡»æŒ‘é€‰ {min_count}-{max_count} ä¸ªæ—¶åˆ»
- **ğŸ’° æˆæœ¬æ§åˆ¶**ï¼šæ¯å¼ å›¾ç‰‡çº¦0.12å…ƒï¼Œç›®æ ‡æ€»æˆæœ¬ {min_count*0.12:.2f}-{max_count*0.12:.2f} å…ƒ/å¤©
- **ä¼˜å…ˆçº§åŸåˆ™**ï¼šåªé€‰æ‹©æœ€å€¼å¾—è®°å½•çš„æ—¶åˆ»ï¼Œæ™®é€šå¹³æ·¡çš„æ—¶åˆ»ä¸ç”Ÿæˆå›¾ç‰‡

**æ˜¯å¦ç”Ÿæˆå›¾ç‰‡çš„åˆ¤æ–­æ ‡å‡†**ï¼š
ä¼˜å…ˆè€ƒè™‘ä»¥ä¸‹æƒ…å†µï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š
1. **èµ·åºŠæˆ–ç¡è§‰ç›¸å…³çš„ç»å†**ï¼ˆé€šå¸¸è®¾ç½®ä¸ºtrueï¼Œè‡ªæ‹ç±»å‹ï¼‰- æœ€ä¼˜å…ˆ
2. é‡è¦çš„æ—¶åˆ»æˆ–äº‹ä»¶ï¼ˆå®Œæˆé‡è¦ä»»åŠ¡ã€ç‰¹æ®Šåº†ç¥ã€éš¾å¿˜ç¬é—´ç­‰ï¼‰
3. ä¸æœ‹å‹çš„æ¸©é¦¨æ—¶åˆ»ï¼ˆä¸€èµ·ç”¨é¤ã€èŠå¤©ã€åˆä½œç­‰ï¼‰
4. æœ‰è¶£çš„åœºæ™¯æˆ–ç»å†ï¼ˆæ„å¤–å‘ç”Ÿçš„è¶£äº‹ã€ç‰¹åˆ«çš„äº’åŠ¨ç­‰ï¼‰
5. é‡åˆ°ç¾ä¸½çš„é£æ™¯ã€ç‰¹æ®Šçš„å¤©æ°”æ™¯è§‚ï¼ˆæ—¥è½ã€æ™šéœã€é›¨åå½©è™¹ç­‰ï¼‰

**å›¾ç‰‡ç±»å‹ (image_type)**ï¼š
- **ä¼˜å…ˆé€‰æ‹©"selfie"ï¼ˆè‡ªæ‹ï¼‰**ï¼šè‡³å°‘å 80%
  - ç¤ºä¾‹ï¼šèµ·åºŠ/ç¡è§‰çŠ¶æ€ã€å¿ƒæƒ…å¥½/ä¸å¥½æ—¶çš„è‡ªæ‹ã€ä¸æœ‹å‹çš„åˆç…§ã€å®Œæˆä»»åŠ¡åçš„çŠ¶æ€ã€æ—¥å¸¸ç”Ÿæ´»ç‰‡æ®µ
  - **ç»å¤§å¤šæ•°æƒ…å†µä¸‹åº”è¯¥é€‰æ‹©selfieç±»å‹**

- "scene"ï¼ˆåœºæ™¯ï¼‰ï¼š**ä»…åœ¨ä»¥ä¸‹ç‰¹æ®Šæƒ…å†µä½¿ç”¨ï¼ˆä¸è¶…è¿‡20%ï¼‰**
  - 1. é‡åˆ°ç‰¹åˆ«å¥½çœ‹çš„åœºæ™¯ï¼ˆç¾ä¸½çš„æ—¥è½ã€æ™šéœã€é›¨åå½©è™¹ç­‰å£®è§‚æ™¯è‰²ï¼‰
  - 2. éœ€è¦ç”¨ç…§ç‰‡è¾…åŠ©è¯´æ˜æ‰èƒ½æœ‰æœ€å¥½çš„å±•ç¤ºæ•ˆæœï¼ˆå¤æ‚çš„å·¥ä½œåœºæ™¯ã€ç‰¹æ®Šçš„ç¯å¢ƒç­‰ï¼‰
  - **ä¸€èˆ¬çš„åœºæ™¯ã€æ™®é€šçš„ç¯å¢ƒä¸éœ€è¦ç”Ÿæˆsceneå›¾ç‰‡ï¼Œä¼˜å…ˆç”¨selfieæ›¿ä»£**

**å›¾ç‰‡åŸå›  (image_reason)**ï¼š
ç®€çŸ­è¯´æ˜ä¸ºä»€ä¹ˆè¦ç”Ÿæˆè¿™å¼ å›¾ç‰‡ï¼ˆ20å­—ä»¥å†…ï¼‰ï¼Œä¾‹å¦‚ï¼š
- "èµ·åºŠæ—¶çš„æ…µæ‡’çŠ¶æ€"
- "å®Œæˆé‡è¦ä»»åŠ¡çš„å¿ƒæƒ…"
- "ä¸èƒ½å¤©ä½¿çš„æœ‰è¶£å¯¹è¯"
- "ç¾ä¸½çš„æ™šéœ"

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š

{{
  "selected_experiences": [
    {{
      "micro_experience_id": "å€™é€‰é¡¹çš„IDï¼ˆä»ä¸Šé¢åˆ—è¡¨ä¸­é€‰æ‹©ï¼‰",
      "need_image": true,
      "image_type": "selfie",
      "image_reason": "ç”ŸæˆåŸå› ï¼ˆ20å­—ä»¥å†…ï¼‰"
    }},
    {{
      "micro_experience_id": "å€™é€‰é¡¹çš„ID",
      "need_image": true,
      "image_type": "scene",
      "image_reason": "ç”ŸæˆåŸå› ï¼ˆ20å­—ä»¥å†…ï¼‰"
    }}
  ]
}}

**é‡è¦æé†’**ï¼š
- åªè¿”å›éœ€è¦ç”Ÿæˆå›¾ç‰‡çš„å¾®è§‚ç»å†
- æ•°é‡å¿…é¡»åœ¨ {min_count}-{max_count} ä¹‹é—´
- ä¼˜å…ˆè‡ªæ‹ï¼ˆselfieï¼‰ï¼Œsceneä¸è¶…è¿‡20%
- æŒ‘é€‰æœ€å€¼å¾—è®°å½•çš„æ—¶åˆ»ï¼Œä¸è¦é€‰æ‹©å¹³æ·¡çš„æ—¥å¸¸
- micro_experience_id å¿…é¡»ä»ä¸Šé¢çš„å€™é€‰åˆ—è¡¨ä¸­é€‰æ‹©
"""

    messages = [{"role": "user", "content": prompt}]

    try:
        # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
        response = await call_structured_generation(messages)

        if "error" in response:
            logger.error(f"[AIå›¾ç‰‡å†³ç­–] AIè°ƒç”¨å¤±è´¥: {response['error']}")
            return []

        # éªŒè¯è¿”å›æ ¼å¼
        if "selected_experiences" not in response:
            logger.error(f"[AIå›¾ç‰‡å†³ç­–] AIè¿”å›æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘selected_experienceså­—æ®µ")
            return []

        selected = response["selected_experiences"]

        # éªŒè¯æ•°é‡
        if not (min_count <= len(selected) <= max_count):
            logger.warning(f"[AIå›¾ç‰‡å†³ç­–] AIé€‰æ‹©äº†{len(selected)}å¼ å›¾ç‰‡ï¼Œä¸åœ¨èŒƒå›´{min_count}-{max_count}å†…")

        # éªŒè¯æ¯ä¸ªé€‰æ‹©çš„IDæ˜¯å¦æœ‰æ•ˆ
        valid_ids = {c["id"] for c in candidates}
        validated_selected = []

        for item in selected:
            exp_id = item.get("micro_experience_id")
            if exp_id not in valid_ids:
                logger.warning(f"[AIå›¾ç‰‡å†³ç­–] æ— æ•ˆçš„ID: {exp_id}ï¼Œè·³è¿‡")
                continue

            validated_selected.append({
                "micro_experience_id": exp_id,
                "need_image": True,  # å¼ºåˆ¶ä¸ºTrue
                "image_type": item.get("image_type", "selfie"),
                "image_reason": item.get("image_reason", "å€¼å¾—è®°å½•çš„æ—¶åˆ»")
            })

        logger.info(f"[AIå›¾ç‰‡å†³ç­–] æˆåŠŸé€‰æ‹© {len(validated_selected)} å¼ å›¾ç‰‡")

        # ç»Ÿè®¡å›¾ç‰‡ç±»å‹
        selfie_count = sum(1 for item in validated_selected if item["image_type"] == "selfie")
        scene_count = sum(1 for item in validated_selected if item["image_type"] == "scene")
        logger.info(f"[AIå›¾ç‰‡å†³ç­–] å›¾ç‰‡ç±»å‹åˆ†å¸ƒ: selfie={selfie_count}, scene={scene_count}")

        return validated_selected

    except Exception as e:
        logger.error(f"[AIå›¾ç‰‡å†³ç­–] è°ƒç”¨å¤±è´¥: {e}")
        return []


async def analyze_intimacy_event(context_messages: list) -> dict:
    """
    åˆ†æäº²å¯†äº’åŠ¨äº‹ä»¶ (CG Gallery Analysis)
    
    Args:
        context_messages: æœ€è¿‘çš„èŠå¤©è®°å½• (list of dict)
        
    Returns:
        {
            "body_part": "Mouth",
            "act_type": "Fellatio",
            "summary": "Short summary",
            "full_story": "Detailed description",
            "tags": ["Tag1", "Tag2"],
            "intensity": 8
        }
    """
    logger.info(f"[AIåˆ†æ] å¼€å§‹åˆ†æäº²å¯†äº‹ä»¶ï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context_messages)}")
    
    # è½¬æ¢ä¸Šä¸‹æ–‡ä¸ºæ–‡æœ¬
    context_text = ""
    for msg in context_messages[-20:]: # å–æœ€è¿‘20æ¡
        role = "å¾·å…‹è¨æ–¯" if msg["role"] == "assistant" else "Kawaro"
        context_text += f"{role}: {msg['content']}\n"
        
    prompt = f"""You are an erotic literature analyst. Analyze the following roleplay interaction between Texas and Kawaro.
The interaction has just reached a "Release" or "Climax" point. Your job is to document this event for a gallery record.

## Context
{context_text}

## Task
Extract the following details from the interaction:
1. **Body Part**: The primary body part involved in the climax or main act (e.g., Mouth, Chest, Hands, Feet, Thighs, Vaginal, Anal, Toy, WholeBody).
2. **Act Type**: The specific act performed (e.g., Fellatio, Paizuri, Handjob, Footjob, Cowgirl, Missionary, DeepThroat, Creampie, etc.).
3. **Intensity**: A score from 1-10 based on the description's heat and emotional depth.
4. **Summary**: A short, one-sentence summary (Chinese, max 20 chars).
5. **Full Story**: A vivid, 3rd-person summary of the entire sequence of events leading to the release (Chinese, ~100 words). Capture the sensations and atmosphere.
6. **Tags**: 3-5 keywords describing the play (e.g., "Sweaty", "Gentle", "Rough", "Lingerie").

## Output Format (JSON Only)
{{
  "body_part": "String",
  "act_type": "String",
  "intensity": Integer,
  "summary": "String",
  "full_story": "String",
  "tags": ["String", "String"]
}}
"""
    messages = [{"role": "user", "content": prompt}]
    
    try:
        # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆæˆ–æ™®é€šè°ƒç”¨ï¼ˆå¦‚æœæ˜¯ OpenAI/Geminiï¼‰
        # è¿™é‡Œä¸ºäº†ç¨³å¦¥ï¼Œä½¿ç”¨æ™®é€šè°ƒç”¨å¹¶è§£æ JSONï¼Œæˆ–è€…å¤ç”¨ structured generation
        # å‡è®¾ call_structured_generation å†…éƒ¨ä½¿ç”¨çš„æ˜¯èƒ½å¤„ç† JSON çš„æ¨¡å‹
        response = await call_structured_generation(messages)
        
        if "error" in response:
            logger.error(f"[AIåˆ†æ] åˆ†æå¤±è´¥: {response['error']}")
            return None
            
        return response
        
    except Exception as e:
        logger.error(f"[AIåˆ†æ] è°ƒç”¨å¼‚å¸¸: {e}")
        return None
