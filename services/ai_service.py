import os
import httpx
import logging
import json
import asyncio
from typing import AsyncGenerator, Optional
from app.config import Settings

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_URL = "https://yunwu.ai/v1/chat/completions"
OPENAI_API_MODEL = "gemini-2.5-pro"

logger = logging.getLogger(__name__)


async def retry_with_backoff(func, max_retries: int = 3, base_delay: float = 1.0):
    """
    é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿
    """
    for attempt in range(max_retries):
        try:
            return await func()
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒé‡è¯•")
                    raise
            else:
                # å…¶ä»–HTTPé”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                raise
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(f"{e}")
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯{type(e)}ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒé‡è¯•")
                raise


async def stream_openrouter(
    messages, model="deepseek/deepseek-chat-v3-0324:free"
) -> AsyncGenerator[str, None]:
    """
    æµå¼è°ƒç”¨OpenRouter APIï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
    """
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ stream_openrouter(): {model}")
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {"model": model, "messages": messages, "stream": True}

    async def _stream_request():
        async with httpx.AsyncClient(timeout=60) as client:
            async with client.stream(
                "POST", OPENROUTER_API_URL, headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                async for chunk in response.aiter_lines():
                    chunk = chunk.strip()
                    if chunk == "":
                        continue  # è·³è¿‡ç©ºè¡Œ
                    if chunk.startswith(":"):
                        # è·³è¿‡SSEæ³¨é‡Šè¡Œ
                        continue
                    if chunk.startswith("data:"):
                        data_part = chunk[5:].strip()
                        if data_part == "[DONE]":
                            # è·³è¿‡æµç»“æŸæ ‡è®°
                            continue
                        try:
                            data = json.loads(data_part)
                            if "choices" in data and data["choices"]:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    yield delta["content"]
                        except json.JSONDecodeError as json_err:
                            logger.error(
                                f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: JSONè§£æé”™è¯¯: {json_err}. åŸå§‹æ•°æ®: '{chunk}'"
                            )
                            continue
                    elif chunk.startswith("event:"):
                        # è·³è¿‡å¦‚ event: end ä¹‹ç±»çš„äº‹ä»¶è¡Œ
                        continue
                    else:
                        # è·³è¿‡æœªçŸ¥è¡Œï¼Œä¸å†è®°å½•warning
                        continue

    # å¯¹äºæµå¼è¯·æ±‚ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç†é‡è¯•é€»è¾‘ï¼Œä¸ä½¿ç”¨ retry_with_backoff
    max_retries = 3
    base_delay = 1.0

    for attempt in range(max_retries):
        try:
            async for chunk in _stream_request():
                yield chunk
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
        except httpx.HTTPStatusError as http_err:
            status_code = http_err.response.status_code
            if status_code == 429:
                logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(
                        "âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: APIè°ƒç”¨é¢‘ç‡é™åˆ¶ (429 Too Many Requests)"
                    )
                    yield "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    return
            else:
                try:
                    error_content = await http_err.response.aread()
                    error_text = (
                        error_content.decode("utf-8") if error_content else "æœªçŸ¥é”™è¯¯"
                    )
                except Exception:
                    error_text = "æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…"

                logger.error(
                    f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {error_text}"
                )
                yield f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
                return
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error(f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
                yield ""
                return


async def stream_reply_ai(
    messages, model=OPENAI_API_MODEL
) -> AsyncGenerator[str, None]:
    """
    æµå¼è°ƒç”¨ Reply AI API (æ”¯æŒ OpenAI åè®®)ï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
    """
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ stream_reply_ai(): {model}")
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    if model == "gemini-2.5-flash":
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "frequency_penalty": 0.3,
            "temperature": 0.7,
            "reasoning_effort": "medium",
        }
    else:
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "frequency_penalty": 0.3,
            "temperature": 0.7,
        }

    async def _stream_request():
        async with httpx.AsyncClient(timeout=60) as client:
            async with client.stream(
                "POST", OPENAI_API_URL, headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                async for chunk in response.aiter_lines():
                    chunk = chunk.strip()
                    if chunk == "":
                        continue  # è·³è¿‡ç©ºè¡Œ
                    if chunk.startswith(":"):
                        # è·³è¿‡SSEæ³¨é‡Šè¡Œ
                        continue
                    if chunk.startswith("data:"):
                        data_part = chunk[5:].strip()
                        if data_part == "[DONE]":
                            # è·³è¿‡æµç»“æŸæ ‡è®°
                            continue
                        try:
                            data = json.loads(data_part)
                            if "choices" in data and data["choices"]:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    yield delta["content"]
                        except json.JSONDecodeError as json_err:
                            logger.error(
                                f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: JSONè§£æé”™è¯¯: {json_err}. åŸå§‹æ•°æ®: '{chunk}'"
                            )
                            continue
                    elif chunk.startswith("event:"):
                        # è·³è¿‡å¦‚ event: end ä¹‹ç±»çš„äº‹ä»¶è¡Œ
                        continue
                    else:
                        # è·³è¿‡æœªçŸ¥è¡Œï¼Œä¸å†è®°å½•warning
                        continue

    # å¯¹äºæµå¼è¯·æ±‚ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç†é‡è¯•é€»è¾‘ï¼Œä¸ä½¿ç”¨ retry_with_backoff
    max_retries = 3
    base_delay = 1.0

    for attempt in range(max_retries):
        try:
            async for chunk in _stream_request():
                yield chunk
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
        except httpx.HTTPStatusError as http_err:
            status_code = http_err.response.status_code
            if status_code == 429:
                logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(
                        "âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: APIè°ƒç”¨é¢‘ç‡é™åˆ¶ (429 Too Many Requests)"
                    )
                    yield "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    return
            else:
                try:
                    error_content = await http_err.response.aread()
                    error_text = (
                        error_content.decode("utf-8") if error_content else "æœªçŸ¥é”™è¯¯"
                    )
                except Exception:
                    error_text = "æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…"

                logger.error(
                    f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {error_text}"
                )
                yield f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
                return
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error(f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
                yield ""
                return


async def stream_ai_chat(messages: list, model: Optional[str] = None):
    """
    æµå¼ç”ŸæˆAIå›å¤ï¼ŒæŒ‰åˆ†éš”ç¬¦åˆ†æ®µè¾“å‡ºã€‚
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œæˆ–è€…æŒ‡å®šçš„æ˜¯ DeepSeek V3 æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ Reply AI æ¸ é“
    if model is None or model == "deepseek-v3-250324":
        logger.info(
            f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ Reply AI æ¸ é“è¿›è¡Œ stream_ai_chat(): {OPENAI_API_MODEL}"
        )
        stream_func = stream_reply_ai
        actual_model = OPENAI_API_MODEL
    else:
        # å¦åˆ™ï¼Œä½¿ç”¨ OpenRouter æ¸ é“
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ OpenRouter æ¸ é“è¿›è¡Œ stream_ai_chat(): {model}")
        stream_func = stream_openrouter
        actual_model = model

    buffer = ""
    async for chunk in stream_func(messages, model=actual_model):
        buffer += chunk

        # ä¼˜å…ˆæŒ‰å¥å·åˆ‡åˆ†ï¼ˆåŒ…æ‹¬ä¸­æ–‡å¥å·ï¼‰
        while True:
            period_index = buffer.find("ã€‚")
            if period_index != -1:
                segment = buffer[: period_index + 1].strip()
                if segment:
                    yield segment
                buffer = buffer[period_index + 1 :]
                continue
            # å†å°è¯•æŒ‰æ¢è¡Œç¬¦åˆ‡åˆ†
            newline_index = buffer.find("\n")
            if newline_index != -1:
                segment = buffer[:newline_index].strip()
                if segment:
                    yield segment
                buffer = buffer[newline_index + 1 :]
                continue
            break

    # æœ€ç»ˆå‰©ä½™å†…å®¹
    if buffer.strip():
        yield buffer.strip()


async def call_openrouter(messages, model="mistralai/mistral-7b-instruct:free") -> str:
    """
    éæµå¼è°ƒç”¨ï¼ˆç”¨äºæ‘˜è¦ç­‰åœºæ™¯ï¼‰
    """
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model,
        "messages": messages,
    }

    async def _call_request():
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_openrouter(): {model}")
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                OPENROUTER_API_URL, headers=headers, json=payload
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

    try:
        return await retry_with_backoff(_call_request)
    except httpx.HTTPStatusError as http_err:
        status_code = http_err.response.status_code
        if status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
            return "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            logger.error(
                f"âŒ OpenRouterè°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {http_err.response.text}"
            )
            return f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
    except Exception as e:
        logger.error(f"âŒ OpenRouterè°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
        return ""


async def call_gemini(messages, model="gemini-2.5-flash") -> str:
    """
    éæµå¼è°ƒç”¨ï¼ˆç”¨äºæ‘˜è¦ç­‰åœºæ™¯ï¼‰
    """
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {GEMINI_API_KEY}",
    }
    payload = {
        "model": model,
        "messages": messages,
    }

    async def _call_request():
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_gemini(): {model}")
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                "https://n8n-xfyamddg.ap-northeast-1.clawcloudrun.com/webhook/gemini",
                headers=headers,
                json=payload,
            )
            logger.info(f"ğŸŒ çŠ¶æ€ç : {response.status_code}")
            logger.info(f"ğŸ“¥ è¿”å›å†…å®¹: {response.text}")
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

    try:
        return await retry_with_backoff(_call_request)
    except httpx.HTTPStatusError as http_err:
        status_code = http_err.response.status_code
        if status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
            return "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            logger.error(
                f"âŒ Gemini è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {http_err.response.text}"
            )
            return f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
    except Exception as e:
        logger.error(f"âŒ Gemini è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
        return ""


# æ–°å¢ OpenAI åè®®è°ƒç”¨å‡½æ•°
async def call_openai(messages, model="gpt-4o-mini") -> str:
    """
    éæµå¼è°ƒç”¨ OpenAI åè®®ï¼ˆç”¨äºæ‘˜è¦ç­‰åœºæ™¯ï¼‰
    """
    SUMMARY_API_KEY = os.getenv("SUMMARY_API_KEY")
    SUMMARY_API_URL = os.getenv(
        "SUMMARY_API_URL", "https://api.openai.com/v1/chat/completions"
    )
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SUMMARY_API_KEY}",
    }
    payload = {
        "model": model,
        "messages": messages,
    }

    async def _call_request():
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_openai(): {model}")
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                SUMMARY_API_URL,
                headers=headers,
                json=payload,
            )
            logger.info(f"ğŸŒ çŠ¶æ€ç : {response.status_code}")
            logger.info(f"ğŸ“¥ è¿”å›å†…å®¹: {response.text}")
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

    try:
        return await retry_with_backoff(_call_request)
    except httpx.HTTPStatusError as http_err:
        status_code = http_err.response.status_code
        if status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
            return "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            logger.error(
                f"âŒ OpenAI è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {http_err.response.text}"
            )
            return f"âŒ APIè°ƒç”¨å¤±è´¥ (é”™è¯¯ä»£ç : {status_code})"
    except Exception as e:
        logger.error(f"âŒ OpenAI è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
        return ""


async def call_ai_summary(prompt: str) -> str:
    """
    è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦ï¼Œå¯ç”¨äº context_merger.pyã€‚
    """
    messages = [{"role": "user", "content": prompt}]
    model = "mistralai/mistral-7b-instruct:free"
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_ai_summary(): {model}")
    # ä½ å¯ä»¥æ ¹æ®éœ€æ±‚è‡ªç”±åˆ‡æ¢æ¨¡å‹å
    return await call_openrouter(messages, model)


# if os.getenv("USE_GEMINI") == "true":
#     STRUCTURED_API_KEY = os.getenv("GEMINI_API_KEY", OPENROUTER_API_KEY)
#     STRUCTURED_API_URL = os.getenv("GEMINI_API_URL", OPENROUTER_API_URL)
#     STRUCTURED_API_MODEL = os.getenv(
#         "GEMINI_MODEL", "deepseek/deepseek-r1-0528:free"
#     )
# else:
STRUCTURED_API_KEY = os.getenv("STRUCTURED_API_KEY")
STRUCTURED_API_URL = os.getenv("STRUCTURED_API_URL", OPENAI_API_URL)
STRUCTURED_API_MODEL = os.getenv("STRUCTURED_API_MODEL", "gemini-2.5-flash")


async def call_structured_generation(messages: list, max_retries: int = 3) -> dict:
    """
    ä¸“ç”¨ç»“æ„åŒ–æ•°æ®ç”Ÿæˆå‡½æ•°ï¼ˆéæµå¼ï¼‰
    å‚æ•°ï¼š
    - messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    è¿”å›ï¼š
    - dict: è§£æåçš„JSONå¯¹è±¡
    - é”™è¯¯æ—¶è¿”å›{"error": é”™è¯¯ä¿¡æ¯}
    """
    headers = {
        "Authorization": f"Bearer {STRUCTURED_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": STRUCTURED_API_MODEL,
        "messages": messages,
        "response_format": {"type": "json_object"},  # å¼ºåˆ¶JSONè¾“å‡º
    }

    async def _call_api():
        logger.info(f"ğŸ”„ ç»“æ„åŒ–ç”Ÿæˆè°ƒç”¨: {STRUCTURED_API_MODEL}")
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:  # å¢åŠ è¶…æ—¶åˆ°60ç§’
                response = await client.post(
                    STRUCTURED_API_URL, headers=headers, json=payload
                )
                response.raise_for_status()
                return response.json()
        except httpx.ReadTimeout:
            logger.warning(f"âš ï¸ ç»“æ„åŒ–ç”Ÿæˆè°ƒç”¨è¶…æ—¶ (æ¨¡å‹: {STRUCTURED_API_MODEL})")
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿é‡è¯•æœºåˆ¶å¤„ç†
        except Exception as e:
            logger.error(f"âŒ ç»“æ„åŒ–ç”Ÿæˆè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            raise

    for attempt in range(max_retries):
        try:
            response_data = await _call_api()
            content = response_data["choices"][0]["message"]["content"]

            # å°è¯•è§£æJSON - é¦–å…ˆå»é™¤å¯èƒ½çš„Markdownä»£ç å—
            try:
                # å¦‚æœå†…å®¹ä»¥```jsonå¼€å¤´ï¼Œå»é™¤æ ‡è®°
                if content.strip().startswith("```json"):
                    # ç§»é™¤å¼€å¤´çš„```jsonå’Œç»“å°¾çš„```
                    content = content.strip().replace("```json", "", 1)
                    if content.endswith("```"):
                        content = content[:-3].strip()

                # å¦‚æœå†…å®¹ä»¥```å¼€å¤´ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰ï¼Œä¹Ÿå°è¯•ç§»é™¤
                elif content.strip().startswith("```"):
                    content = content.strip().replace("```", "", 1)
                    if content.endswith("```"):
                        content = content[:-3].strip()

                # å°è¯•ç›´æ¥è§£æ
                try:
                    result = json.loads(content)
                    if not isinstance(result, dict):
                        raise ValueError("AIè¿”å›çš„ä¸æ˜¯JSONå¯¹è±¡")
                    return result
                except json.JSONDecodeError:
                    # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆJSONå¯¹è±¡
                    start_idx = content.find("{")
                    end_idx = content.rfind("}")
                    if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                        json_str = content[start_idx : end_idx + 1]
                        result = json.loads(json_str)
                        if not isinstance(result, dict):
                            raise ValueError("æå–çš„JSONä¸æ˜¯å¯¹è±¡")
                        logger.warning(
                            f"âš ï¸ ä»å“åº”ä¸­æå–JSONå¯¹è±¡ (å°è¯• {attempt+1}/{max_retries})"
                        )
                        return result
                    else:
                        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")

                # æœ€åä¸€æ¬¡å°è¯•æ—¶è¿”å›é”™è¯¯
                if attempt == max_retries - 1:
                    return {"error": f"JSONè§£æå¤±è´¥: {str(e)}", "raw": content}

                # æ·»åŠ è§£æé”™è¯¯æç¤ºåé‡è¯•
                messages.append(
                    {
                        "role": "system",
                        "content": "è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–ä»£ç å—æ ‡è®°ã€‚",
                    }
                )
                continue

        except httpx.HTTPStatusError as e:
            status_code = e.response.status_code
            error_msg = f"HTTPé”™è¯¯ {status_code}"
            if status_code == 429:
                logger.warning(f"âš ï¸ é€Ÿç‡é™åˆ¶ (å°è¯• {attempt+1}/{max_retries})")
                await asyncio.sleep(2**attempt)  # æŒ‡æ•°é€€é¿
                continue
            else:
                logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {error_msg}")
                return {"error": error_msg}

        except Exception as e:
            error_type = type(e).__name__
            logger.error(
                f"âŒ æœªçŸ¥é”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {error_type}: {str(e)}"
            )
            if attempt == max_retries - 1:
                return {"error": f"è°ƒç”¨å¤±è´¥: {error_type}: {str(e)}"}
            await asyncio.sleep(1)

    return {"error": "è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"}


def get_weather_info(date: str, location: str = "") -> str:
    """
    è·å–æŒ‡å®šæ—¥æœŸå’Œåœ°ç‚¹çš„å¤©æ°”ä¿¡æ¯ï¼ˆæ¥å…¥å’Œé£å¤©æ°”APIï¼Œå¤±è´¥æ—¶é€€å›ä¼ªéšæœºç”Ÿæˆï¼‰

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        location: ä½ç½®ï¼ˆä»…ç”¨äºç§å­ï¼‰

    Returns:
        str: ç»¼åˆå¤©æ°”æè¿°
    """
    import hashlib
    import random
    import httpx
    import logging
    import os

    # é»˜è®¤locationåˆ—è¡¨
    default_locations = [
        "101320101",
        "101320103",
        "14606",
        "1B6D3",
        "1D255",
        "1DC87",
        "275A5",
        "28FE1",
        "2BBD1",
        "2BC09",
        "39CD9",
        "407DA",
        "4622E",
        "55E7E",
        "8A9CA",
        "8E1C5",
        "9173",
        "D5EC3",
        "DD9B5",
        "E87DC",
    ]
    if not location:
        location = random.choice(default_locations)

    try:
        url = os.getenv("HEFENG_API_HOST") + "/v7/weather/7d"
        params = {
            "location": location,
            "key": os.getenv("HEFENG_API_KEY"),
            "lang": "zh",
        }
        response = httpx.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get("code") != "200":
            raise ValueError(f"API error code: {data.get('code')}")
        for day in data.get("daily", []):
            if day.get("fxDate") == date:
                # æ‹¼æ¥å­—æ®µ
                result = (
                    f"ç™½å¤©{day.get('textDay')}ï¼Œå¤œæ™š{day.get('textNight')}ã€‚"
                    f"æ°”æ¸©{day.get('tempMin')}Â°C~{day.get('tempMax')}Â°Cï¼Œ"
                    f"ç™½å¤©é£ï¼š{day.get('windDirDay')} {day.get('windScaleDay')}çº§ï¼Œ"
                    f"å¤œæ™šé£ï¼š{day.get('windDirNight')} {day.get('windScaleNight')}çº§ï¼Œ"
                    f"æ¹¿åº¦ï¼š{day.get('humidity')}%ï¼Œ"
                    f"é™æ°´ï¼š{day.get('precip')}mmï¼Œ"
                    f"ç´«å¤–çº¿æŒ‡æ•°ï¼š{day.get('uvIndex')}ã€‚"
                )
                return result
        day = data["daily"][-1]
        result = (
            f"ç™½å¤©{day.get('textDay')}ï¼Œå¤œæ™š{day.get('textNight')}ã€‚"
            f"æ°”æ¸©{day.get('tempMin')}Â°C~{day.get('tempMax')}Â°Cï¼Œ"
            f"ç™½å¤©é£ï¼š{day.get('windDirDay')} {day.get('windScaleDay')}çº§ï¼Œ"
            f"å¤œæ™šé£ï¼š{day.get('windDirNight')} {day.get('windScaleNight')}çº§ï¼Œ"
            f"æ¹¿åº¦ï¼š{day.get('humidity')}%ï¼Œ"
            f"é™æ°´ï¼š{day.get('precip')}mmï¼Œ"
            f"ç´«å¤–çº¿æŒ‡æ•°ï¼š{day.get('uvIndex')}ã€‚"
        )
        return result
    except Exception as e:
        logging.warning(f"âš ï¸ è·å–å¤©æ°”å¤±è´¥: {e}")

    # å›é€€ï¼šä½¿ç”¨ä¼ªéšæœºå¤©æ°”
    seed = int(hashlib.md5(f"{date}-{location}".encode()).hexdigest()[:8], 16)
    random.seed(seed)

    weather_options = ["æ™´å¤©", "é˜´å¤©", "é›¨å¤©", "é›ªå¤©", "é›¾å¤©"]
    weather_weights = [0.4, 0.25, 0.2, 0.05, 0.1]

    return random.choices(weather_options, weights=weather_weights)[0]


async def generate_daily_schedule(
    date: str,
    day_type: str,
    weather: str,
    is_in_major_event: bool,
    major_event_context: Optional[dict] = None,
    special_flags: Optional[list] = None,
) -> dict:
    """
    åŠŸèƒ½ï¼šç”Ÿæˆä¸»æ—¥ç¨‹
    """
    import uuid

    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”ŸæˆçœŸå®ã€è¿è´¯çš„æ—¥å¸¸ç”Ÿæ´»å®‰æ’ã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åä¿¡ä½¿ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å½“å‰æƒ…å†µ
- æ—¥æœŸ: {date}
- æ—¥æœŸç±»å‹: {day_type} ({'å·¥ä½œæ—¥' if day_type == 'weekday' else 'å‘¨æœ«'})
- å¤©æ°”çŠ¶å†µ: {weather}
- æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­: {'æ˜¯' if is_in_major_event else 'å¦'}"""

    if is_in_major_event and major_event_context:
        prompt += (
            f"\n- å¤§äº‹ä»¶èƒŒæ™¯: {json.dumps(major_event_context, ensure_ascii=False)}"
        )
    if special_flags:
        prompt += f"\n- ç‰¹æ®Šæƒ…å†µ: {', '.join(special_flags)}"

    prompt += f"""

## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œå½“å‰æƒ…å†µï¼Œç”Ÿæˆä¸€ä»½ç¬¦åˆé€»è¾‘çš„æ—¥ç¨‹å®‰æ’ã€‚æ³¨æ„ï¼š
1. å·¥ä½œæ—¥é€šå¸¸åŒ…å«å¿«é€’é…é€ä»»åŠ¡ï¼Œå‘¨æœ«å¯èƒ½æœ‰åŠ ç­æˆ–ä¼‘é—²æ´»åŠ¨
2. å¤©æ°”ä¼šå½±å“æˆ·å¤–æ´»åŠ¨å’Œé…é€éš¾åº¦
3. ä¸åŒäº‹çš„äº’åŠ¨è¦ç¬¦åˆè§’è‰²å…³ç³»
4. æ—¶é—´å®‰æ’è¦åˆç†ï¼Œæ´»åŠ¨ä¹‹é—´è¦æœ‰é€»è¾‘è¿æ¥

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "date": "{date}",
  "day_type": "{day_type}",
  "weather": "{weather}",
  "is_overtime": false,
  "daily_summary": "ç®€è¦æè¿°è¿™ä¸€å¤©çš„æ•´ä½“å®‰æ’å’Œä¸»è¦æ´»åŠ¨",
  "schedule_items": [
    {{
      "start_time": "HH:MM",
      "end_time": "HH:MMï¼ˆå¦‚æœåˆ°æ¬¡æ—¥ï¼Œåˆ™å†™23:59ã€‚æœ€å¤šä¸å¾—è¶…è¿‡23:59ï¼‰", 
      "duration_minutes": æ•°å­—,
      "title": "æ´»åŠ¨æ ‡é¢˜",
      "category": "personal|work|social|rest",
      "priority": "high|medium|low",
      "location": "å…·ä½“åœ°ç‚¹",
      "description": "è¯¦ç»†çš„æ´»åŠ¨æè¿°",
      "weather_affected": trueæˆ–false,
      "companions": ["å‚ä¸çš„å…¶ä»–è§’è‰²"],
      "emotional_impact_tags": ["ç›¸å…³æƒ…ç»ªæ ‡ç­¾"],
      "interaction_potential": "low|medium|high"
    }}
  ]
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨éæµå¼è°ƒç”¨ï¼ŒæŒ‡å®šClaudeæ¨¡å‹
    try:
        # ä½¿ç”¨ä¸“ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
        response = await call_structured_generation(messages)
        if "error" in response:
            return response  # ç›´æ¥è¿”å›é”™è¯¯

        result = response  # å·²ç»æ˜¯è§£æå¥½çš„å­—å…¸

        # ä¸ºæ¯ä¸ªschedule_itemæ·»åŠ UUID
        for item in result.get("schedule_items", []):
            item["id"] = str(uuid.uuid4())

        return result
    except json.JSONDecodeError:
        logger.error(f"âŒ generate_daily_schedule: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response}")
        return {"error": "AIè¿”å›æ ¼å¼é”™è¯¯", "raw_response": response}
    except Exception as e:
        logger.error(f"âŒ generate_daily_schedule: è°ƒç”¨å¤±è´¥: {e}")
        return {"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}


async def generate_major_event(
    duration_days: int,
    event_type: str,
    start_date: str,
    weather_forecast: Optional[dict] = None,
) -> dict:
    """
    åŠŸèƒ½ï¼šç”Ÿæˆå¤§äº‹ä»¶
    """
    import uuid
    from datetime import datetime, timedelta

    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”Ÿæˆé‡è¦çš„ç”Ÿæ´»äº‹ä»¶ã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åä¿¡ä½¿ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å¤§äº‹ä»¶å®šä¹‰
å¤§äº‹ä»¶æ˜¯æŒ‡æŒç»­å¤šå¤©ã€å¯¹å¾·å…‹è¨æ–¯ç”Ÿæ´»äº§ç”Ÿé‡è¦å½±å“çš„äº‹ä»¶ï¼Œå¦‚ï¼š
- é‡è¦çš„é…é€ä»»åŠ¡ï¼ˆè·¨åŸå¸‚ã€é«˜ä»·å€¼è´§ç‰©ï¼‰
- ä¼é¹…ç‰©æµçš„å›¢é˜Ÿæ´»åŠ¨æˆ–åŸ¹è®­
- ä¸ªäººé‡è¦äº‹åŠ¡ï¼ˆæ¬å®¶ã€ä¼‘å‡ã€åŒ»ç–—ç­‰ï¼‰
- é¾™é—¨åŸå¸‚äº‹ä»¶ï¼ˆèŠ‚æ—¥ã€ç´§æ€¥çŠ¶å†µç­‰ï¼‰

## å½“å‰å¤§äº‹ä»¶å‚æ•°
- äº‹ä»¶ç±»å‹: {event_type}
- å¼€å§‹æ—¥æœŸ: {start_date}
- æŒç»­å¤©æ•°: {duration_days}å¤©"""

    if weather_forecast:
        prompt += (
            f"\n- æœŸé—´å¤©æ°”é¢„æŠ¥: {json.dumps(weather_forecast, ensure_ascii=False)}"
        )

    prompt += f"""

## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œäº‹ä»¶å‚æ•°ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å¤§äº‹ä»¶è®¡åˆ’ã€‚æ³¨æ„ï¼š
1. äº‹ä»¶å†…å®¹è¦ç¬¦åˆå¾·å…‹è¨æ–¯çš„èŒä¸šå’Œæ€§æ ¼ç‰¹ç‚¹
2. æ¯æ—¥è®¡åˆ’è¦æœ‰é€»è¾‘è¿è´¯æ€§å’Œæ¸è¿›æ€§
3. è€ƒè™‘å¤©æ°”å¯¹äº‹ä»¶æ‰§è¡Œçš„å½±å“
4. åŒ…å«åˆç†çš„æŒ‘æˆ˜å’Œé£é™©å› ç´ 

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "event_title": "äº‹ä»¶çš„ç®€æ´æ ‡é¢˜",
  "event_type": "{event_type}",
  "main_objective": "è¿™ä¸ªå¤§äº‹ä»¶çš„ä¸»è¦ç›®æ ‡å’Œæ„ä¹‰",
  "total_days": {duration_days},
  "daily_plans": [
    {{
      "day": 1,
      "date": "YYYY-MM-DD",
      "phase": "äº‹ä»¶çš„å½“å‰é˜¶æ®µï¼ˆå¦‚ï¼šå‡†å¤‡é˜¶æ®µã€æ‰§è¡Œé˜¶æ®µã€æ”¶å°¾é˜¶æ®µï¼‰",
      "summary": "å½“æ—¥çš„ä¸»è¦å®‰æ’å’Œç›®æ ‡",
      "key_activities": ["å…·ä½“æ´»åŠ¨1", "å…·ä½“æ´»åŠ¨2"],
      "expected_challenges": ["å¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜"],
      "emotional_state": "å¾·å…‹è¨æ–¯åœ¨è¿™ä¸€å¤©çš„æƒ…ç»ªçŠ¶æ€",
      "location_start": "ä¸€å¤©å¼€å§‹çš„åœ°ç‚¹",
      "location_end": "ä¸€å¤©ç»“æŸçš„åœ°ç‚¹"
    }}
  ],
  "success_criteria": ["åˆ¤æ–­äº‹ä»¶æˆåŠŸçš„æ ‡å‡†"],
  "risk_factors": ["å¯èƒ½å½±å“äº‹ä»¶çš„é£é™©å› ç´ "]
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
    try:
        response = await call_structured_generation(messages)
        if "error" in response:
            return response  # ç›´æ¥è¿”å›é”™è¯¯

        result = response  # å·²ç»æ˜¯è§£æå¥½çš„å­—å…¸

        # æ·»åŠ UUID
        result["event_id"] = str(uuid.uuid4())

        return result
    except json.JSONDecodeError:
        logger.error(f"âŒ generate_major_event: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response}")
        return {"error": "AIè¿”å›æ ¼å¼é”™è¯¯", "raw_response": response}
    except Exception as e:
        logger.error(f"âŒ generate_major_event: è°ƒç”¨å¤±è´¥: {e}")
        return {"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}


async def generate_micro_experiences(
    schedule_item: dict,
    current_date: str,
    previous_experiences: Optional[list] = None,
    major_event_context: Optional[dict] = None,
) -> list:
    """
    åŠŸèƒ½ï¼šä¸ºå•ä¸ªæ—¥ç¨‹é¡¹ç›®ç”Ÿæˆå¤šä¸ªå¾®è§‚ç»å†é¡¹ï¼ˆ5-30åˆ†é’Ÿé¢—ç²’åº¦ï¼‰
    """
    import uuid
    from datetime import datetime, timedelta

    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„å¾®è§‚ç»å†ç”Ÿæˆæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”ŸæˆçœŸå®ã€ç»†è…»çš„ç”Ÿæ´»ç‰‡æ®µã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åå‘˜å·¥ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å½“å‰æƒ…å†µ
- å½“å‰æ—¥æœŸ: {current_date}
- æ—¥ç¨‹é¡¹ç›®: {schedule_item.get('title', 'æœªçŸ¥æ´»åŠ¨')}
- é¡¹ç›®å¼€å§‹æ—¶é—´: {schedule_item.get('start_time', 'æœªçŸ¥')}
- é¡¹ç›®ç»“æŸæ—¶é—´: {schedule_item.get('end_time', 'æœªçŸ¥')}
- æ´»åŠ¨åœ°ç‚¹: {schedule_item.get('location', 'æœªçŸ¥åœ°ç‚¹')}
- æ´»åŠ¨æè¿°: {schedule_item.get('description', 'æ— æè¿°')}
- åŒä¼´: {', '.join(schedule_item.get('companions', [])) if schedule_item.get('companions') else 'ç‹¬è‡ªä¸€äºº'}"""

    if previous_experiences:
        prompt += f"\n- ä¹‹å‰çš„ç»å†æ‘˜è¦: {json.dumps(previous_experiences, ensure_ascii=False)}"
    if major_event_context:
        prompt += (
            f"\n- å¤§äº‹ä»¶èƒŒæ™¯: {json.dumps(major_event_context, ensure_ascii=False)}"
        )

    prompt += f"""## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œå½“å‰æƒ…å†µï¼Œå°†æ—¥ç¨‹é¡¹ç›®æ‹†è§£æˆå¤šä¸ª5-30åˆ†é’Ÿé¢—ç²’åº¦çš„å¾®è§‚ç»å†é¡¹ã€‚æ³¨æ„ï¼š
1. æ¯ä¸ªç»å†é¡¹åº”åŒ…å«å…·ä½“çš„æ—¶é—´æ®µï¼ˆå¼€å§‹å’Œç»“æŸæ—¶é—´ï¼‰å¹¶ä¸”æ‰€æœ‰å¾®è§‚ç»å†è¿ç»­èµ·æ¥æ•´ä½“ä¸Šè¦ä»å¤´åˆ°åˆ°å°¾è¦†ç›–æ•´ä¸ªæ—¥ç¨‹é¡¹ç›®
2. å†…å®¹è¦ç¬¦åˆå¾·å…‹è¨æ–¯çš„æ€§æ ¼ç‰¹ç‚¹ï¼ˆå†·é™ã€ä¸“ä¸šã€å†…æ•›ï¼‰
3. æƒ…ç»ªè¡¨è¾¾è¦ç»†è…»ä½†ä¸å¤¸å¼ 
4. æ€è€ƒè¦ç¬¦åˆå¥¹çš„èŒä¸šèƒŒæ™¯å’Œç»å†
5. å¦‚æœéœ€è¦äº¤äº’ï¼Œè¦ç¬¦åˆè§’è‰²å…³ç³»å’Œæƒ…å¢ƒ

## ä¸»åŠ¨äº¤äº’é¡»çŸ¥

è¿™æ˜¯ä¸€ä¸ª AI è§’è‰²æ‰®æ¼”çš„ä¸€éƒ¨åˆ†ã€‚è¿™é‡Œæ˜¯æ¨¡æ‹Ÿè§’è‰²çš„æ—¥å¸¸ç”Ÿæ´»ã€‚æ‰€è°“ä¸»åŠ¨äº¤äº’ï¼Œæ˜¯æŒ‡è§’è‰²ï¼ˆå¾·å…‹è¨æ–¯ï¼‰æ˜¯å¦è¦ä¸ç”¨æˆ·è¿›è¡Œäº’åŠ¨ã€‚
å¦‚æœéœ€è¦ï¼Œäº¤äº’çš„å†…å®¹åˆ™æ˜¯è§’è‰²ï¼ˆå¾·å…‹è¨æ–¯ï¼‰å‘é€ç»™ç”¨æˆ·çš„å†…å®¹ã€‚
å¦‚æœå¾·å…‹è¨æ–¯è®¤ä¸ºè¿™ä»¶äº‹å€¼å¾—åˆ†äº«ç»™ç”¨æˆ·ï¼Œåˆ™è®¾ç½®ä¸ºtureï¼Œäº¤äº’å†…å®¹æ˜¯å¾·å…‹è¨æ–¯å¯¹è¿™ä»¶äº‹æƒ³è¦å’Œç”¨æˆ·åˆ†äº«çš„ç»å†å’Œæ„Ÿå—ã€‚
è€Œä¸æ˜¯æŒ‡å¯¹å¾·å…‹è¨æ–¯æ—¥ç¨‹ä¸­çš„ä¼™ä¼´ï¼Œè€Œæ˜¯å’Œå¥¹åªèƒ½é€šè¿‡ç½‘ç»œè¿›è¡Œäº¤æµï¼Œä½†æ˜¯æ˜¯å…³ç³»æœ€å¥½çš„æœ‹å‹çš„ä¸»åŠ¨äº¤äº’ã€‚å³åˆ¤æ–­æ­¤æ—¶å¾·å…‹è¨æ–¯æ˜¯å¦ä¼šæƒ³è¦å°†å½“å‰çš„ç»å†å‘é€ç»™è¯¥å¥½å‹ã€‚
æ³¨æ„ï¼Œå¦‚æœæ˜¯ä¸æ—©ä¸Šèµ·åºŠç›¸å…³çš„æ—¥ç¨‹ï¼Œåˆ™å¿…é¡»åœ¨æŸä¸€ä¸ªåˆé€‚çš„itemä¸­è®¾ç½®need_interactionä¸ºtrueï¼Œäº¤äº’å†…å®¹æ˜¯å¾·å…‹è¨æ–¯å¯¹æ—©ä¸Šèµ·åºŠçš„æ„Ÿå—å’Œé“æ—©å®‰ã€‚
ä¸»åŠ¨äº¤äº’ä¸ºtrueå¤§æ¦‚è¦å æ®40%å·¦å³ï¼Œä¸è¦è¿‡ä½ï¼Œä½†ä¸è¦è¶…è¿‡ä¸€åŠã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "date": "{current_date}",
  "schedule_item_id": "{schedule_item.get('id', '')}",
  "items": [
    {{
      "start_time": "HH:MM",
      "end_time": "HH:MM",
      "content": "è¯¦ç»†æè¿°è¿™æ®µç»å†",
      "emotions": "æƒ…ç»ªçŠ¶æ€",
      "thoughts": "å†…å¿ƒçš„æƒ³æ³•",
      "need_interaction": trueæˆ–false,
      "interaction_content": "äº¤äº’å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰"
    }},
    // æ›´å¤šç»å†é¡¹...
  ],
  "created_at": "è‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€å¡«å†™"
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
    try:
        response = await call_structured_generation(messages)
        if "error" in response:
            return [response]  # è¿”å›é”™è¯¯åˆ—è¡¨

        # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
        if "items" not in response or not isinstance(response["items"], list):
            return [
                {"error": "AIè¿”å›æ ¼å¼é”™è¯¯: ç¼ºå°‘itemsåˆ—è¡¨", "raw_response": response}
            ]

        # ä¸ºæ¯ä¸ªç»å†é¡¹æ·»åŠ å”¯ä¸€ID
        for item in response["items"]:
            item["id"] = str(uuid.uuid4())
            item["schedule_item_id"] = schedule_item.get("id", "")

        return response["items"]
    except json.JSONDecodeError:
        logger.error(f"âŒ generate_micro_experiences: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON")
        return [{"error": "AIè¿”å›æ ¼å¼é”™è¯¯"}]
    except Exception as e:
        logger.error(f"âŒ generate_micro_experiences: è°ƒç”¨å¤±è´¥: {e}")
        return [{"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}]


async def summarize_past_micro_experiences(experiences: list) -> str:
    """
    åŠŸèƒ½ï¼šå°†è¿‡å»çš„å¾®è§‚ç»å†æ•´ç†æˆæ•…äº‹åŒ–çš„æ–‡æœ¬
    """
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯ï¼ˆæ˜æ—¥æ–¹èˆŸè§’è‰²ï¼‰ã€‚
ç°åœ¨è¯·ä½ ä»¥ç¬¬ä¸€äººç§°å›é¡¾åˆšåˆšç»å†çš„å¾®è§‚äº‹ä»¶ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä»½å®Œæ•´ã€çœŸå®ã€æœ‰æ¡ç†çš„è‡ªæˆ‘è®°å½•ã€‚

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
	1.	æŒ‰ç…§æ—¶é—´é¡ºåºï¼Œé€æ¡ç”¨è‡ªç„¶è¯­è¨€æµç•…åœ°é™ˆè¿°æ¯ä¸€æ®µç»å†çš„å‘ç”Ÿå†…å®¹ã€æ‰€è§æ‰€é—»ã€å†…å¿ƒæƒ³æ³•ã€æƒ…ç»ªå˜åŒ–ï¼›
	2.	ä¸å¾—é—æ¼ä»»ä½•ç»å†é¡¹ï¼Œæ¯æ®µç»å†éƒ½è¦è¦†ç›–åŸºæœ¬è¦ç´ ï¼ˆåšäº†ä»€ä¹ˆã€æƒ³äº†ä»€ä¹ˆã€å½“æ—¶çš„æƒ…ç»ªï¼‰ï¼›
	3.	ä¸è¿›è¡Œæ–‡å­¦åŒ–åŠ å·¥ï¼Œä¹Ÿä¸ç¼–é€ æœªåœ¨ç»å†ä¸­å‡ºç°çš„å†…å®¹ï¼›
	4.	å¦‚æœæŸäº›ç»å†ä¹‹é—´å­˜åœ¨å‰åå…³è”ï¼Œå¯ä»¥æŒ‡å‡ºï¼Œè®©è¡”æ¥æµç•…ã€‚

ä½ æ­£åœ¨ç”Ÿæˆçš„æ–‡æœ¬ç›®çš„åœ¨äºå®Œæ•´è®°å½•å½“å¤©ç”Ÿæ´»ç»†èŠ‚ã€‚
æ³¨æ„è¯­è¨€è¦è¿è´¯è‡ªç„¶ï¼Œè®©å…¶ä»–äººé˜…è¯»çš„æ—¶å€™ï¼Œèƒ½ç†è§£ä½ çš„æƒ³æ³•ï¼Œäº†è§£ä½ ä»Šå¤©ä¸ºæ­¢çš„å…¨éƒ¨ç»å†ã€‚
æ³¨æ„è¯¦ç•¥å¾—å½“ï¼ŒæŠŠä½ è®¤ä¸ºå°è±¡æ·±åˆ»çš„å†…å®¹è¯¦ç»†åœ°è®°å½•ä¸‹æ¥ã€‚å…¶ä»–çš„å¯ä»¥ç®€è¦ä¸€äº›ã€‚
æœ‰ç‚¹ç±»ä¼¼äºæ—¥è®°ï¼Œæˆ–è€…æ˜¯ä½ ç»å†è¿™äº›äº‹æƒ…åçš„å›å¿†è¿‡ç¨‹ã€‚

ä»¥ä¸‹æ˜¯ä½ ä»Šå¤©çš„å¾®è§‚ç»å†æ•°æ®ï¼š
{json.dumps(experiences, ensure_ascii=False, indent=2)}

è¯·å¼€å§‹è®°å½•ï¼š
"""

    messages = [{"role": "user", "content": prompt}]

    try:
        # ä½¿ç”¨éæµå¼è°ƒç”¨ï¼Œè·å–æ•…äº‹åŒ–æ–‡æœ¬
        # response = await call_openrouter(
        #     messages, model="deepseek/deepseek-chat-v3-0324:free"
        # )
        response = await call_openai(messages, model="gpt-4o-mini")
        return response
    except Exception as e:
        logger.error(f"âŒ summarize_past_micro_experiences: è°ƒç”¨å¤±è´¥: {e}")
        return f"æ•…äº‹ç”Ÿæˆå¤±è´¥: {str(e)}"
