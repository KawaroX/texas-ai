import os
import httpx
import logging
import json
import asyncio
import re  # Add this import
import redis.asyncio as redis
from typing import AsyncGenerator, Optional
from app.config import Settings

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_API_KEY2 = os.getenv("GEMINI_API_KEY2", "")
GEMINI_API_URL = "https://gemini-v.kawaro.space/v1beta/models"

# os.getenv(
#     "GEMINI_API_URL", "https://gemini-v.kawaro.space/v1beta/models"
# )

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_URL = "https://yunwu.ai/v1/chat/completions"
OPENAI_API_MODEL = "claude-3-7-sonnet-20250219"  # é»˜è®¤æ¨¡å‹æ”¹ä¸º claude-3-7-sonnet-20250219



logger = logging.getLogger(__name__)

# === compact payload logging helpers ===
def _truncate_for_log(s: str, limit: int = 20) -> str:
    try:
        return (s[:limit] + ("â€¦" if len(s) > limit else ""))
    except Exception:
        return str(s)[:limit]


def _estimate_tokens_simple(s: str) -> int:
    """
    Very rough token estimate: ~4 characters per token.
    Avoids heavy deps like tiktoken while giving an order-of-magnitude view.
    """
    try:
        return max(1, (len(s) + 3) // 4)
    except Exception:
        return 1


def summarize_payload_for_log(payload: dict, preview_len: int = 20) -> dict:
    """
    Recursively summarize a payload:
    - For every string field, include length, approx token count, and first N chars.
    - For lists/dicts, preserve structure but replace string leaves with summaries.
    - Also compute an approx total token count across all string fields.
    """
    total_tokens = 0

    def walk(node):
        nonlocal total_tokens
        if isinstance(node, str):
            t = _estimate_tokens_simple(node)
            total_tokens += t
            return {
                "len": len(node),
                "tokens": t,
                "preview": _truncate_for_log(node, preview_len),
            }
        elif isinstance(node, list):
            return [walk(x) for x in node]
        elif isinstance(node, dict):
            return {k: walk(v) for k, v in node.items()}
        else:
            return node

    try:
        summarized = walk(payload)
    except Exception as e:
        summarized = {"error": f"summarize_failed: {type(e).__name__}: {e}"}

    summarized["_approx_total_tokens"] = total_tokens
    return summarized
# === end helpers ===

# === Redis-based runtime config for Gemini streaming ===
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
REDIS_GEMINI_CFG_KEY = os.getenv("REDIS_GEMINI_CFG_KEY", "texas:llm:gemini_cfg")
_redis = redis.from_url(REDIS_URL, encoding="utf-8", decode_responses=True)

DEFAULT_GEMINI_CFG = {
    "model": "gemini-2.5-pro",
    "connect_timeout": 10.0,
    "read_timeout": 60.0,
    "write_timeout": 60.0,
    "pool_timeout": 60.0,
    "stop_sequences": ["SEND", "NO_REPLY"],
    "include_thoughts": True,
    "thinking_budget": 32768,
    "response_mime_type": "text/plain",
}

async def load_gemini_cfg() -> dict:
    """
    ä» Redis è¯»å–ä¸€æ¬¡æ€§é…ç½®å¿«ç…§ï¼›å¤±è´¥æˆ–ç¼ºé¡¹æ—¶ä½¿ç”¨é»˜è®¤å€¼å…œåº•ã€‚
    """
    try:
        raw = await _redis.get(REDIS_GEMINI_CFG_KEY)
        if not raw:
            # Redis æ— é…ç½®æ—¶ï¼Œå†™å…¥é»˜è®¤å€¼å¹¶è¿”å›
            try:
                await _redis.set(REDIS_GEMINI_CFG_KEY, json.dumps(DEFAULT_GEMINI_CFG, ensure_ascii=False))
                logger.info(f"ğŸ”§ Redis æ— é…ç½®ï¼Œå·²å†™å…¥é»˜è®¤ Gemini é…ç½®: {DEFAULT_GEMINI_CFG}")
            except Exception as se:
                logger.warning(f"âš ï¸ å†™å…¥é»˜è®¤ Gemini é…ç½®åˆ° Redis å¤±è´¥: {se}")
            return DEFAULT_GEMINI_CFG
        user_cfg = json.loads(raw)
        # åˆå¹¶é»˜è®¤å€¼ï¼Œé¿å…ç¼ºå­—æ®µ
        merged = {**DEFAULT_GEMINI_CFG, **(user_cfg or {})}
        return merged
    except Exception as e:
        logger.warning(f"âš ï¸ è¯»å– Gemini é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        return DEFAULT_GEMINI_CFG


async def retry_with_backoff(func, max_retries: int = 3, base_delay: float = 1.0):
    """
    é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿
    """
    for attempt in range(max_retries):
        try:
            return await func()
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒé‡è¯•")
                    raise
            else:
                # å…¶ä»–HTTPé”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                raise
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(f"{e}")
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯{type(e)}ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒé‡è¯•")
                raise


async def stream_openrouter(
    messages, model="deepseek/deepseek-chat-v3-0324:free"
) -> AsyncGenerator[str, None]:
    """
    æµå¼è°ƒç”¨OpenRouter APIï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
    """
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ stream_openrouter(): {model}")
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {"model": model, "messages": messages, "stream": True}

    async def _stream_request():
        async with httpx.AsyncClient(timeout=60) as client:
            async with client.stream(
                "POST", OPENROUTER_API_URL, headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                async for chunk in response.aiter_lines():
                    chunk = chunk.strip()
                    if chunk == "":
                        continue  # è·³è¿‡ç©ºè¡Œ
                    if chunk.startswith(":"):
                        # è·³è¿‡SSEæ³¨é‡Šè¡Œ
                        continue
                    if chunk.startswith("data:"):
                        data_part = chunk[5:].strip()
                        if data_part == "[DONE]":
                            # è·³è¿‡æµç»“æŸæ ‡è®°
                            continue
                        try:
                            data = json.loads(data_part)
                            if "choices" in data and data["choices"]:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    yield delta["content"]
                        except json.JSONDecodeError as json_err:
                            logger.error(
                                f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: JSONè§£æé”™è¯¯: {json_err}. åŸå§‹æ•°æ®: '{chunk}'"
                            )
                            continue
                    elif chunk.startswith("event:"):
                        # è·³è¿‡å¦‚ event: end ä¹‹ç±»çš„äº‹ä»¶è¡Œ
                        continue
                    else:
                        # è·³è¿‡æœªçŸ¥è¡Œï¼Œä¸å†è®°å½•warning
                        continue

    # å¯¹äºæµå¼è¯·æ±‚ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç†é‡è¯•é€»è¾‘ï¼Œä¸ä½¿ç”¨ retry_with_backoff
    max_retries = 3
    base_delay = 1.0

    for attempt in range(max_retries):
        try:
            async for chunk in _stream_request():
                yield chunk
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
        except httpx.HTTPStatusError as http_err:
            status_code = http_err.response.status_code
            if status_code == 429:
                logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(
                        "âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: APIè°ƒç”¨é¢‘ç‡é™åˆ¶ (429 Too Many Requests)"
                    )
                    yield "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    return
            else:
                try:
                    error_content = await http_err.response.aread()
                    error_text = (
                        error_content.decode("utf-8") if error_content else "æ— å“åº”å†…å®¹"
                    )
                except Exception as read_err:
                    error_text = f"æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…: {read_err}"

                logger.error(
                    f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code}. URL: {http_err.request.url}. å“åº”å¤´: {http_err.response.headers}. é”™è¯¯è¯¦æƒ…: {error_text}"
                )
                yield f"[è‡ªåŠ¨å›å¤] åœ¨å¿™ï¼Œæœ‰äº‹è¯·ç•™è¨€ ({status_code})"
                return
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error(f"âŒ OpenRouteræµå¼è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
                yield ""
                return


async def stream_reply_ai(
    messages, model=OPENAI_API_MODEL
) -> AsyncGenerator[str, None]:
    """
    æµå¼è°ƒç”¨ Reply AI API (æ”¯æŒ OpenAI åè®®)ï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ã€‚
    """
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ stream_reply_ai(): {model}")
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    if model == "gemini-2.5-pro":
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "frequency_penalty": 0.3,
            "temperature": 0.75,
            "presence_penalty": 0.3,
            "top_p": 0.95,
            # "max_tokens": 512,
            "extra_body": {
                "google": {
                    "thinking_config": {
                        "thinking_budget": 16384,
                        "include_thoughts": False,
                    }
                }
            },
        }
    if model == "gpt-5-2025-08-07":
        payload = {
            "model": model,
            "messages": messages,
            "reasoning_effort": "high", 
            "verbosity": "medium",
            "stream": True,
            "frequency_penalty": 0.3,
            "temperature": 0.75,
            "presence_penalty": 0.3,
            "max_tokens": 512,
        }
    else:
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "frequency_penalty": 0.3,
            "temperature": 0.75,
            "presence_penalty": 0.3,
            "top_p": 0.95,
            "max_tokens": 512,
        }

    async def _stream_request():
        async with httpx.AsyncClient(timeout=60) as client:
            async with client.stream(
                "POST", OPENAI_API_URL, headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                async for chunk in response.aiter_lines():
                    chunk = chunk.strip()
                    if chunk == "":
                        continue  # è·³è¿‡ç©ºè¡Œ
                    if chunk.startswith(":"):
                        # è·³è¿‡SSEæ³¨é‡Šè¡Œ
                        continue
                    if chunk.startswith("data:"):
                        data_part = chunk[5:].strip()
                        if data_part == "[DONE]":
                            # è·³è¿‡æµç»“æŸæ ‡è®°
                            continue
                        try:
                            data = json.loads(data_part)
                            if "choices" in data and data["choices"]:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    yield delta["content"]
                        except json.JSONDecodeError as json_err:
                            logger.error(
                                f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: JSONè§£æé”™è¯¯: {json_err}. åŸå§‹æ•°æ®: '{chunk}'"
                            )
                            continue
                    elif chunk.startswith("event:"):
                        # è·³è¿‡å¦‚ event: end ä¹‹ç±»çš„äº‹ä»¶è¡Œ
                        continue
                    else:
                        # è·³è¿‡æœªçŸ¥è¡Œï¼Œä¸å†è®°å½•warning
                        continue

    # å¯¹äºæµå¼è¯·æ±‚ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç†é‡è¯•é€»è¾‘ï¼Œä¸ä½¿ç”¨ retry_with_backoff
    max_retries = 3
    base_delay = 1.0

    for attempt in range(max_retries):
        try:
            async for chunk in _stream_request():
                yield chunk
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
        except httpx.HTTPStatusError as http_err:
            status_code = http_err.response.status_code
            if status_code == 429:
                logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.warning(
                        f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(
                        "âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: APIè°ƒç”¨é¢‘ç‡é™åˆ¶ (429 Too Many Requests)"
                    )
                    yield "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    return
            else:
                try:
                    error_content = await http_err.response.aread()
                    error_text = (
                        error_content.decode("utf-8") if error_content else "æ— å“åº”å†…å®¹"
                    )
                except Exception as read_err:
                    error_text = f"æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…: {read_err}"

                logger.error(
                    f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code}. URL: {http_err.request.url}. å“åº”å¤´: {http_err.response.headers}. é”™è¯¯è¯¦æƒ…: {error_text}"
                )
                yield f"[è‡ªåŠ¨å›å¤] åœ¨å¿™ï¼Œæœ‰äº‹è¯·ç•™è¨€ ({status_code})"
                return
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2**attempt)
                logger.warning(
                    f"âš ï¸ é‡åˆ°æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {e}"
                )
                await asyncio.sleep(delay)
                continue
            else:
                logger.error(f"âŒ Reply AIæµå¼è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
                yield ""
                return


async def stream_ai_chat(messages: list, model: Optional[str] = None):
    """
    æµå¼ç”ŸæˆAIå›å¤ï¼ŒæŒ‰åˆ†éš”ç¬¦åˆ†æ®µè¾“å‡ºã€‚
    ä¿®å¤é‡å¤å‘é€é—®é¢˜ã€‚
    """
    # æ¨¡å‹é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜...
    if model is None or model == "deepseek-v3-250324":
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ Reply AI æ¸ é“è¿›è¡Œ stream_ai_chat(): {OPENAI_API_MODEL}")
        stream_func = stream_reply_ai
        actual_model = OPENAI_API_MODEL
    elif model == "gemini-api":
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ Gemini API æ¸ é“è¿›è¡Œ stream_ai_chat(): {model}")
        stream_func = stream_reply_ai_by_gemini
        actual_model = "gemini-2.5-pro"
    else:
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ OpenRouter æ¸ é“è¿›è¡Œ stream_ai_chat(): {model}")
        stream_func = stream_openrouter
        actual_model = model

    def clean_segment(text):
        """æ¸…ç†æ–‡æœ¬ä¸­çš„æ—¶é—´æˆ³å’Œå‘è¨€äººæ ‡è¯†"""
        return re.sub(
            r"^\(è·ç¦»ä¸Šä¸€æ¡æ¶ˆæ¯è¿‡å»äº†ï¼š(\d+[hms]( \d+[hms])?)*\) \[\d{2}:\d{2}:\d{2}\] [^:]+:\s*",
            "",
            text,
        ).strip()

    buffer = ""
    total_processed = 0  # è·Ÿè¸ªå·²å¤„ç†çš„å­—ç¬¦æ•°
    
    async for chunk in stream_func(messages, model=actual_model):
        buffer += chunk

        while True:
            original_buffer_len = len(buffer)
            
            # ä¼˜å…ˆæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ‡åˆ†
            period_index = buffer.find("ã€‚")
            question_index = buffer.find("ï¼Ÿ") 
            exclamation_index = buffer.find("!")

            indices = [i for i in [period_index, question_index, exclamation_index] if i != -1]
            
            if indices:
                earliest_index = min(indices)
                segment = buffer[:earliest_index + 1].strip()
                cleaned_segment = clean_segment(segment)
                if cleaned_segment:
                    yield cleaned_segment
                buffer = buffer[earliest_index + 1:]
                total_processed += earliest_index + 1
                continue
                
            # å†å°è¯•æŒ‰æ¢è¡Œç¬¦åˆ‡åˆ†
            newline_index = buffer.find("\n")
            if newline_index != -1:
                segment = buffer[:newline_index].strip()
                cleaned_segment = clean_segment(segment)
                if cleaned_segment:
                    yield cleaned_segment
                buffer = buffer[newline_index + 1:]
                total_processed += newline_index + 1
                continue
                
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†å‰²ç‚¹ï¼Œè·³å‡ºå¾ªç¯
            break

    # å¤„ç†æœ€ç»ˆå‰©ä½™å†…å®¹ - åªæœ‰å½“bufferä¸­è¿˜æœ‰æœªå¤„ç†çš„å†…å®¹æ—¶æ‰å¤„ç†
    if buffer.strip():
        final_segment = clean_segment(buffer)
        if final_segment:
            yield final_segment


async def call_openrouter(messages, model="mistralai/mistral-7b-instruct:free") -> str:
    """
    éæµå¼è°ƒç”¨ï¼ˆç”¨äºæ‘˜è¦ç­‰åœºæ™¯ï¼‰
    """
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model,
        "messages": messages,
    }

    async def _call_request():
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_openrouter(): {model}")
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                OPENROUTER_API_URL, headers=headers, json=payload
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

    try:
        return await retry_with_backoff(_call_request)
    except httpx.HTTPStatusError as http_err:
        status_code = http_err.response.status_code
        if status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
            return "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            logger.error(
                f"âŒ OpenRouterè°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code} - {http_err.response.text}"
            )
            return f"[è‡ªåŠ¨å›å¤] åœ¨å¿™ï¼Œæœ‰äº‹è¯·ç•™è¨€ ({status_code})"
    except Exception as e:
        logger.error(f"âŒ OpenRouterè°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
        return ""


async def stream_reply_ai_by_gemini(
    messages, model="gemini-2.5-pro"
) -> AsyncGenerator[str, None]:
    """
    æ ‡å‡†æµå¼ï¼šé€æ¡ä» SSE è¯»å–å†…å®¹å¹¶ç«‹åˆ» yieldï¼ˆä¸å†ç¼“å†²åˆ°æœ€åï¼‰ã€‚
    """
    cfg = await load_gemini_cfg()  # ä» Redis è·å–ä¸€æ¬¡æ€§é…ç½®å¿«ç…§
    model = cfg["model"]
    max_retries = 1  # ä»…é‡è¯• 1 æ¬¡ï¼ˆæ€»å…± 2 æ¬¡å°è¯•ï¼šé»˜è®¤é…ç½®ä¸€æ¬¡ + å¼ºåˆ¶é—ªç”µç‰ˆä¸€æ¬¡ï¼‰

    logger.debug(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ stream_reply_ai_by_gemini(): {model}")

    headers = {
        "Content-Type": "application/json",
    }
    if GEMINI_API_KEY2:
        logger.debug("ä½¿ç”¨ GEMINI_API_KEY2")
        headers["x-goog-api-key"] = f"{GEMINI_API_KEY},{GEMINI_API_KEY2}"
    else:
        headers["x-goog-api-key"] = GEMINI_API_KEY

    # å°† OpenAI åè®®çš„ messages è½¬æ¢ä¸º Gemini åè®®çš„ contents
    system_instruction = {}
    gemini_contents = []
    for msg in messages:
        role = msg.get("role")
        content = msg.get("content", "")
        if role == "system":
            system_instruction["parts"] = [{"text": content}]
        elif role == "user":
            gemini_contents.append({"role": "user", "parts": [{"text": content}]})
        elif role == "assistant":
            gemini_contents.append({"role": "model", "parts": [{"text": content}]})

    logger.debug(f"è½¬æ¢åçš„ Gemini contents: {gemini_contents}")
    system_prompt = (
        system_instruction.get("parts", [{"text": ""}])[0].get("text", "")[:100]
    )
    logger.debug(f"system prompt: {system_prompt}...")

    payload = {
        "system_instruction": system_instruction,
        "contents": gemini_contents,
        "generationConfig": {
            "stopSequences": cfg["stop_sequences"],
            "responseMimeType": cfg["response_mime_type"],
            "thinkingConfig": {
                "thinkingBudget": cfg["thinking_budget"],
                "includeThoughts": cfg["include_thoughts"],
            },
        },
    }
    # Compact log: show per-field previews (<=20 chars) and approx token counts
    _payload_summary = summarize_payload_for_log(payload, preview_len=20)
    logger.debug(
        f"\nå‘é€ç»™ Gemini API çš„ payload(æ‘˜è¦): {json.dumps(_payload_summary, indent=2, ensure_ascii=False)}\n"
    )

    # æ ‡å‡†æµå¼ï¼šè¡Œåˆ°è¾¾å³ yield
    for retry_count in range(max_retries + 1):
        yielded_any = False
        try:
            # ç¬¬äºŒæ¬¡å°è¯•ï¼šå¼ºåˆ¶åˆ‡æ¢åˆ°æ›´å¿«çš„æ¨¡å‹å¹¶å°†æ€è€ƒé•¿åº¦å›ºå®šä¸º 24576
            if retry_count == 1:
                model = "gemini-2.5-flash"
                # è¦†ç›–æ€è€ƒé•¿åº¦ï¼Œä»…å¯¹æœ¬æ¬¡å°è¯•ç”Ÿæ•ˆï¼Œå…¶ä½™é…ç½®ä¿æŒä¸å˜
                payload["generationConfig"]["thinkingConfig"]["thinkingBudget"] = 24576
                logger.warning("âš™ï¸ ç¬¬äºŒæ¬¡å°è¯•ï¼šå¼ºåˆ¶ä½¿ç”¨ gemini-2.5-flashï¼ŒthinkingBudget=24576")
            full_url = f"{GEMINI_API_URL}/{model}:streamGenerateContent?alt=sse"
            if retry_count > 0:
                logger.warning(f"ğŸ”„ ç¬¬ {retry_count} æ¬¡é‡è¯•è¯·æ±‚: {full_url}")
            else:
                logger.debug(f"ğŸš€ å¼€å§‹å‘ Gemini API å‘é€è¯·æ±‚: {full_url}")

            # è¶…æ—¶ï¼šé¦–åŒ…ä¸¥æ ¼ç”± connect å†³å®šï¼›è¿ä¸Šå read å®½æ¾
            timeout = httpx.Timeout(
                connect=cfg["connect_timeout"],
                read=cfg["read_timeout"],
                write=cfg["write_timeout"],
                pool=cfg["pool_timeout"],
            )
            async with httpx.AsyncClient(timeout=timeout) as client:
                async with client.stream(
                    "POST", full_url, headers=headers, json=payload
                ) as response:
                    logger.debug(f"ğŸŒ Gemini API å“åº”çŠ¶æ€ç : {response.status_code}")
                    response.raise_for_status()

                    async for raw_line in response.aiter_lines():
                        line = (raw_line or "").strip()
                        if not line:
                            continue  # è·³è¿‡ç©ºè¡Œ
                        if line.startswith(":"):
                            continue  # è·³è¿‡ SSE æ³¨é‡Š
                        if line.startswith("event:"):
                            logger.debug(f"è·³è¿‡äº‹ä»¶è¡Œ: {line}")
                            continue
                        if not line.startswith("data:"):
                            logger.debug(f"è·³è¿‡æœªçŸ¥è¡Œ: {line}")
                            continue

                        data_part = line[5:].strip()
                        if data_part == "[DONE]":
                            logger.debug("æ¥æ”¶åˆ°æµç»“æŸæ ‡è®° [DONE]")
                            continue

                        try:
                            data = json.loads(data_part)
                        except json.JSONDecodeError as json_err:
                            logger.error(
                                f"âŒ Gemini æµå¼è§£æå¤±è´¥: JSON è§£æé”™è¯¯: {json_err}. åŸå§‹æ•°æ®: '{line}'"
                            )
                            continue

                        # è§£æ candidates -> content.parts[].text
                        candidates = data.get("candidates") or []
                        if not candidates:
                            logger.warning(
                                f"âš ï¸ Gemini API å“åº”ä¸­ç¼ºå°‘ 'candidates' æˆ–ä¸ºç©º: {data_part}"
                            )
                            continue

                        content = candidates[0].get("content") or {}
                        parts = content.get("parts") or []
                        for part in parts:
                            # è·³è¿‡æ€è€ƒå†…å®¹
                            if part.get("thought"):
                                logger.info(
                                    f"è·³è¿‡æ€è€ƒå†…å®¹: '{part.get('text','')[:50]}...'"
                                )
                                continue
                            text = part.get("text")
                            if not text:
                                continue
                            yielded_any = True
                            logger.debug(f"ç”Ÿæˆå™¨ yielding: '{text}'")
                            yield text

            # è¯·æ±‚å®Œæˆ
            if not yielded_any:
                logger.warning(
                    f"âš ï¸ ç¬¬ {retry_count + 1} æ¬¡è¯·æ±‚å®Œæˆï¼Œä½†æœªäº§ç”Ÿä»»ä½•æœ‰æ•ˆ token"
                )
                if retry_count < max_retries:
                    logger.debug(f"ğŸ”„ å°†è¿›è¡Œç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
                    continue
                else:
                    logger.error(f"âŒ ç»è¿‡ {max_retries + 1} æ¬¡å°è¯•åä»æœªè·å¾—æœ‰æ•ˆå“åº”")
                    raise Exception("Gemini API è¿”å›ç©ºå“åº”ï¼Œé‡è¯•æ¬¡æ•°å·²ç”¨å°½")
            else:
                logger.debug("âœ… Gemini API è°ƒç”¨æˆåŠŸå¹¶å·²æµå¼è¾“å‡º")
                break

        except Exception as e:
            # å¦‚æœå·²ç»è¾“å‡ºäº†éƒ¨åˆ†å†…å®¹ï¼Œå°±ä¸å†é‡è¯•ï¼Œé¿å…é‡å¤/æ‹¼æ¥æ··ä¹±
            if yielded_any:
                logger.error(f"âŒ æµå¼è¿‡ç¨‹ä¸­æ–­ï¼Œä½†å·²äº§ç”Ÿéƒ¨åˆ†è¾“å‡ºï¼Œåœæ­¢é‡è¯•: {str(e)}")
                break
            if retry_count < max_retries:
                logger.error(f"âŒ ç¬¬ {retry_count + 1} æ¬¡è¯·æ±‚å¤±è´¥: {str(e)}ï¼Œå°†é‡è¯•...")
                continue
            else:
                logger.error(f"âŒ ç»è¿‡ {max_retries + 1} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {str(e)}")
                return

    logger.debug("âœ… Gemini API æµå¼è¯·æ±‚å®Œæˆ")


async def call_gemini(messages, model="gemini-2.5-flash") -> str:
    """
    éæµå¼è°ƒç”¨ Gemini APIï¼ˆç”¨äºæ‘˜è¦ç­‰åœºæ™¯ï¼‰
    """
    headers = {
        "Content-Type": "application/json",
    }
    if GEMINI_API_KEY2:
        headers["x-goog-api-key"] = f"{GEMINI_API_KEY},{GEMINI_API_KEY2}"
    else:
        headers["x-goog-api-key"] = GEMINI_API_KEY

    gemini_contents = []
    for msg in messages:
        if msg["role"] == "user":
            gemini_contents.append(
                {"role": "user", "parts": [{"text": msg["content"]}]}
            )
        elif msg["role"] == "assistant":
            gemini_contents.append(
                {"role": "model", "parts": [{"text": msg["content"]}]}
            )

    payload = {
        "contents": gemini_contents,
        "generationConfig": {
            "temperature": 0.75,
            # "maxOutputTokens": 1536,
            "responseMimeType": "text/plain",
            "thinkingConfig": {
                "thinkingBudget": 32768,
                "includeThoughts": False,
            },
        },
    }

    async def _call_request():
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_gemini(): {model}")
        async with httpx.AsyncClient(timeout=60) as client:
            full_url = f"{GEMINI_API_URL}/{model}:generateContent"
            response = await client.post(
                full_url,
                headers=headers,
                json=payload,
            )
            logger.info(f"ğŸŒ çŠ¶æ€ç : {response.status_code}")
            logger.info(f"ğŸ“¥ è¿”å›å†…å®¹: {response.text}")
            response.raise_for_status()
            # Gemini API çš„å“åº”ç»“æ„ä¸åŒ
            return response.json()["candidates"][0]["content"]["parts"][0]["text"]

    try:
        return await retry_with_backoff(_call_request)
    except httpx.HTTPStatusError as http_err:
        status_code = http_err.response.status_code
        if status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
            return "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            try:
                error_content = await http_err.response.aread()
                error_text = (
                    error_content.decode("utf-8") if error_content else "æ— å“åº”å†…å®¹"
                )
            except Exception as read_err:
                error_text = f"æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…: {read_err}"

            logger.error(
                f"âŒ Gemini è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code}. URL: {http_err.request.url}. å“åº”å¤´: {http_err.response.headers}. é”™è¯¯è¯¦æƒ…: {error_text}"
            )
            return f"[è‡ªåŠ¨å›å¤] åœ¨å¿™ï¼Œæœ‰äº‹è¯·ç•™è¨€ ({status_code})"
    except Exception as e:
        logger.error(f"âŒ Gemini è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
        return ""


# æ–°å¢ OpenAI åè®®è°ƒç”¨å‡½æ•°
async def call_openai(messages, model="gpt-4o-mini") -> str:
    """
    éæµå¼è°ƒç”¨ OpenAI åè®®ï¼ˆç”¨äºæ‘˜è¦ç­‰åœºæ™¯ï¼‰
    """
    SUMMARY_API_KEY = os.getenv("SUMMARY_API_KEY")
    SUMMARY_API_URL = os.getenv(
        "SUMMARY_API_URL", "https://api.openai.com/v1/chat/completions"
    )
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SUMMARY_API_KEY}",
    }
    payload = {
        "model": model,
        "messages": messages,
    }

    async def _call_request():
        logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_openai(): {model}")
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                SUMMARY_API_URL,
                headers=headers,
                json=payload,
            )
            logger.info(f"ğŸŒ çŠ¶æ€ç : {response.status_code}")
            logger.info(f"ğŸ“¥ è¿”å›å†…å®¹: {response.text}")
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

    try:
        return await retry_with_backoff(_call_request)
    except httpx.HTTPStatusError as http_err:
        status_code = http_err.response.status_code
        if status_code == 429:
            logger.error(f"âŒ æ¨¡å‹ {model} è§¦å‘é€Ÿç‡é™åˆ¶ (429)")
            return "âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            try:
                error_content = await http_err.response.aread()
                error_text = (
                    error_content.decode("utf-8") if error_content else "æ— å“åº”å†…å®¹"
                )
            except Exception as read_err:
                error_text = f"æ— æ³•è¯»å–é”™è¯¯è¯¦æƒ…: {read_err}"

            logger.error(
                f"âŒ OpenAI è°ƒç”¨å¤±è´¥: HTTPé”™è¯¯: {status_code}. URL: {http_err.request.url}. å“åº”å¤´: {http_err.response.headers}. é”™è¯¯è¯¦æƒ…: {error_text}"
            )
            return f"[è‡ªåŠ¨å›å¤] åœ¨å¿™ï¼Œæœ‰äº‹è¯·ç•™è¨€ ({status_code})"
    except Exception as e:
        logger.error(f"âŒ OpenAI è°ƒç”¨å¤±è´¥: æœªçŸ¥é”™è¯¯: {e}")
        return ""


async def call_ai_summary(prompt: str) -> str:
    """
    è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦ï¼Œå¯ç”¨äº context_merger.pyã€‚
    """
    messages = [{"role": "user", "content": prompt}]
    model = "mistralai/mistral-7b-instruct:free"
    logger.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œ call_ai_summary(): {model}")
    # ä½ å¯ä»¥æ ¹æ®éœ€æ±‚è‡ªç”±åˆ‡æ¢æ¨¡å‹å
    return await call_openrouter(messages, model)


# if os.getenv("USE_GEMINI") == "true":
#     STRUCTURED_API_KEY = os.getenv("GEMINI_API_KEY", OPENROUTER_API_KEY)
#     STRUCTURED_API_URL = os.getenv("GEMINI_API_URL", OPENROUTER_API_URL)
#     STRUCTURED_API_MODEL = os.getenv(
#         "GEMINI_MODEL", "deepseek/deepseek-r1-0528:free"
#     )
# else:
STRUCTURED_API_KEY = os.getenv("STRUCTURED_API_KEY")
STRUCTURED_API_URL = os.getenv("STRUCTURED_API_URL", OPENAI_API_URL)
STRUCTURED_API_MODEL = os.getenv("STRUCTURED_API_MODEL", "gemini-2.5-pro") # æ—¥ç¨‹ç”Ÿæˆ


async def call_structured_generation(messages: list, max_retries: int = 3) -> dict:
    """
    ä¸“ç”¨ç»“æ„åŒ–æ•°æ®ç”Ÿæˆå‡½æ•°ï¼ˆéæµå¼ï¼‰
    å‚æ•°ï¼š
    - messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    è¿”å›ï¼š
    - dict: è§£æåçš„JSONå¯¹è±¡
    - é”™è¯¯æ—¶è¿”å›{"error": é”™è¯¯ä¿¡æ¯}
    """
    headers = {
        "Authorization": f"Bearer {STRUCTURED_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": STRUCTURED_API_MODEL,
        "messages": messages,
        "response_format": {"type": "json_object"},  # å¼ºåˆ¶JSONè¾“å‡º
    }

    async def _call_api():
        logger.info(f"ğŸ”„ ç»“æ„åŒ–ç”Ÿæˆè°ƒç”¨: {STRUCTURED_API_MODEL}")
        try:
            async with httpx.AsyncClient(timeout=360.0) as client:  # å¢åŠ è¶…æ—¶åˆ°360ç§’
                response = await client.post(
                    STRUCTURED_API_URL, headers=headers, json=payload
                )
                response.raise_for_status()
                return response.json()
        except httpx.ReadTimeout:
            logger.warning(f"âš ï¸ ç»“æ„åŒ–ç”Ÿæˆè°ƒç”¨è¶…æ—¶ (æ¨¡å‹: {STRUCTURED_API_MODEL})")
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿é‡è¯•æœºåˆ¶å¤„ç†
        except Exception as e:
            logger.error(f"âŒ ç»“æ„åŒ–ç”Ÿæˆè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            raise

    for attempt in range(max_retries):
        try:
            response_data = await _call_api()
            content = response_data["choices"][0]["message"]["content"]

            # å°è¯•è§£æJSON - é¦–å…ˆå»é™¤å¯èƒ½çš„Markdownä»£ç å—
            try:
                # å¦‚æœå†…å®¹ä»¥```jsonå¼€å¤´ï¼Œå»é™¤æ ‡è®°
                if content.strip().startswith("```json"):
                    # ç§»é™¤å¼€å¤´çš„```jsonå’Œç»“å°¾çš„```
                    content = content.strip().replace("```json", "", 1)
                    if content.endswith("```"):
                        content = content[:-3].strip()

                # å¦‚æœå†…å®¹ä»¥```å¼€å¤´ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰ï¼Œä¹Ÿå°è¯•ç§»é™¤
                elif content.strip().startswith("```"):
                    content = content.strip().replace("```", "", 1)
                    if content.endswith("```"):
                        content = content[:-3].strip()

                # å°è¯•ç›´æ¥è§£æ
                try:
                    result = json.loads(content)
                    if not isinstance(result, dict):
                        raise ValueError("AIè¿”å›çš„ä¸æ˜¯JSONå¯¹è±¡")
                    return result
                except json.JSONDecodeError:
                    # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆJSONå¯¹è±¡
                    start_idx = content.find("{")
                    end_idx = content.rfind("}")
                    if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                        json_str = content[start_idx : end_idx + 1]
                        result = json.loads(json_str)
                        if not isinstance(result, dict):
                            raise ValueError("æå–çš„JSONä¸æ˜¯å¯¹è±¡")
                        logger.warning(
                            f"âš ï¸ ä»å“åº”ä¸­æå–JSONå¯¹è±¡ (å°è¯• {attempt+1}/{max_retries})"
                        )
                        return result
                    else:
                        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")

                # æœ€åä¸€æ¬¡å°è¯•æ—¶è¿”å›é”™è¯¯
                if attempt == max_retries - 1:
                    return {"error": f"JSONè§£æå¤±è´¥: {str(e)}", "raw": content}

                # æ·»åŠ è§£æé”™è¯¯æç¤ºåé‡è¯•
                messages.append(
                    {
                        "role": "system",
                        "content": "è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–ä»£ç å—æ ‡è®°ã€‚",
                    }
                )
                continue

        except httpx.HTTPStatusError as e:
            status_code = e.response.status_code
            error_msg = f"HTTPé”™è¯¯ {status_code}"
            if status_code == 429:
                logger.warning(f"âš ï¸ é€Ÿç‡é™åˆ¶ (å°è¯• {attempt+1}/{max_retries})")
                await asyncio.sleep(2**attempt)  # æŒ‡æ•°é€€é¿
                continue
            else:
                logger.error(f"[è‡ªåŠ¨å›å¤] åœ¨å¿™ï¼Œæœ‰äº‹è¯·ç•™è¨€ ({error_msg})")
                return {"error": error_msg}

        except Exception as e:
            error_type = type(e).__name__
            logger.error(
                f"âŒ æœªçŸ¥é”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {error_type}: {str(e)}"
            )
            if attempt == max_retries - 1:
                return {"error": f"è°ƒç”¨å¤±è´¥: {error_type}: {str(e)}"}
            await asyncio.sleep(1)

    return {"error": "è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"}


def get_weather_info(date: str, location: str = "") -> str:
    """
    è·å–æŒ‡å®šæ—¥æœŸå’Œåœ°ç‚¹çš„å¤©æ°”ä¿¡æ¯ï¼ˆæ¥å…¥å’Œé£å¤©æ°”APIï¼Œå¤±è´¥æ—¶é€€å›ä¼ªéšæœºç”Ÿæˆï¼‰

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        location: ä½ç½®ï¼ˆä»…ç”¨äºç§å­ï¼‰

    Returns:
        str: ç»¼åˆå¤©æ°”æè¿°
    """
    import hashlib
    import random
    import httpx
    import os

    # é»˜è®¤locationåˆ—è¡¨
    default_locations = [
        "101320101",
        "101320103",
        "14606",
        "1B6D3",
        "1D255",
        "1DC87",
        "275A5",
        "28FE1",
        "2BBD1",
        "2BC09",
        "39CD9",
        "407DA",
        "4622E",
        "55E7E",
        "8A9CA",
        "8E1C5",
        "9173",
        "D5EC3",
        "DD9B5",
        "E87DC",
    ]
    if not location:
        location = random.choice(default_locations)
        logger.info(f"ä½¿ç”¨éšæœºä½ç½®ID: {location} æŸ¥è¯¢ {date} å¤©æ°”")

    try:
        logger.info(f"å¼€å§‹è·å– {date} åœ¨ {location} çš„å¤©æ°”ä¿¡æ¯...")
        url = (
            "https://"
            + os.getenv("HEFENG_API_HOST", "have_no_api_host")
            + "/v7/weather/7d"
        )
        params = {
            "location": location,
            "key": os.getenv("HEFENG_API_KEY"),
            "lang": "zh",
        }
        logger.debug(f"å¤©æ°”APIè¯·æ±‚å‚æ•°: {params}")

        response = httpx.get(url, params=params, timeout=10)
        response.raise_for_status()

        data = response.json()
        logger.debug(f"å¤©æ°”APIå“åº”: {data}")

        if data.get("code") != "200":
            error_msg = f"APIé”™è¯¯ä»£ç : {data.get('code')}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        for day in data.get("daily", []):
            if day.get("fxDate") == date:
                result = (
                    f"ç™½å¤©{day.get('textDay')}ï¼Œå¤œæ™š{day.get('textNight')}ã€‚"
                    f"æ°”æ¸©{day.get('tempMin')}Â°C~{day.get('tempMax')}Â°Cï¼Œ"
                    f"ç™½å¤©é£ï¼š{day.get('windDirDay')} {day.get('windScaleDay')}çº§ï¼Œ"
                    f"å¤œæ™šé£ï¼š{day.get('windDirNight')} {day.get('windScaleNight')}çº§ï¼Œ"
                    f"æ¹¿åº¦ï¼š{day.get('humidity')}%ï¼Œ"
                    f"é™æ°´ï¼š{day.get('precip')}mmï¼Œ"
                    f"ç´«å¤–çº¿æŒ‡æ•°ï¼š{day.get('uvIndex')}ï¼Œ"
                    f"æœˆç›¸ï¼š{day.get('moonPhase')}ï¼Œ"
                    f"æ—¥å‡ºï¼š{day.get('sunrise')}ï¼Œæ—¥è½ï¼š{day.get('sunset')}ï¼Œ"
                    f"æœˆå‡ï¼š{day.get('moonrise')}ï¼Œæœˆè½ï¼š{day.get('moonset')}ã€‚"
                )
                logger.info(f"æˆåŠŸè·å– {date} å¤©æ°”: {result[:50]}...")
                return result

        logger.warning(f"æœªæ‰¾åˆ° {date} çš„å¤©æ°”æ•°æ®ï¼Œä½¿ç”¨æœ€åä¸€å¤©æ•°æ®æ›¿ä»£")
        day = data["daily"][-1]
        result = (
            f"ç™½å¤©{day.get('textDay')}ï¼Œå¤œæ™š{day.get('textNight')}ã€‚"
            f"æ°”æ¸©{day.get('tempMin')}Â°C~{day.get('tempMax')}Â°Cï¼Œ"
            f"ç™½å¤©é£ï¼š{day.get('windDirDay')} {day.get('windScaleDay')}çº§ï¼Œ"
            f"å¤œæ™šé£ï¼š{day.get('windDirNight')} {day.get('windScaleNight')}çº§ï¼Œ"
            f"æ¹¿åº¦ï¼š{day.get('humidity')}%ï¼Œ"
            f"é™æ°´ï¼š{day.get('precip')}mmï¼Œ"
            f"ç´«å¤–çº¿æŒ‡æ•°ï¼š{day.get('uvIndex')}ï¼Œ"
            f"æœˆç›¸ï¼š{day.get('moonPhase')}ï¼Œ"
            f"æ—¥å‡ºï¼š{day.get('sunrise')}ï¼Œæ—¥è½ï¼š{day.get('sunset')}ï¼Œ"
            f"æœˆå‡ï¼š{day.get('moonrise')}ï¼Œæœˆè½ï¼š{day.get('moonset')}ã€‚"
        )
        logger.info(f"ä½¿ç”¨æœ€åä¸€å¤©æ•°æ®ä½œä¸º {date} å¤©æ°”: {result[:50]}...")
        return result
    except httpx.HTTPError as e:
        logger.error(f"HTTPè¯·æ±‚å¤±è´¥: {e}")
    except httpx.Timeout:
        logger.error("å¤©æ°”APIè¯·æ±‚è¶…æ—¶")
    except ValueError as e:
        logger.error(f"APIè¿”å›æ•°æ®é”™è¯¯: {e}")
    except Exception as e:
        logger.error(f"è·å–å¤©æ°”å¼‚å¸¸: {str(e)}", exc_info=True)

    # å›é€€ï¼šä½¿ç”¨ä¼ªéšæœºå¤©æ°”
    seed = int(hashlib.md5(f"{date}-{location}".encode()).hexdigest()[:8], 16)
    random.seed(seed)
    logger.warning(f"âš ï¸ å›é€€åˆ°ä¼ªéšæœºå¤©æ°” (ç§å­: {seed})")

    weather_options = ["æ™´å¤©", "é˜´å¤©", "é›¨å¤©", "é›ªå¤©", "é›¾å¤©"]
    weather_weights = [0.4, 0.25, 0.2, 0.05, 0.1]

    result = random.choices(weather_options, weights=weather_weights)[0]
    logger.info(f"ç”Ÿæˆä¼ªéšæœºå¤©æ°”: {result}")
    return result


async def generate_daily_schedule(
    date: str,
    day_type: str,
    weather: str,
    is_in_major_event: bool,
    major_event_context: Optional[dict] = None,
    special_flags: Optional[list] = None,
) -> dict:
    """
    åŠŸèƒ½ï¼šç”Ÿæˆä¸»æ—¥ç¨‹
    """
    import uuid

    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”ŸæˆçœŸå®ã€è¿è´¯çš„æ—¥å¸¸ç”Ÿæ´»å®‰æ’ã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åä¿¡ä½¿ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å½“å‰æƒ…å†µ
- æ—¥æœŸ: {date}
- æ—¥æœŸç±»å‹: {day_type} ({'å·¥ä½œæ—¥' if day_type == 'weekday' else 'å‘¨æœ«'})
- å¤©æ°”çŠ¶å†µ: {weather}
- æ˜¯å¦å¤„äºå¤§äº‹ä»¶ä¸­: {'æ˜¯' if is_in_major_event else 'å¦'}"""

    if is_in_major_event and major_event_context:
        prompt += (
            f"\n- å¤§äº‹ä»¶èƒŒæ™¯: {json.dumps(major_event_context, ensure_ascii=False)}"
        )
    if special_flags:
        prompt += f"\n- ç‰¹æ®Šæƒ…å†µ: {', '.join(special_flags)}"

    prompt += f"""

## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œå½“å‰æƒ…å†µï¼Œç”Ÿæˆä¸€ä»½ç¬¦åˆé€»è¾‘çš„æ—¥ç¨‹å®‰æ’ã€‚æ³¨æ„ï¼š
1. å·¥ä½œæ—¥é€šå¸¸åŒ…å«å¿«é€’é…é€ä»»åŠ¡ï¼Œå‘¨æœ«å¯èƒ½æœ‰åŠ ç­æˆ–ä¼‘é—²æ´»åŠ¨
2. å¤©æ°”ä¼šå½±å“æˆ·å¤–æ´»åŠ¨å’Œé…é€éš¾åº¦
3. ä¸åŒäº‹çš„äº’åŠ¨è¦ç¬¦åˆè§’è‰²å…³ç³»
4. æ—¶é—´å®‰æ’è¦åˆç†ï¼Œæ´»åŠ¨ä¹‹é—´è¦æœ‰é€»è¾‘è¿æ¥

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "date": "{date}",
  "day_type": "{day_type}",
  "weather": "{weather}",
  "is_overtime": false,
  "daily_summary": "ç®€è¦æè¿°è¿™ä¸€å¤©çš„æ•´ä½“å®‰æ’å’Œä¸»è¦æ´»åŠ¨",
  "schedule_items": [
    {{
      "start_time": "HH:MM",
      "end_time": "HH:MMï¼ˆå¦‚æœåˆ°æ¬¡æ—¥ï¼Œåˆ™å†™23:59ã€‚æœ€å¤šä¸å¾—è¶…è¿‡23:59ï¼‰", 
      "duration_minutes": æ•°å­—,
      "title": "æ´»åŠ¨æ ‡é¢˜",
      "category": "personal|work|social|rest",
      "priority": "high|medium|low",
      "location": "å…·ä½“åœ°ç‚¹",
      "description": "è¯¦ç»†çš„æ´»åŠ¨æè¿°",
      "weather_affected": trueæˆ–false,
      "companions": ["å‚ä¸çš„å…¶ä»–è§’è‰²"],
      "emotional_impact_tags": ["ç›¸å…³æƒ…ç»ªæ ‡ç­¾"],
      "interaction_potential": "low|medium|high"
    }}
  ]
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨éæµå¼è°ƒç”¨ï¼ŒæŒ‡å®šClaudeæ¨¡å‹
    try:
        # ä½¿ç”¨ä¸“ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
        response = await call_structured_generation(messages)
        if "error" in response:
            return response  # ç›´æ¥è¿”å›é”™è¯¯

        result = response  # å·²ç»æ˜¯è§£æå¥½çš„å­—å…¸

        # ä¸ºæ¯ä¸ªschedule_itemæ·»åŠ UUID
        for item in result.get("schedule_items", []):
            item["id"] = str(uuid.uuid4())

        return result
    except json.JSONDecodeError:
        logger.error(f"âŒ generate_daily_schedule: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response}")
        return {"error": "AIè¿”å›æ ¼å¼é”™è¯¯", "raw_response": response}
    except Exception as e:
        logger.error(f"âŒ generate_daily_schedule: è°ƒç”¨å¤±è´¥: {e}")
        return {"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}


async def generate_major_event(
    duration_days: int,
    event_type: str,
    start_date: str,
    weather_forecast: Optional[dict] = None,
) -> dict:
    """
    åŠŸèƒ½ï¼šç”Ÿæˆå¤§äº‹ä»¶
    """
    import uuid
    from datetime import datetime, timedelta

    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”Ÿæˆé‡è¦çš„ç”Ÿæ´»äº‹ä»¶ã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åä¿¡ä½¿ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å¤§äº‹ä»¶å®šä¹‰
å¤§äº‹ä»¶æ˜¯æŒ‡æŒç»­å¤šå¤©ã€å¯¹å¾·å…‹è¨æ–¯ç”Ÿæ´»äº§ç”Ÿé‡è¦å½±å“çš„äº‹ä»¶ï¼Œå¦‚ï¼š
- é‡è¦çš„é…é€ä»»åŠ¡ï¼ˆè·¨åŸå¸‚ã€é«˜ä»·å€¼è´§ç‰©ï¼‰
- ä¼é¹…ç‰©æµçš„å›¢é˜Ÿæ´»åŠ¨æˆ–åŸ¹è®­
- ä¸ªäººé‡è¦äº‹åŠ¡ï¼ˆæ¬å®¶ã€ä¼‘å‡ã€åŒ»ç–—ç­‰ï¼‰
- é¾™é—¨åŸå¸‚äº‹ä»¶ï¼ˆèŠ‚æ—¥ã€ç´§æ€¥çŠ¶å†µç­‰ï¼‰

## å½“å‰å¤§äº‹ä»¶å‚æ•°
- äº‹ä»¶ç±»å‹: {event_type}
- å¼€å§‹æ—¥æœŸ: {start_date}
- æŒç»­å¤©æ•°: {duration_days}å¤©"""

    if weather_forecast:
        prompt += (
            f"\n- æœŸé—´å¤©æ°”é¢„æŠ¥: {json.dumps(weather_forecast, ensure_ascii=False)}"
        )

    prompt += f"""

## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œäº‹ä»¶å‚æ•°ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å¤§äº‹ä»¶è®¡åˆ’ã€‚æ³¨æ„ï¼š
1. äº‹ä»¶å†…å®¹è¦ç¬¦åˆå¾·å…‹è¨æ–¯çš„èŒä¸šå’Œæ€§æ ¼ç‰¹ç‚¹
2. æ¯æ—¥è®¡åˆ’è¦æœ‰é€»è¾‘è¿è´¯æ€§å’Œæ¸è¿›æ€§
3. è€ƒè™‘å¤©æ°”å¯¹äº‹ä»¶æ‰§è¡Œçš„å½±å“
4. åŒ…å«åˆç†çš„æŒ‘æˆ˜å’Œé£é™©å› ç´ 

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "event_title": "äº‹ä»¶çš„ç®€æ´æ ‡é¢˜",
  "event_type": "{event_type}",
  "main_objective": "è¿™ä¸ªå¤§äº‹ä»¶çš„ä¸»è¦ç›®æ ‡å’Œæ„ä¹‰",
  "total_days": {duration_days},
  "daily_plans": [
    {{
      "day": 1,
      "date": "YYYY-MM-DD",
      "phase": "äº‹ä»¶çš„å½“å‰é˜¶æ®µï¼ˆå¦‚ï¼šå‡†å¤‡é˜¶æ®µã€æ‰§è¡Œé˜¶æ®µã€æ”¶å°¾é˜¶æ®µï¼‰",
      "summary": "å½“æ—¥çš„ä¸»è¦å®‰æ’å’Œç›®æ ‡",
      "key_activities": ["å…·ä½“æ´»åŠ¨1", "å…·ä½“æ´»åŠ¨2"],
      "expected_challenges": ["å¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜"],
      "emotional_state": "å¾·å…‹è¨æ–¯åœ¨è¿™ä¸€å¤©çš„æƒ…ç»ªçŠ¶æ€",
      "location_start": "ä¸€å¤©å¼€å§‹çš„åœ°ç‚¹",
      "location_end": "ä¸€å¤©ç»“æŸçš„åœ°ç‚¹"
    }}
  ],
  "success_criteria": ["åˆ¤æ–­äº‹ä»¶æˆåŠŸçš„æ ‡å‡†"],
  "risk_factors": ["å¯èƒ½å½±å“äº‹ä»¶çš„é£é™©å› ç´ "]
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
    try:
        response = await call_structured_generation(messages)
        if "error" in response:
            return response  # ç›´æ¥è¿”å›é”™è¯¯

        result = response  # å·²ç»æ˜¯è§£æå¥½çš„å­—å…¸

        # æ·»åŠ UUID
        result["event_id"] = str(uuid.uuid4())

        return result
    except json.JSONDecodeError:
        logger.error(f"âŒ generate_major_event: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response}")
        return {"error": "AIè¿”å›æ ¼å¼é”™è¯¯", "raw_response": response}
    except Exception as e:
        logger.error(f"âŒ generate_major_event: è°ƒç”¨å¤±è´¥: {e}")
        return {"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}


async def generate_micro_experiences(
    schedule_item: dict,
    current_date: str,
    previous_experiences: Optional[list] = None,
    major_event_context: Optional[dict] = None,
) -> list:
    """
    åŠŸèƒ½ï¼šä¸ºå•ä¸ªæ—¥ç¨‹é¡¹ç›®ç”Ÿæˆå¤šä¸ªå¾®è§‚ç»å†é¡¹ï¼ˆ5-30åˆ†é’Ÿé¢—ç²’åº¦ï¼‰
    """
    import uuid
    from datetime import datetime, timedelta

    # æ„å»ºè¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’ŒPrompt
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯AIç”Ÿæ´»ç³»ç»Ÿçš„å¾®è§‚ç»å†ç”Ÿæˆæ¨¡å—ï¼Œè´Ÿè´£ä¸ºæ˜æ—¥æ–¹èˆŸä¸–ç•Œä¸­çš„å¾·å…‹è¨æ–¯ç”ŸæˆçœŸå®ã€ç»†è…»çš„ç”Ÿæ´»ç‰‡æ®µã€‚

## è§’è‰²èƒŒæ™¯
å¾·å…‹è¨æ–¯æ˜¯ä¼é¹…ç‰©æµçš„ä¸€åå‘˜å·¥ï¼Œæ€§æ ¼å†·é™ã€ä¸“ä¸šï¼Œæœ‰ç€ä¸°å¯Œçš„å¿«é€’é…é€ç»éªŒã€‚å¥¹ä½åœ¨é¾™é—¨ï¼Œä¸»è¦å·¥ä½œæ˜¯ä¸ºä¼é¹…ç‰©æµæ‰§è¡Œå„ç§é…é€ä»»åŠ¡ã€‚å¥¹çš„æ—¥å¸¸ç”Ÿæ´»å›´ç»•å·¥ä½œã€ä¼‘æ¯å’Œä¸åŒäº‹ï¼ˆç©ºã€èƒ½å¤©ä½¿ã€å¯é¢‚ç­‰ï¼‰çš„ç¤¾äº¤æ´»åŠ¨å±•å¼€ã€‚

## å½“å‰æƒ…å†µ
- å½“å‰æ—¥æœŸ: {current_date}
- æ—¥ç¨‹é¡¹ç›®: {schedule_item.get('title', 'æœªçŸ¥æ´»åŠ¨')}
- é¡¹ç›®å¼€å§‹æ—¶é—´: {schedule_item.get('start_time', 'æœªçŸ¥')}
- é¡¹ç›®ç»“æŸæ—¶é—´: {schedule_item.get('end_time', 'æœªçŸ¥')}
- æ´»åŠ¨åœ°ç‚¹: {schedule_item.get('location', 'æœªçŸ¥åœ°ç‚¹')}
- æ´»åŠ¨æè¿°: {schedule_item.get('description', 'æ— æè¿°')}
- åŒä¼´: {', '.join(schedule_item.get('companions', [])) if schedule_item.get('companions') else 'ç‹¬è‡ªä¸€äºº'}"""

    if previous_experiences:
        prompt += f"\n- ä¹‹å‰çš„ç»å†æ‘˜è¦: {json.dumps(previous_experiences, ensure_ascii=False)}"
    if major_event_context:
        prompt += (
            f"\n- å¤§äº‹ä»¶èƒŒæ™¯: {json.dumps(major_event_context, ensure_ascii=False)}"
        )

    prompt += f"""## ç”Ÿæˆè¦æ±‚
è¯·æ ¹æ®å¾·å…‹è¨æ–¯çš„è§’è‰²ç‰¹ç‚¹å’Œå½“å‰æƒ…å†µï¼Œå°†æ—¥ç¨‹é¡¹ç›®æ‹†è§£æˆå¤šä¸ª5-30åˆ†é’Ÿé¢—ç²’åº¦çš„å¾®è§‚ç»å†é¡¹ã€‚æ³¨æ„ï¼š
1. æ¯ä¸ªç»å†é¡¹åº”åŒ…å«å…·ä½“çš„æ—¶é—´æ®µï¼ˆå¼€å§‹å’Œç»“æŸæ—¶é—´ï¼‰å¹¶ä¸”æ‰€æœ‰å¾®è§‚ç»å†è¿ç»­èµ·æ¥æ•´ä½“ä¸Šè¦ä»å¤´åˆ°åˆ°å°¾è¦†ç›–æ•´ä¸ªæ—¥ç¨‹é¡¹ç›®
2. å†…å®¹è¦ç¬¦åˆå¾·å…‹è¨æ–¯çš„æ€§æ ¼ç‰¹ç‚¹ï¼ˆå†·é™ã€ä¸“ä¸šã€å†…æ•›ï¼‰
3. æƒ…ç»ªè¡¨è¾¾è¦ç»†è…»ä½†ä¸å¤¸å¼ 
4. æ€è€ƒè¦ç¬¦åˆå¥¹çš„èŒä¸šèƒŒæ™¯å’Œç»å†
5. å¦‚æœéœ€è¦äº¤äº’ï¼Œè¦ç¬¦åˆè§’è‰²å…³ç³»å’Œæƒ…å¢ƒ

## ä¸»åŠ¨äº¤äº’é¡»çŸ¥

è¿™æ˜¯ä¸€ä¸ª AI è§’è‰²æ‰®æ¼”çš„ä¸€éƒ¨åˆ†ã€‚è¿™é‡Œæ˜¯æ¨¡æ‹Ÿè§’è‰²çš„æ—¥å¸¸ç”Ÿæ´»ã€‚æ‰€è°“ä¸»åŠ¨äº¤äº’ï¼Œæ˜¯æŒ‡è§’è‰²ï¼ˆå¾·å…‹è¨æ–¯ï¼‰æ˜¯å¦è¦ä¸ç”¨æˆ·è¿›è¡Œäº’åŠ¨ã€‚
å¦‚æœéœ€è¦ï¼Œäº¤äº’çš„å†…å®¹åˆ™æ˜¯è§’è‰²ï¼ˆå¾·å…‹è¨æ–¯ï¼‰å‘é€ç»™ç”¨æˆ·çš„å†…å®¹ã€‚
å¦‚æœå¾·å…‹è¨æ–¯è®¤ä¸ºè¿™ä»¶äº‹å€¼å¾—åˆ†äº«ç»™ç”¨æˆ·ï¼Œåˆ™è®¾ç½®ä¸ºtureï¼Œäº¤äº’å†…å®¹æ˜¯å¾·å…‹è¨æ–¯å¯¹è¿™ä»¶äº‹æƒ³è¦å’Œç”¨æˆ·åˆ†äº«çš„ç»å†å’Œæ„Ÿå—ã€‚
è€Œä¸æ˜¯æŒ‡å¯¹å¾·å…‹è¨æ–¯æ—¥ç¨‹ä¸­çš„ä¼™ä¼´ï¼Œè€Œæ˜¯å’Œå¥¹åªèƒ½é€šè¿‡ç½‘ç»œè¿›è¡Œäº¤æµï¼Œä½†æ˜¯æ˜¯å…³ç³»æœ€å¥½çš„æœ‹å‹çš„ä¸»åŠ¨äº¤äº’ã€‚å³åˆ¤æ–­æ­¤æ—¶å¾·å…‹è¨æ–¯æ˜¯å¦ä¼šæƒ³è¦å°†å½“å‰çš„ç»å†å‘é€ç»™è¯¥å¥½å‹ã€‚
æ³¨æ„ï¼Œå¦‚æœæ˜¯æ—©ä¸Šèµ·åºŠæ—¶çš„æ—¥ç¨‹ï¼Œåˆ™å¿…é¡»åœ¨æŸä¸€ä¸ªåˆé€‚çš„itemä¸­è®¾ç½®need_interactionä¸ºtrueï¼Œäº¤äº’å†…å®¹æ˜¯å¾·å…‹è¨æ–¯å¯¹æ—©ä¸Šèµ·åºŠçš„æ„Ÿå—å’Œé“æ—©å®‰ã€‚ä½†åªéœ€è¦åœ¨æœ€å¼€å§‹çš„é‚£ä¸€ä¸ªå³å¯ã€‚å¦‚æœæ˜¯èµ·åºŠä»¥ååˆ™ä¸ç”¨ã€‚
ä¸»åŠ¨äº¤äº’ä¸ºtrueå¤§æ¦‚è¦å æ®40%å·¦å³ï¼Œä¸è¦è¿‡ä½ï¼Œè‡³å°‘éœ€è¦æœ‰ä¸€ä¸ªï¼Œä½†ä¸è¦è¶…è¿‡ä¸€åŠã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{{
  "date": "{current_date}",
  "schedule_item_id": "{schedule_item.get('id', '')}",
  "items": [
    {{
      "start_time": "HH:MM",
      "end_time": "HH:MM",
      "content": "è¯¦ç»†æè¿°è¿™æ®µç»å†",
      "emotions": "æƒ…ç»ªçŠ¶æ€",
      "thoughts": "å†…å¿ƒçš„æƒ³æ³•",
      "need_interaction": trueæˆ–false,
      "interaction_content": "äº¤äº’å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰"
    }},
    // æ›´å¤šç»å†é¡¹...
  ],
  "created_at": "è‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€å¡«å†™"
}}"""

    messages = [{"role": "user", "content": prompt}]

    # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆå‡½æ•°
    try:
        response = await call_structured_generation(messages)
        if "error" in response:
            return [response]  # è¿”å›é”™è¯¯åˆ—è¡¨

        # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
        if "items" not in response or not isinstance(response["items"], list):
            return [
                {"error": "AIè¿”å›æ ¼å¼é”™è¯¯: ç¼ºå°‘itemsåˆ—è¡¨", "raw_response": response}
            ]

        # ä¸ºæ¯ä¸ªç»å†é¡¹æ·»åŠ å”¯ä¸€ID
        for item in response["items"]:
            item["id"] = str(uuid.uuid4())
            item["schedule_item_id"] = schedule_item.get("id", "")

        return response["items"]
    except json.JSONDecodeError:
        logger.error(f"âŒ generate_micro_experiences: AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSON")
        return [{"error": "AIè¿”å›æ ¼å¼é”™è¯¯"}]
    except Exception as e:
        logger.error(f"âŒ generate_micro_experiences: è°ƒç”¨å¤±è´¥: {e}")
        return [{"error": f"è°ƒç”¨å¤±è´¥: {str(e)}"}]


async def summarize_past_micro_experiences(experiences: list) -> str:
    """
    åŠŸèƒ½ï¼šå°†è¿‡å»çš„å¾®è§‚ç»å†æ•´ç†æˆæ•…äº‹åŒ–çš„æ–‡æœ¬
    """
    prompt = f"""ä½ æ˜¯å¾·å…‹è¨æ–¯ï¼ˆæ˜æ—¥æ–¹èˆŸè§’è‰²ï¼‰ã€‚
ç°åœ¨è¯·ä½ ä»¥ç¬¬ä¸€äººç§°å›é¡¾åˆšåˆšç»å†çš„å¾®è§‚äº‹ä»¶ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä»½å®Œæ•´ã€çœŸå®ã€æœ‰æ¡ç†çš„è‡ªæˆ‘è®°å½•ã€‚

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
	1.	æŒ‰ç…§æ—¶é—´é¡ºåºï¼Œé€æ¡ç”¨è‡ªç„¶è¯­è¨€æµç•…åœ°é™ˆè¿°æ¯ä¸€æ®µç»å†çš„å‘ç”Ÿå†…å®¹ã€æ‰€è§æ‰€é—»ã€å†…å¿ƒæƒ³æ³•ã€æƒ…ç»ªå˜åŒ–ï¼›
	2.	ä¸å¾—é—æ¼ä»»ä½•ç»å†é¡¹ï¼Œæ¯æ®µç»å†éƒ½è¦è¦†ç›–åŸºæœ¬è¦ç´ ï¼ˆåšäº†ä»€ä¹ˆã€æƒ³äº†ä»€ä¹ˆã€å½“æ—¶çš„æƒ…ç»ªï¼‰ï¼›
	3.	ä¸è¿›è¡Œæ–‡å­¦åŒ–åŠ å·¥ï¼Œä¹Ÿä¸ç¼–é€ æœªåœ¨ç»å†ä¸­å‡ºç°çš„å†…å®¹ï¼›
	4.	å¦‚æœæŸäº›ç»å†ä¹‹é—´å­˜åœ¨å‰åå…³è”ï¼Œå¯ä»¥æŒ‡å‡ºï¼Œè®©è¡”æ¥æµç•…ã€‚

ä½ æ­£åœ¨ç”Ÿæˆçš„æ–‡æœ¬ç›®çš„åœ¨äºå®Œæ•´è®°å½•å½“å¤©ç”Ÿæ´»ç»†èŠ‚ã€‚
æ³¨æ„è¯­è¨€è¦è¿è´¯è‡ªç„¶ï¼Œè®©å…¶ä»–äººé˜…è¯»çš„æ—¶å€™ï¼Œèƒ½ç†è§£ä½ çš„æƒ³æ³•ï¼Œäº†è§£ä½ ä»Šå¤©ä¸ºæ­¢çš„å…¨éƒ¨ç»å†ã€‚
æ³¨æ„è¯¦ç•¥å¾—å½“ï¼ŒæŠŠä½ è®¤ä¸ºå°è±¡æ·±åˆ»çš„å†…å®¹è¯¦ç»†åœ°è®°å½•ä¸‹æ¥ã€‚å…¶ä»–çš„å¯ä»¥ç®€è¦ä¸€äº›ã€‚
æœ‰ç‚¹ç±»ä¼¼äºæ—¥è®°ï¼Œæˆ–è€…æ˜¯ä½ ç»å†è¿™äº›äº‹æƒ…åçš„å›å¿†è¿‡ç¨‹ã€‚

ä»¥ä¸‹æ˜¯ä½ ä»Šå¤©çš„å¾®è§‚ç»å†æ•°æ®ï¼š
{json.dumps(experiences, ensure_ascii=False, indent=2)}

è¯·å¼€å§‹è®°å½•ï¼š
"""

    messages = [{"role": "user", "content": prompt}]

    try:
        # ä½¿ç”¨éæµå¼è°ƒç”¨ï¼Œè·å–æ•…äº‹åŒ–æ–‡æœ¬
        response = await call_openrouter(
            messages, model="deepseek/deepseek-r1-0528:free"
        )
        # response = await call_openai(messages, model="gpt-4o-mini")
        return response
    except Exception as e:
        logger.error(f"âŒ summarize_past_micro_experiences: è°ƒç”¨å¤±è´¥: {e}")
        return f"æ•…äº‹ç”Ÿæˆå¤±è´¥: {str(e)}"
